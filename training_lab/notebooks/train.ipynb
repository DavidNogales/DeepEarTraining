{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def load_dataframe(relative_path,dataframe_name):\n",
    "    df = pd.read_pickle(f'{relative_path}/{dataframe_name}.pkl')    \n",
    "    return df\n",
    "\n",
    "def read_file(relative_path,file_name):\n",
    "    text= \"\"\n",
    "    with open(f'{relative_path}/{file_name}.abc','r') as f:\n",
    "        text = f.read()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['unit_note_length', 'tuneBook', 'title', 'reference_number',\n",
       "       'original_header', 'original_body', 'meter', 'key', 'clean_song',\n",
       "       'clean_header', 'clean_body', 'chord_progression', '\"fm\"', '\"ff'\"',\n",
       "       '\"f7\"', '\"em\"', '\"ee'\"', '\"e7\"', '\"e\"', '\"dm\"', '\"dd'\"', '\"d7\"', '\"d\"',\n",
       "       '\"cm\"', '\"cc'\"', '\"c7\"', '\"c#m\"', '\"c#7\"', '\"c\"', '\"Gm\"', '\"Gg\"',\n",
       "       '\"Gd'\"', '\"G7\"', '\"G#m\"', '\"G#7\"', '\"G\"', '\"Fm\"', '\"Ff\"', '\"Fc'\"',\n",
       "       '\"F7\"', '\"F#m\"', '\"F#7\"', '\"F\"', '\"Em\"', '\"Eb\"', '\"E7\"', '\"E#m\"',\n",
       "       '\"E#7\"', '\"E\"', '\"Dm\"', '\"Da\"', '\"D7\"', '\"D#m\"', '\"D#7\"', '\"D\"', '\"Cm\"',\n",
       "       '\"Cg\"', '\"C7\"', '\"C#m\"', '\"C#7\"', '\"C\"', '\"Bm\"', '\"Bf\"', '\"Bb\"', '\"B7\"',\n",
       "       '\"B#m\"', '\"B#7\"', '\"B\"', '\"Am\"', '\"Ae'\"', '\"Aa\"', '\"A7\"', '\"A#m\"',\n",
       "       '\"A#7\"', '\"A\"'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relative_path =\"notebooks/data/final_dataset\"\n",
    "filename_name = 'clean_augmented_data'\n",
    "#filename_name = 'clean_original_training_data'\n",
    "#relative_path =\"notebooks/data/original_dataset\"\n",
    "training_data_df = load_dataframe(relative_path,filename_name)\n",
    "training_data_df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unit_note_length</th>\n",
       "      <th>tuneBook</th>\n",
       "      <th>title</th>\n",
       "      <th>reference_number</th>\n",
       "      <th>original_header</th>\n",
       "      <th>original_body</th>\n",
       "      <th>meter</th>\n",
       "      <th>key</th>\n",
       "      <th>clean_song</th>\n",
       "      <th>clean_header</th>\n",
       "      <th>...</th>\n",
       "      <th>\"B#m\"</th>\n",
       "      <th>\"B#7\"</th>\n",
       "      <th>\"B\"</th>\n",
       "      <th>\"Am\"</th>\n",
       "      <th>\"Ae'\"</th>\n",
       "      <th>\"Aa\"</th>\n",
       "      <th>\"A7\"</th>\n",
       "      <th>\"A#m\"</th>\n",
       "      <th>\"A#7\"</th>\n",
       "      <th>\"A\"</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9491</th>\n",
       "      <td>1/4</td>\n",
       "      <td>dataset_min5.abc</td>\n",
       "      <td>Grandpa's</td>\n",
       "      <td>78</td>\n",
       "      <td>X:78\\nT:Grandpa's\\nM:4/4\\nL:1/4\\nK:Amajor</td>\n",
       "      <td>E/2D/2|\"A,\"CE\"E7\"FG|\"A,\"A/2G/2A/2B/2ce|\"B,m\"dc...</td>\n",
       "      <td>4/4</td>\n",
       "      <td>A</td>\n",
       "      <td>M:4/4\\nL:1/4\\nK:A\\n|\"A\"\"E7\"|\"A\"|\"Bm\"\"B7\"|\"E7\"|...</td>\n",
       "      <td>M:4/4\\nL:1/4\\nK:A\\n|\"A\"\"E7\"|\"A\"|\"Bm\"\"B7\"|\"E7\"|...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9492</th>\n",
       "      <td>1/4</td>\n",
       "      <td>dataset_min5.abc</td>\n",
       "      <td>The Girl With The Green Hat On</td>\n",
       "      <td>79</td>\n",
       "      <td>X:79\\nT:The Girl With The Green Hat On\\nM:4/4\\...</td>\n",
       "      <td>(3E/2F/2G/2|\"A,\"AE\"E7\"E/2F/2E/2D/2|\"A,\"C/2D/2E...</td>\n",
       "      <td>4/4</td>\n",
       "      <td>A</td>\n",
       "      <td>M:4/4\\nL:1/4\\nK:A\\n|\"A\"\"E7\"|\"A\"\"E7\"|\"A\"|\"E7\"|\"...</td>\n",
       "      <td>M:4/4\\nL:1/4\\nK:A\\n|\"A\"\"E7\"|\"A\"\"E7\"|\"A\"|\"E7\"|\"...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9493</th>\n",
       "      <td>1/4</td>\n",
       "      <td>dataset_min5.abc</td>\n",
       "      <td>Green Meadow</td>\n",
       "      <td>80</td>\n",
       "      <td>X:80\\nT:Green Meadow\\nM:4/4\\nL:1/4\\nK:Dmajor</td>\n",
       "      <td>(3A,/2B,/2C/2|\"D\"DD/2E/2F/2D/2F/2A/2|\"G,\"B/2c/...</td>\n",
       "      <td>4/4</td>\n",
       "      <td>D</td>\n",
       "      <td>M:4/4\\nL:1/4\\nK:D\\n|\"D\"|\"G\"\"D\"|\"Em\"|\"Em\"\"A7\"|\"...</td>\n",
       "      <td>M:4/4\\nL:1/4\\nK:D\\n|\"D\"|\"G\"\"D\"|\"Em\"|\"Em\"\"A7\"|\"...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9494</th>\n",
       "      <td>1/4</td>\n",
       "      <td>dataset_min5.abc</td>\n",
       "      <td>The Old Grey Cat</td>\n",
       "      <td>82</td>\n",
       "      <td>X:82\\nT:The Old Grey Cat\\nM:4/4\\nL:1/4\\nK:Bminor</td>\n",
       "      <td>F|\"B,m\"BBB,B,/2C/2|\"B,m\"D/2C/2D/2E/2F/2E/2F/2^...</td>\n",
       "      <td>4/4</td>\n",
       "      <td>Bm</td>\n",
       "      <td>M:4/4\\nL:1/4\\nK:Bm\\n|\"Bm\"|\"Bm\"|\"A\"|\"A\"|\"Bm\"|\"B...</td>\n",
       "      <td>M:4/4\\nL:1/4\\nK:Bm\\n|\"Bm\"|\"Bm\"|\"A\"|\"A\"|\"Bm\"|\"B...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9495</th>\n",
       "      <td>1/4</td>\n",
       "      <td>dataset_min5.abc</td>\n",
       "      <td>Gyre And Gimble</td>\n",
       "      <td>84</td>\n",
       "      <td>X:84\\nT:Gyre And Gimble\\nM:4/4\\nL:1/4\\nK:Amajor</td>\n",
       "      <td>E|\"A,\"AECE|\"B,m\"FD\"E7\"B,D|\"A,\"CEA3/2B/2|\"E7\"c/...</td>\n",
       "      <td>4/4</td>\n",
       "      <td>A</td>\n",
       "      <td>M:4/4\\nL:1/4\\nK:A\\n|\"A\"|\"Bm\"\"E7\"|\"A\"|\"E7\"\"A\"|\"...</td>\n",
       "      <td>M:4/4\\nL:1/4\\nK:A\\n|\"A\"|\"Bm\"\"E7\"|\"A\"|\"E7\"\"A\"|\"...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     unit_note_length          tuneBook                           title  \\\n",
       "9491              1/4  dataset_min5.abc                       Grandpa's   \n",
       "9492              1/4  dataset_min5.abc  The Girl With The Green Hat On   \n",
       "9493              1/4  dataset_min5.abc                    Green Meadow   \n",
       "9494              1/4  dataset_min5.abc                The Old Grey Cat   \n",
       "9495              1/4  dataset_min5.abc                 Gyre And Gimble   \n",
       "\n",
       "     reference_number                                    original_header  \\\n",
       "9491               78          X:78\\nT:Grandpa's\\nM:4/4\\nL:1/4\\nK:Amajor   \n",
       "9492               79  X:79\\nT:The Girl With The Green Hat On\\nM:4/4\\...   \n",
       "9493               80       X:80\\nT:Green Meadow\\nM:4/4\\nL:1/4\\nK:Dmajor   \n",
       "9494               82   X:82\\nT:The Old Grey Cat\\nM:4/4\\nL:1/4\\nK:Bminor   \n",
       "9495               84    X:84\\nT:Gyre And Gimble\\nM:4/4\\nL:1/4\\nK:Amajor   \n",
       "\n",
       "                                          original_body meter key  \\\n",
       "9491  E/2D/2|\"A,\"CE\"E7\"FG|\"A,\"A/2G/2A/2B/2ce|\"B,m\"dc...   4/4   A   \n",
       "9492  (3E/2F/2G/2|\"A,\"AE\"E7\"E/2F/2E/2D/2|\"A,\"C/2D/2E...   4/4   A   \n",
       "9493  (3A,/2B,/2C/2|\"D\"DD/2E/2F/2D/2F/2A/2|\"G,\"B/2c/...   4/4   D   \n",
       "9494  F|\"B,m\"BBB,B,/2C/2|\"B,m\"D/2C/2D/2E/2F/2E/2F/2^...   4/4  Bm   \n",
       "9495  E|\"A,\"AECE|\"B,m\"FD\"E7\"B,D|\"A,\"CEA3/2B/2|\"E7\"c/...   4/4   A   \n",
       "\n",
       "                                             clean_song  \\\n",
       "9491  M:4/4\\nL:1/4\\nK:A\\n|\"A\"\"E7\"|\"A\"|\"Bm\"\"B7\"|\"E7\"|...   \n",
       "9492  M:4/4\\nL:1/4\\nK:A\\n|\"A\"\"E7\"|\"A\"\"E7\"|\"A\"|\"E7\"|\"...   \n",
       "9493  M:4/4\\nL:1/4\\nK:D\\n|\"D\"|\"G\"\"D\"|\"Em\"|\"Em\"\"A7\"|\"...   \n",
       "9494  M:4/4\\nL:1/4\\nK:Bm\\n|\"Bm\"|\"Bm\"|\"A\"|\"A\"|\"Bm\"|\"B...   \n",
       "9495  M:4/4\\nL:1/4\\nK:A\\n|\"A\"|\"Bm\"\"E7\"|\"A\"|\"E7\"\"A\"|\"...   \n",
       "\n",
       "                                           clean_header  ... \"B#m\" \"B#7\"  \"B\"  \\\n",
       "9491  M:4/4\\nL:1/4\\nK:A\\n|\"A\"\"E7\"|\"A\"|\"Bm\"\"B7\"|\"E7\"|...  ...     0     0    0   \n",
       "9492  M:4/4\\nL:1/4\\nK:A\\n|\"A\"\"E7\"|\"A\"\"E7\"|\"A\"|\"E7\"|\"...  ...     0     0    0   \n",
       "9493  M:4/4\\nL:1/4\\nK:D\\n|\"D\"|\"G\"\"D\"|\"Em\"|\"Em\"\"A7\"|\"...  ...     0     0    0   \n",
       "9494  M:4/4\\nL:1/4\\nK:Bm\\n|\"Bm\"|\"Bm\"|\"A\"|\"A\"|\"Bm\"|\"B...  ...     0     0    0   \n",
       "9495  M:4/4\\nL:1/4\\nK:A\\n|\"A\"|\"Bm\"\"E7\"|\"A\"|\"E7\"\"A\"|\"...  ...     0     0    0   \n",
       "\n",
       "      \"Am\"  \"Ae'\"  \"Aa\"  \"A7\"  \"A#m\"  \"A#7\"  \"A\"  \n",
       "9491     0      0     0     0      0      0    9  \n",
       "9492     0      0     0     0      0      0    9  \n",
       "9493     0      0     0     7      0      0    0  \n",
       "9494     0      0     0     0      0      0    5  \n",
       "9495     0      0     0     0      0      0   12  \n",
       "\n",
       "[5 rows x 75 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "373"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_df[\"clean_header\"].str.len().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1257"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_df[\"clean_body\"].str.len().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab:  \n",
      "\"#'(),-/1234567=ABCDEFG[]^_abcdefgmz|~\n",
      "vocab_size 39\n",
      "silences  516\n"
     ]
    }
   ],
   "source": [
    "bodies = \"\"\n",
    "silences = 0\n",
    "for body in training_data_df[\"clean_body\"]:\n",
    "    if 'z' in body:\n",
    "        silences +=1 \n",
    "    bodies += body+\"\\n\"\n",
    "chars = sorted(list(set(bodies)))\n",
    "vocab_size = len(chars)\n",
    "print('vocab: ',''.join(chars))\n",
    "print('vocab_size',vocab_size)\n",
    "print(\"silences \",silences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of chars: 4062773\n"
     ]
    }
   ],
   "source": [
    "training_data_text = read_file(relative_path,filename_name)\n",
    "\n",
    "print(\"number of chars:\",len(training_data_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_data_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m chars \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(\u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(training_data_text)))\n\u001b[1;32m      2\u001b[0m vocab_size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(chars)\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(chars))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'training_data_text' is not defined"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(training_data_text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0+cu117\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.28.0.dev0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14.2\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import tiktoken\n",
    "\n",
    "print(wandb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dockerfile  docker-compose.yaml  overrides.json\n",
      "README.md   notebooks\t\t requirements.txt\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "nano_path = 'notebooks/nanoGPT'\n",
    "os.chdir(nano_path)\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LICENSE      assets\t      data\t  out-abc-char\twandb\n",
      "README.md    config\t      model.py\t  sample.py\n",
      "__pycache__  configurator.py  older_ckpt  train.py\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset with multiple voices present\n",
    "#length of dataset in characters: 4,149,703\n",
    "#all the unique characters: \n",
    "#\"#'()+,-/123456789:=ABCDEFGKLM[]^_abcdefgmz|~\n",
    "#vocab size: 46\n",
    "#train has 3,734,732 tokens\n",
    "#val has 414,971 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters: 4,062,773\n",
      "all the unique characters: \n",
      "\"#'(),-/123456789:=ABCDEFGKLM[]^_abcdefgmz|~\n",
      "vocab size: 45\n",
      "train has 3,656,495 tokens\n",
      "val has 406,278 tokens\n"
     ]
    }
   ],
   "source": [
    "!python3 data/abc_char/prepare.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_abc_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-abc-char'\n",
      "eval_interval = 10 # keep frequent because we'll overfit\n",
      "eval_iters = 500\n",
      "log_interval = 5 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = True # override via command line if you like\n",
      "wandb_project = 'abc-char'\n",
      "wandb_run_name = 'mini-char-gpt-hd-8-ly-12-embd-320'\n",
      "\n",
      "dataset = 'abc_char'\n",
      "batch_size = 32\n",
      "block_size = 512 # context of up to 512 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 12\n",
      "n_head = 8\n",
      "n_embd = 320\n",
      "dropout = 0.2\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 5000\n",
      "lr_decay_iters = 5000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 5 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "found vocab_size = 45 (inside data/abc_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 14.77M\n",
      "using fused AdamW: True\n",
      "compiling the model... (takes a ~minute)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdavidnogales\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.0 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.14.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/pt-env/notebooks/nanoGPT/wandb/run-20230423_150559-6nouofqi\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmini-char-gpt-hd-8-ly-12-embd-320\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/davidnogales/abc-char\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/davidnogales/abc-char/runs/6nouofqi\u001b[0m\n",
      "step 0: train loss 3.9001, val loss 3.9000\n",
      "[2023-04-23 15:06:53,928] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-23 15:06:54,240] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-23 15:06:54,552] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-23 15:06:54,729] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-23 15:06:54,977] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-23 15:06:55,156] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-23 15:06:55,410] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-23 15:06:55,590] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-23 15:06:55,842] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-23 15:06:56,022] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-23 15:06:56,269] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-23 15:06:56,447] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-23 15:06:56,793] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-23 15:06:56,969] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-23 15:06:57,221] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-23 15:06:57,393] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-23 15:06:57,643] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-23 15:06:57,824] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-23 15:06:58,075] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-23 15:06:58,254] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-23 15:06:58,501] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-23 15:06:58,682] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-23 15:06:58,935] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-23 15:06:59,123] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "iter 0: loss 3.9057, time 67314.27ms, mfu -100.00%\n",
      "iter 5: loss 3.0441, time 6624.54ms, mfu 3.56%\n",
      "step 10: train loss 2.7509, val loss 2.7568\n",
      "saving checkpoint to out-abc-char\n",
      "iter 10: loss 2.7841, time 53265.41ms, mfu 3.25%\n",
      "iter 15: loss 2.5675, time 6653.56ms, mfu 3.28%\n",
      "step 20: train loss 2.4145, val loss 2.4004\n",
      "saving checkpoint to out-abc-char\n",
      "iter 20: loss 2.3765, time 53113.61ms, mfu 2.99%\n",
      "iter 25: loss 2.2523, time 6439.13ms, mfu 3.06%\n",
      "step 30: train loss 2.1839, val loss 2.2089\n",
      "saving checkpoint to out-abc-char\n",
      "iter 30: loss 2.1577, time 51407.20ms, mfu 2.80%\n",
      "iter 35: loss 2.1367, time 6440.71ms, mfu 2.89%\n",
      "step 40: train loss 2.0802, val loss 2.1155\n",
      "saving checkpoint to out-abc-char\n",
      "iter 40: loss 2.1149, time 51407.94ms, mfu 2.64%\n",
      "iter 45: loss 1.9744, time 6441.70ms, mfu 2.74%\n",
      "step 50: train loss 1.9876, val loss 2.0335\n",
      "saving checkpoint to out-abc-char\n",
      "iter 50: loss 1.9493, time 51401.96ms, mfu 2.52%\n",
      "iter 55: loss 1.8599, time 6443.58ms, mfu 2.63%\n",
      "step 60: train loss 1.9045, val loss 1.9203\n",
      "saving checkpoint to out-abc-char\n",
      "iter 60: loss 1.9243, time 51392.64ms, mfu 2.41%\n",
      "iter 65: loss 1.8759, time 6439.72ms, mfu 2.54%\n",
      "step 70: train loss 1.8398, val loss 1.8610\n",
      "saving checkpoint to out-abc-char\n",
      "iter 70: loss 1.8753, time 51436.12ms, mfu 2.33%\n",
      "iter 75: loss 1.9302, time 6441.65ms, mfu 2.46%\n",
      "step 80: train loss 1.8008, val loss 1.8181\n",
      "saving checkpoint to out-abc-char\n",
      "iter 80: loss 1.8887, time 51423.13ms, mfu 2.26%\n",
      "iter 85: loss 1.7968, time 6442.27ms, mfu 2.40%\n",
      "step 90: train loss 1.7503, val loss 1.7718\n",
      "saving checkpoint to out-abc-char\n",
      "iter 90: loss 1.8361, time 51390.48ms, mfu 2.21%\n",
      "iter 95: loss 1.7631, time 6438.66ms, mfu 2.35%\n",
      "step 100: train loss 1.7025, val loss 1.7260\n",
      "saving checkpoint to out-abc-char\n",
      "iter 100: loss 1.7270, time 51413.85ms, mfu 2.16%\n",
      "iter 105: loss 1.6396, time 6440.63ms, mfu 2.31%\n",
      "step 110: train loss 1.6755, val loss 1.6986\n",
      "saving checkpoint to out-abc-char\n",
      "iter 110: loss 1.6776, time 51421.02ms, mfu 2.13%\n",
      "iter 115: loss 1.6848, time 6439.30ms, mfu 2.28%\n",
      "step 120: train loss 1.6904, val loss 1.7093\n",
      "iter 120: loss 1.7472, time 51227.00ms, mfu 2.10%\n",
      "iter 125: loss 1.6479, time 6438.40ms, mfu 2.25%\n",
      "step 130: train loss 1.6272, val loss 1.6495\n",
      "saving checkpoint to out-abc-char\n",
      "iter 130: loss 1.6364, time 51408.29ms, mfu 2.08%\n",
      "iter 135: loss 1.6300, time 6443.89ms, mfu 2.23%\n",
      "step 140: train loss 1.5883, val loss 1.6046\n",
      "saving checkpoint to out-abc-char\n",
      "iter 140: loss 1.5833, time 51420.55ms, mfu 2.06%\n",
      "iter 145: loss 1.6174, time 6441.73ms, mfu 2.22%\n",
      "step 150: train loss 1.5749, val loss 1.5930\n",
      "saving checkpoint to out-abc-char\n",
      "iter 150: loss 1.5358, time 51392.40ms, mfu 2.04%\n",
      "iter 155: loss 1.6487, time 6440.33ms, mfu 2.20%\n",
      "step 160: train loss 1.5451, val loss 1.5713\n",
      "saving checkpoint to out-abc-char\n",
      "iter 160: loss 1.4938, time 51390.20ms, mfu 2.03%\n",
      "iter 165: loss 1.5120, time 6440.88ms, mfu 2.19%\n",
      "step 170: train loss 1.5425, val loss 1.5732\n",
      "iter 170: loss 1.5048, time 51261.66ms, mfu 2.02%\n",
      "iter 175: loss 1.5388, time 6440.59ms, mfu 2.18%\n",
      "step 180: train loss 1.5361, val loss 1.5537\n",
      "saving checkpoint to out-abc-char\n",
      "iter 180: loss 1.5737, time 51415.37ms, mfu 2.01%\n",
      "iter 185: loss 1.5843, time 6441.42ms, mfu 2.17%\n",
      "step 190: train loss 1.5119, val loss 1.5319\n",
      "saving checkpoint to out-abc-char\n",
      "iter 190: loss 1.5200, time 51408.04ms, mfu 2.00%\n",
      "iter 195: loss 1.5185, time 6440.73ms, mfu 2.17%\n",
      "step 200: train loss 1.4834, val loss 1.5052\n",
      "saving checkpoint to out-abc-char\n",
      "iter 200: loss 1.5542, time 51438.94ms, mfu 2.00%\n",
      "iter 205: loss 1.5352, time 6441.37ms, mfu 2.16%\n",
      "step 210: train loss 1.4731, val loss 1.4936\n",
      "saving checkpoint to out-abc-char\n",
      "iter 210: loss 1.5602, time 51423.83ms, mfu 1.99%\n",
      "iter 215: loss 1.4227, time 6440.14ms, mfu 2.16%\n",
      "step 220: train loss 1.4591, val loss 1.4862\n",
      "saving checkpoint to out-abc-char\n",
      "iter 220: loss 1.5407, time 51412.98ms, mfu 1.99%\n",
      "iter 225: loss 1.5412, time 6441.43ms, mfu 2.16%\n",
      "step 230: train loss 1.4518, val loss 1.4722\n",
      "saving checkpoint to out-abc-char\n",
      "iter 230: loss 1.5093, time 51420.33ms, mfu 1.99%\n",
      "iter 235: loss 1.4079, time 6441.22ms, mfu 2.15%\n",
      "step 240: train loss 1.4344, val loss 1.4525\n",
      "saving checkpoint to out-abc-char\n",
      "iter 240: loss 1.4920, time 51426.93ms, mfu 1.98%\n",
      "iter 245: loss 1.4240, time 6440.13ms, mfu 2.15%\n",
      "step 250: train loss 1.4260, val loss 1.4474\n",
      "saving checkpoint to out-abc-char\n",
      "iter 250: loss 1.4485, time 51381.26ms, mfu 1.98%\n",
      "iter 255: loss 1.4400, time 6441.18ms, mfu 2.15%\n",
      "step 260: train loss 1.4078, val loss 1.4355\n",
      "saving checkpoint to out-abc-char\n",
      "iter 260: loss 1.4158, time 51402.96ms, mfu 1.98%\n",
      "iter 265: loss 1.4043, time 6441.81ms, mfu 2.15%\n",
      "step 270: train loss 1.3923, val loss 1.4087\n",
      "saving checkpoint to out-abc-char\n",
      "iter 270: loss 1.3486, time 51419.40ms, mfu 1.98%\n",
      "iter 275: loss 1.3943, time 6440.62ms, mfu 2.15%\n",
      "step 280: train loss 1.3548, val loss 1.3922\n",
      "saving checkpoint to out-abc-char\n",
      "iter 280: loss 1.4486, time 51395.89ms, mfu 1.98%\n",
      "iter 285: loss 1.4645, time 6444.36ms, mfu 2.15%\n",
      "step 290: train loss 1.3468, val loss 1.3694\n",
      "saving checkpoint to out-abc-char\n",
      "iter 290: loss 1.3822, time 51398.97ms, mfu 1.98%\n",
      "iter 295: loss 1.3704, time 6440.31ms, mfu 2.15%\n",
      "step 300: train loss 1.3231, val loss 1.3331\n",
      "saving checkpoint to out-abc-char\n",
      "iter 300: loss 1.3556, time 51432.80ms, mfu 1.98%\n",
      "iter 305: loss 1.2637, time 6440.21ms, mfu 2.15%\n",
      "step 310: train loss 1.2909, val loss 1.3152\n",
      "saving checkpoint to out-abc-char\n",
      "iter 310: loss 1.2885, time 51414.03ms, mfu 1.98%\n",
      "iter 315: loss 1.3012, time 6442.39ms, mfu 2.14%\n",
      "step 320: train loss 1.2610, val loss 1.2912\n",
      "saving checkpoint to out-abc-char\n",
      "iter 320: loss 1.2560, time 51409.02ms, mfu 1.98%\n",
      "iter 325: loss 1.3785, time 6439.65ms, mfu 2.14%\n",
      "step 330: train loss 1.2496, val loss 1.2746\n",
      "saving checkpoint to out-abc-char\n",
      "iter 330: loss 1.3005, time 51443.13ms, mfu 1.98%\n",
      "iter 335: loss 1.2658, time 6441.48ms, mfu 2.14%\n",
      "step 340: train loss 1.2183, val loss 1.2418\n",
      "saving checkpoint to out-abc-char\n",
      "iter 340: loss 1.2156, time 51411.72ms, mfu 1.98%\n",
      "iter 345: loss 1.3001, time 6441.17ms, mfu 2.14%\n",
      "step 350: train loss 1.1968, val loss 1.2219\n",
      "saving checkpoint to out-abc-char\n",
      "iter 350: loss 1.2726, time 51397.24ms, mfu 1.98%\n",
      "iter 355: loss 1.2325, time 6441.09ms, mfu 2.14%\n",
      "step 360: train loss 1.1728, val loss 1.2026\n",
      "saving checkpoint to out-abc-char\n",
      "iter 360: loss 1.2257, time 51410.41ms, mfu 1.98%\n",
      "iter 365: loss 1.1359, time 6440.96ms, mfu 2.14%\n",
      "step 370: train loss 1.1624, val loss 1.2042\n",
      "iter 370: loss 1.2259, time 51231.32ms, mfu 1.98%\n",
      "iter 375: loss 1.1652, time 6441.56ms, mfu 2.14%\n",
      "step 380: train loss 1.1461, val loss 1.1667\n",
      "saving checkpoint to out-abc-char\n",
      "iter 380: loss 1.1952, time 51396.92ms, mfu 1.98%\n",
      "iter 385: loss 1.1571, time 6442.40ms, mfu 2.14%\n",
      "step 390: train loss 1.1140, val loss 1.1424\n",
      "saving checkpoint to out-abc-char\n",
      "iter 390: loss 1.1049, time 51420.69ms, mfu 1.97%\n",
      "iter 395: loss 1.1249, time 6443.61ms, mfu 2.14%\n",
      "step 400: train loss 1.0943, val loss 1.1263\n",
      "saving checkpoint to out-abc-char\n",
      "iter 400: loss 1.1294, time 51430.24ms, mfu 1.97%\n",
      "iter 405: loss 1.1360, time 6443.47ms, mfu 2.14%\n",
      "step 410: train loss 1.0830, val loss 1.1235\n",
      "saving checkpoint to out-abc-char\n",
      "iter 410: loss 1.1510, time 51407.82ms, mfu 1.97%\n",
      "iter 415: loss 1.1729, time 6439.08ms, mfu 2.14%\n",
      "step 420: train loss 1.0690, val loss 1.1097\n",
      "saving checkpoint to out-abc-char\n",
      "iter 420: loss 1.1203, time 51381.81ms, mfu 1.97%\n",
      "iter 425: loss 1.0696, time 6437.75ms, mfu 2.14%\n",
      "step 430: train loss 1.0456, val loss 1.0795\n",
      "saving checkpoint to out-abc-char\n",
      "iter 430: loss 1.1143, time 51422.52ms, mfu 1.97%\n",
      "iter 435: loss 1.1005, time 6440.93ms, mfu 2.14%\n",
      "step 440: train loss 1.0305, val loss 1.0669\n",
      "saving checkpoint to out-abc-char\n",
      "iter 440: loss 1.0540, time 51417.96ms, mfu 1.97%\n",
      "iter 445: loss 1.1029, time 6438.57ms, mfu 2.14%\n",
      "step 450: train loss 1.0141, val loss 1.0521\n",
      "saving checkpoint to out-abc-char\n",
      "iter 450: loss 1.0828, time 51364.62ms, mfu 1.97%\n",
      "iter 455: loss 0.9987, time 6438.48ms, mfu 2.14%\n",
      "step 460: train loss 0.9890, val loss 1.0302\n",
      "saving checkpoint to out-abc-char\n",
      "iter 460: loss 1.0912, time 51395.37ms, mfu 1.97%\n",
      "iter 465: loss 1.0157, time 6440.97ms, mfu 2.14%\n",
      "step 470: train loss 0.9880, val loss 1.0298\n",
      "saving checkpoint to out-abc-char\n",
      "iter 470: loss 1.0178, time 51396.18ms, mfu 1.97%\n",
      "iter 475: loss 0.9973, time 6439.54ms, mfu 2.14%\n",
      "step 480: train loss 0.9616, val loss 0.9994\n",
      "saving checkpoint to out-abc-char\n",
      "iter 480: loss 1.0141, time 51372.02ms, mfu 1.97%\n",
      "iter 485: loss 0.9703, time 6439.90ms, mfu 2.14%\n",
      "step 490: train loss 0.9433, val loss 0.9858\n",
      "saving checkpoint to out-abc-char\n",
      "iter 490: loss 1.0389, time 51380.74ms, mfu 1.97%\n",
      "iter 495: loss 0.9837, time 6440.75ms, mfu 2.14%\n",
      "step 500: train loss 0.9280, val loss 0.9723\n",
      "saving checkpoint to out-abc-char\n",
      "iter 500: loss 0.9738, time 51391.13ms, mfu 1.97%\n",
      "iter 505: loss 0.9174, time 6439.26ms, mfu 2.14%\n",
      "step 510: train loss 0.9115, val loss 0.9538\n",
      "saving checkpoint to out-abc-char\n",
      "iter 510: loss 0.9759, time 51369.63ms, mfu 1.97%\n",
      "iter 515: loss 0.9523, time 6440.41ms, mfu 2.14%\n",
      "step 520: train loss 0.9077, val loss 0.9496\n",
      "saving checkpoint to out-abc-char\n",
      "iter 520: loss 1.0154, time 51457.84ms, mfu 1.97%\n",
      "iter 525: loss 0.9000, time 6441.55ms, mfu 2.14%\n",
      "step 530: train loss 0.8965, val loss 0.9426\n",
      "saving checkpoint to out-abc-char\n",
      "iter 530: loss 0.9657, time 51460.71ms, mfu 1.97%\n",
      "iter 535: loss 0.8930, time 6438.70ms, mfu 2.14%\n",
      "step 540: train loss 0.8727, val loss 0.9121\n",
      "saving checkpoint to out-abc-char\n",
      "iter 540: loss 0.9378, time 51375.36ms, mfu 1.97%\n",
      "iter 545: loss 0.8713, time 6442.86ms, mfu 2.14%\n",
      "step 550: train loss 0.8632, val loss 0.9101\n",
      "saving checkpoint to out-abc-char\n",
      "iter 550: loss 0.9154, time 52093.14ms, mfu 1.97%\n",
      "iter 555: loss 0.9150, time 6627.41ms, mfu 2.13%\n",
      "step 560: train loss 0.8483, val loss 0.8961\n",
      "saving checkpoint to out-abc-char\n",
      "iter 560: loss 0.9697, time 53062.65ms, mfu 1.96%\n",
      "iter 565: loss 0.8499, time 6628.82ms, mfu 2.12%\n",
      "step 570: train loss 0.8378, val loss 0.8827\n",
      "saving checkpoint to out-abc-char\n",
      "iter 570: loss 0.8544, time 53073.69ms, mfu 1.95%\n",
      "iter 575: loss 0.8551, time 6638.12ms, mfu 2.11%\n",
      "step 580: train loss 0.8171, val loss 0.8580\n",
      "saving checkpoint to out-abc-char\n",
      "iter 580: loss 0.8297, time 51360.62ms, mfu 1.95%\n",
      "iter 585: loss 0.8512, time 6439.92ms, mfu 2.12%\n",
      "step 590: train loss 0.8083, val loss 0.8565\n",
      "saving checkpoint to out-abc-char\n",
      "iter 590: loss 0.8335, time 51378.38ms, mfu 1.95%\n",
      "iter 595: loss 0.8392, time 6440.13ms, mfu 2.12%\n",
      "step 600: train loss 0.7934, val loss 0.8390\n",
      "saving checkpoint to out-abc-char\n",
      "iter 600: loss 0.7729, time 51363.11ms, mfu 1.96%\n",
      "iter 605: loss 0.8119, time 6439.77ms, mfu 2.13%\n",
      "step 610: train loss 0.7879, val loss 0.8455\n",
      "iter 610: loss 0.8448, time 51171.47ms, mfu 1.96%\n",
      "iter 615: loss 0.8517, time 6441.17ms, mfu 2.13%\n",
      "step 620: train loss 0.7774, val loss 0.8340\n",
      "saving checkpoint to out-abc-char\n",
      "iter 620: loss 0.8589, time 51352.99ms, mfu 1.96%\n",
      "iter 625: loss 0.7850, time 6438.40ms, mfu 2.13%\n",
      "step 630: train loss 0.7603, val loss 0.8125\n",
      "saving checkpoint to out-abc-char\n",
      "iter 630: loss 0.7764, time 51360.38ms, mfu 1.97%\n",
      "iter 635: loss 0.7489, time 6439.78ms, mfu 2.14%\n",
      "step 640: train loss 0.7462, val loss 0.8039\n",
      "saving checkpoint to out-abc-char\n",
      "iter 640: loss 0.7860, time 51356.67ms, mfu 1.97%\n",
      "iter 645: loss 0.8075, time 6439.59ms, mfu 2.14%\n",
      "step 650: train loss 0.7323, val loss 0.7903\n",
      "saving checkpoint to out-abc-char\n",
      "iter 650: loss 0.7503, time 51358.54ms, mfu 1.97%\n",
      "iter 655: loss 0.7640, time 6440.74ms, mfu 2.14%\n",
      "step 660: train loss 0.7168, val loss 0.7711\n",
      "saving checkpoint to out-abc-char\n",
      "iter 660: loss 0.7602, time 51382.58ms, mfu 1.97%\n",
      "iter 665: loss 0.7392, time 6439.63ms, mfu 2.14%\n",
      "step 670: train loss 0.7048, val loss 0.7614\n",
      "saving checkpoint to out-abc-char\n",
      "iter 670: loss 0.7265, time 51359.45ms, mfu 1.97%\n",
      "iter 675: loss 0.7503, time 6438.45ms, mfu 2.14%\n",
      "step 680: train loss 0.6940, val loss 0.7515\n",
      "saving checkpoint to out-abc-char\n",
      "iter 680: loss 0.7553, time 51361.34ms, mfu 1.97%\n",
      "iter 685: loss 0.7705, time 6439.72ms, mfu 2.14%\n",
      "step 690: train loss 0.6801, val loss 0.7424\n",
      "saving checkpoint to out-abc-char\n",
      "iter 690: loss 0.7232, time 51370.61ms, mfu 1.97%\n",
      "iter 695: loss 0.7340, time 6438.53ms, mfu 2.14%\n",
      "step 700: train loss 0.6682, val loss 0.7245\n",
      "saving checkpoint to out-abc-char\n",
      "iter 700: loss 0.7240, time 51371.22ms, mfu 1.97%\n",
      "iter 705: loss 0.7097, time 6439.53ms, mfu 2.14%\n",
      "step 710: train loss 0.6557, val loss 0.7146\n",
      "saving checkpoint to out-abc-char\n",
      "iter 710: loss 0.6444, time 51370.74ms, mfu 1.97%\n",
      "iter 715: loss 0.6955, time 6441.21ms, mfu 2.14%\n",
      "step 720: train loss 0.6508, val loss 0.7086\n",
      "saving checkpoint to out-abc-char\n",
      "iter 720: loss 0.6907, time 51348.02ms, mfu 1.97%\n",
      "iter 725: loss 0.6665, time 6440.63ms, mfu 2.14%\n",
      "step 730: train loss 0.6345, val loss 0.6917\n",
      "saving checkpoint to out-abc-char\n",
      "iter 730: loss 0.6733, time 51903.46ms, mfu 1.97%\n",
      "iter 735: loss 0.6425, time 6653.11ms, mfu 2.13%\n",
      "step 740: train loss 0.6296, val loss 0.6899\n",
      "saving checkpoint to out-abc-char\n",
      "iter 740: loss 0.6311, time 53177.27ms, mfu 1.96%\n",
      "iter 745: loss 0.6998, time 6639.73ms, mfu 2.12%\n",
      "step 750: train loss 0.6161, val loss 0.6768\n",
      "saving checkpoint to out-abc-char\n",
      "iter 750: loss 0.6706, time 52989.10ms, mfu 1.95%\n",
      "iter 755: loss 0.6420, time 6631.64ms, mfu 2.11%\n",
      "step 760: train loss 0.6103, val loss 0.6743\n",
      "saving checkpoint to out-abc-char\n",
      "iter 760: loss 0.6438, time 52954.36ms, mfu 1.95%\n",
      "iter 765: loss 0.6603, time 6674.76ms, mfu 2.10%\n",
      "step 770: train loss 0.6006, val loss 0.6641\n",
      "saving checkpoint to out-abc-char\n",
      "iter 770: loss 0.6779, time 51377.12ms, mfu 1.94%\n",
      "iter 775: loss 0.6452, time 6439.66ms, mfu 2.11%\n",
      "step 780: train loss 0.5863, val loss 0.6483\n",
      "saving checkpoint to out-abc-char\n",
      "iter 780: loss 0.5864, time 51363.67ms, mfu 1.95%\n",
      "iter 785: loss 0.6302, time 6438.69ms, mfu 2.12%\n",
      "step 790: train loss 0.5794, val loss 0.6397\n",
      "saving checkpoint to out-abc-char\n",
      "iter 790: loss 0.6204, time 51383.55ms, mfu 1.95%\n",
      "iter 795: loss 0.5928, time 6439.76ms, mfu 2.12%\n",
      "step 800: train loss 0.5743, val loss 0.6427\n",
      "iter 800: loss 0.5836, time 51210.68ms, mfu 1.96%\n",
      "iter 805: loss 0.6578, time 6437.53ms, mfu 2.13%\n",
      "step 810: train loss 0.5590, val loss 0.6284\n",
      "saving checkpoint to out-abc-char\n",
      "iter 810: loss 0.6238, time 51349.56ms, mfu 1.96%\n",
      "iter 815: loss 0.5729, time 6439.01ms, mfu 2.13%\n",
      "step 820: train loss 0.5474, val loss 0.6165\n",
      "saving checkpoint to out-abc-char\n",
      "iter 820: loss 0.5742, time 51371.01ms, mfu 1.96%\n",
      "iter 825: loss 0.5531, time 6440.46ms, mfu 2.13%\n",
      "step 830: train loss 0.5448, val loss 0.6181\n",
      "iter 830: loss 0.6005, time 51209.90ms, mfu 1.97%\n",
      "iter 835: loss 0.5613, time 6440.82ms, mfu 2.13%\n",
      "step 840: train loss 0.5386, val loss 0.6116\n",
      "saving checkpoint to out-abc-char\n",
      "iter 840: loss 0.5274, time 51355.52ms, mfu 1.97%\n",
      "iter 845: loss 0.5951, time 6441.19ms, mfu 2.14%\n",
      "step 850: train loss 0.5296, val loss 0.6074\n",
      "saving checkpoint to out-abc-char\n",
      "iter 850: loss 0.5669, time 51336.48ms, mfu 1.97%\n",
      "iter 855: loss 0.5629, time 6440.13ms, mfu 2.14%\n",
      "step 860: train loss 0.5253, val loss 0.6035\n",
      "saving checkpoint to out-abc-char\n",
      "iter 860: loss 0.5713, time 51388.67ms, mfu 1.97%\n",
      "iter 865: loss 0.5541, time 6440.24ms, mfu 2.14%\n",
      "step 870: train loss 0.5142, val loss 0.5956\n",
      "saving checkpoint to out-abc-char\n",
      "iter 870: loss 0.5527, time 51359.73ms, mfu 1.97%\n",
      "iter 875: loss 0.5586, time 6442.47ms, mfu 2.14%\n",
      "step 880: train loss 0.5139, val loss 0.5995\n",
      "iter 880: loss 0.5579, time 51176.45ms, mfu 1.97%\n",
      "iter 885: loss 0.5610, time 6437.34ms, mfu 2.14%\n",
      "step 890: train loss 0.4993, val loss 0.5793\n",
      "saving checkpoint to out-abc-char\n",
      "iter 890: loss 0.5344, time 51368.67ms, mfu 1.97%\n",
      "iter 895: loss 0.5283, time 6438.28ms, mfu 2.14%\n",
      "step 900: train loss 0.4945, val loss 0.5782\n",
      "saving checkpoint to out-abc-char\n",
      "iter 900: loss 0.5652, time 51364.73ms, mfu 1.97%\n",
      "iter 905: loss 0.5322, time 6438.94ms, mfu 2.14%\n",
      "step 910: train loss 0.4868, val loss 0.5753\n",
      "saving checkpoint to out-abc-char\n",
      "iter 910: loss 0.5234, time 51366.62ms, mfu 1.97%\n",
      "iter 915: loss 0.4906, time 6439.86ms, mfu 2.14%\n",
      "step 920: train loss 0.4848, val loss 0.5739\n",
      "saving checkpoint to out-abc-char\n",
      "iter 920: loss 0.5330, time 51374.37ms, mfu 1.97%\n",
      "iter 925: loss 0.5271, time 6438.54ms, mfu 2.14%\n",
      "step 930: train loss 0.4753, val loss 0.5667\n",
      "saving checkpoint to out-abc-char\n",
      "iter 930: loss 0.5359, time 52132.17ms, mfu 1.97%\n",
      "iter 935: loss 0.5034, time 6668.79ms, mfu 2.13%\n",
      "step 940: train loss 0.4717, val loss 0.5659\n",
      "saving checkpoint to out-abc-char\n",
      "iter 940: loss 0.5119, time 53053.96ms, mfu 1.96%\n",
      "iter 945: loss 0.4912, time 6634.19ms, mfu 2.12%\n",
      "step 950: train loss 0.4621, val loss 0.5539\n",
      "saving checkpoint to out-abc-char\n",
      "iter 950: loss 0.5099, time 52982.15ms, mfu 1.95%\n",
      "iter 955: loss 0.4806, time 6628.23ms, mfu 2.11%\n",
      "step 960: train loss 0.4584, val loss 0.5572\n",
      "iter 960: loss 0.5180, time 52119.57ms, mfu 1.95%\n",
      "iter 965: loss 0.4899, time 6438.33ms, mfu 2.12%\n",
      "step 970: train loss 0.4444, val loss 0.5429\n",
      "saving checkpoint to out-abc-char\n",
      "iter 970: loss 0.4926, time 51374.34ms, mfu 1.95%\n",
      "iter 975: loss 0.5054, time 6441.88ms, mfu 2.12%\n",
      "step 980: train loss 0.4416, val loss 0.5478\n",
      "iter 980: loss 0.4634, time 51209.65ms, mfu 1.96%\n",
      "iter 985: loss 0.4819, time 6441.27ms, mfu 2.13%\n",
      "step 990: train loss 0.4328, val loss 0.5377\n",
      "saving checkpoint to out-abc-char\n",
      "iter 990: loss 0.5089, time 51366.92ms, mfu 1.96%\n",
      "iter 995: loss 0.4823, time 6440.58ms, mfu 2.13%\n",
      "step 1000: train loss 0.4266, val loss 0.5395\n",
      "iter 1000: loss 0.4530, time 51196.07ms, mfu 1.96%\n",
      "iter 1005: loss 0.4557, time 6438.21ms, mfu 2.13%\n",
      "step 1010: train loss 0.4230, val loss 0.5317\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1010: loss 0.4567, time 51357.26ms, mfu 1.97%\n",
      "iter 1015: loss 0.4728, time 6438.57ms, mfu 2.13%\n",
      "step 1020: train loss 0.4143, val loss 0.5277\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1020: loss 0.4492, time 51374.16ms, mfu 1.97%\n",
      "iter 1025: loss 0.4510, time 6438.71ms, mfu 2.14%\n",
      "step 1030: train loss 0.4097, val loss 0.5227\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1030: loss 0.4515, time 51358.95ms, mfu 1.97%\n",
      "iter 1035: loss 0.4551, time 6438.07ms, mfu 2.14%\n",
      "step 1040: train loss 0.4059, val loss 0.5298\n",
      "iter 1040: loss 0.4415, time 51191.16ms, mfu 1.97%\n",
      "iter 1045: loss 0.4496, time 6439.05ms, mfu 2.14%\n",
      "step 1050: train loss 0.3958, val loss 0.5162\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1050: loss 0.4479, time 51362.69ms, mfu 1.97%\n",
      "iter 1055: loss 0.4356, time 6441.29ms, mfu 2.14%\n",
      "step 1060: train loss 0.3847, val loss 0.5120\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1060: loss 0.4519, time 51360.56ms, mfu 1.97%\n",
      "iter 1065: loss 0.4052, time 6439.64ms, mfu 2.14%\n",
      "step 1070: train loss 0.3843, val loss 0.5121\n",
      "iter 1070: loss 0.4268, time 51190.43ms, mfu 1.97%\n",
      "iter 1075: loss 0.4504, time 6440.80ms, mfu 2.14%\n",
      "step 1080: train loss 0.3770, val loss 0.5053\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1080: loss 0.4292, time 51352.98ms, mfu 1.97%\n",
      "iter 1085: loss 0.3959, time 6441.35ms, mfu 2.14%\n",
      "step 1090: train loss 0.3714, val loss 0.5021\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1090: loss 0.4010, time 51383.86ms, mfu 1.97%\n",
      "iter 1095: loss 0.4052, time 6437.89ms, mfu 2.14%\n",
      "step 1100: train loss 0.3645, val loss 0.5068\n",
      "iter 1100: loss 0.4194, time 51204.79ms, mfu 1.97%\n",
      "iter 1105: loss 0.4145, time 6442.31ms, mfu 2.14%\n",
      "step 1110: train loss 0.3604, val loss 0.5008\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1110: loss 0.4076, time 51350.78ms, mfu 1.97%\n",
      "iter 1115: loss 0.3950, time 6439.46ms, mfu 2.14%\n",
      "step 1120: train loss 0.3484, val loss 0.4818\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1120: loss 0.3818, time 51371.70ms, mfu 1.97%\n",
      "iter 1125: loss 0.4267, time 6440.47ms, mfu 2.14%\n",
      "step 1130: train loss 0.3467, val loss 0.4910\n",
      "iter 1130: loss 0.3832, time 51211.63ms, mfu 1.97%\n",
      "iter 1135: loss 0.4023, time 6442.42ms, mfu 2.14%\n",
      "step 1140: train loss 0.3353, val loss 0.4767\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1140: loss 0.3894, time 51384.89ms, mfu 1.97%\n",
      "iter 1145: loss 0.3852, time 6441.48ms, mfu 2.14%\n",
      "step 1150: train loss 0.3280, val loss 0.4758\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1150: loss 0.3863, time 51363.73ms, mfu 1.97%\n",
      "iter 1155: loss 0.3607, time 6441.60ms, mfu 2.14%\n",
      "step 1160: train loss 0.3259, val loss 0.4787\n",
      "iter 1160: loss 0.3747, time 51214.30ms, mfu 1.97%\n",
      "iter 1165: loss 0.3929, time 6438.30ms, mfu 2.14%\n",
      "step 1170: train loss 0.3155, val loss 0.4707\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1170: loss 0.3641, time 51358.49ms, mfu 1.97%\n",
      "iter 1175: loss 0.3450, time 6441.99ms, mfu 2.14%\n",
      "step 1180: train loss 0.3071, val loss 0.4650\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1180: loss 0.3634, time 51364.76ms, mfu 1.97%\n",
      "iter 1185: loss 0.3653, time 6439.32ms, mfu 2.14%\n",
      "step 1190: train loss 0.3047, val loss 0.4604\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1190: loss 0.3482, time 51368.09ms, mfu 1.97%\n",
      "iter 1195: loss 0.3590, time 6440.69ms, mfu 2.14%\n",
      "step 1200: train loss 0.2935, val loss 0.4507\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1200: loss 0.3477, time 53727.27ms, mfu 1.97%\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 293, in <module>\n",
      "    scaler.scale(loss).backward()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\", line 487, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\", line 200, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[31m(failed 255).\u001b[0m Press Control-C to abort syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: | 0.033 MB of 0.033 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       iter ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁█████████████████████████████▇▇▇▇▇▇▇▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        mfu ▁███████████████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/loss █▅▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val/loss █▅▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       iter 1200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr 0.00088\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        mfu 2.14325\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/loss 0.29347\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val/loss 0.45071\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mmini-char-gpt-hd-8-ly-12-embd-320\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/davidnogales/abc-char/runs/6nouofqi\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230423_150559-6nouofqi/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 train.py config/train_abc_char.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test key with most occurrences: G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding: out_dir = out-abc-char\n",
      "Overriding: start = M:4/4L:1/4K:G|\"G\"|\"C\"|\"D\"|\"D\"|\"G\"|\"C\"|\"D\"|\"G\"|]\n",
      "number of parameters: 14.77M\n",
      "abc_char\n",
      "Loading meta from data/abc_char/meta.pkl...\n",
      "M:4/4L:1/4K:G|\"G\"|\"C\"|\"D\"|\"D\"|\"G\"|\"C\"|\"D\"|\"G\"|]\n",
      "B/2A/2|\"G\"G/2B/2DG/2B/2|A/2B/2DG/2B/2|AB/2A/2G2B/2A/2|\"C\"G/2B/2DG/2B/2|Ac/2A/2B/2A/2G/2E/2|\"D\"D4|\"D\"d/2^c/2d/2e/2f/2e/2d/2c/2|\"D\"dd/2A/2^GA/2B/2|A/2B/2A/2G/2F/2DF/2A/2|\"G\"B/2^A/2B/2^G/2B/2A/2B/2|\"C\"c/2_c/2e/2d/2c/2d/2e/2d/2|\"D\"f/2=c/2d/2e/2f/2e/2d/2c/2|\"G\"B/2^A/2B/2G/2\"C\"c/2A/2G/2E/2|\"D\"D4|\"G\"GGB/2A/2G/2A/2|B/2G/2B/2A/2G/2A/2B/2|\"C\"c/2=B/2c/2d/2e/2d/2c/2e/2|\"D\"=f/2=c/2d/2f/2e/2d/2c/2|\"G\"BGB/2A/2G/2A/2|B/2G/2B/2A/2G/2A/2B/2|\"C\"c/2=B/2c/2d/2e/2d/2c/2e/2|\"D\"d4|\"G\"GGBB/2A/2|\"C\"GGcc/2A/2G/2E/2|\"D\"D2\n",
      "---------------\n",
      "M:4/4L:1/4K:G|\"G\"|\"C\"|\"D\"|\"D\"|\"G\"|\"C\"|\"D\"|\"G\"|]\n",
      "B/2c/2|\"G\"d/2d/2B/2GB|\"C\"cCE2|\"D\"A,/2C/2F/2G/2AF/2E/2|\"D\"A/2cA/2FE|\"G\"D/2C/2B,/2G,/2F,/2G,B,|\"D\"A,/2C/2E/2D/2E/2F/2E/2F/2A/2|\"G\"G/2A/2B/2c/2dc/2d/2|\"C\"e/2d/2c/2d/2e/2f/2g/2e/2|\"D\"d/2c/2B/2A/2G/2A/2F/2E/2|\"G\"GGGB/2c/2|\"G\"d/2B/2G/2B/2dc/2d/2|\"C\"e/2d/2c/2B/2dc/2d/2|\"D\"B/2A/2G/2A/2F/2D/2F/2|\"G\"GGGB/2c/2|\"G\"d/2B/2G/2B/2dc/2d/2|\"C\"e/2d/2c/2e/2f/2g/2e/2|\"D\"d/2c/2B/2A/2D2|\"G\"GG/2F/2G|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:G\n",
      "|\"G\"|\"G\"|\"C\"\"G\"|\"Am\"\"D7\"|\"G\"|\"C\"\"G\"|\"C\"\"D7\"|\"G\"|\"G\"|\"C\"|\"G\"|\"A7\"\"D7\"|\"G\"|\"C\"|\"G\"|\"Am\"\"D7\"|\"G\"\"D7\"|\"G\"\n",
      "---------------\n",
      "M:4/4L:1/4K:G|\"G\"|\"C\"|\"D\"|\"D\"|\"G\"|\"C\"|\"D\"|\"G\"|]\n",
      "B/2c/2|\"G\"d3/2B/2GA|G/2B/2DG/2A/2|\"C\"BGB/2c/2|\"D\"dA3/2c/2|\"D\"B/2c/2DD|\"G\"d3/2c/2Bd|G/2B/2DG/2A/2|\"C\"B/2c/2A/2G/2A/2B/2c/2|\"D\"d/2c/2B/2A/2G/2F/2E/2|\"G\"DGG/2A/2|\"C\"B/2c/2A/2B/2G/2A/2B/2c/2|\"D\"d/2c/2B/2A/2G/2F/2E/2F/2|\"G\"GG/2A/2G|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:D\n",
      "|\"D\"|\"D\"|\"G\"|\"Em\"|\"A\"\"D\"|\"D\"|\"G\"|\"A\"|\"D\"|\"D\"|\"A\"\"D\"|\"D\"\"G\"|\"Em\"\"A7\"|\"D\"|\"D\"|\"G\"|\"A\"\"D\"|\"D\"\"G\"|\"Em\"\"A7\"|\"D\"|\"G\"|\"D\"\"G\"|\"A7\"\"D\"|]\n",
      "\"D\"D/2F/2A/2F/2D/2F/2A/2B/2c/2|\"D\"d/2e/2d/2c/2d/2A/2F/2A/2|\"G\"G/2F/2G/2B/2G/2B/2G/2B/2G/2|\"Em\"B/2A/2E/2F/2G/2B/2G/2E/2|\"A\"A/2\n",
      "---------------\n",
      "M:4/4L:1/4K:G|\"G\"|\"C\"|\"D\"|\"D\"|\"G\"|\"C\"|\"D\"|\"G\"|]\n",
      "\"G\"g2|\"G\"bb3/2a/2gb|\"C\"c'c'3/2b/2ag|\"D\"f3/2e/2df|\"D\"aa2a3/2a/2|\"G\"bb2b/2a/2g|\"C\"e3/2d/2ce|\"D\"d4|\"G\"g3/2e/2dcd|\"C\"ee2e|\"D\"d3/2c/2Bd|\"G\"B4|\"C\"cG2|\"D\"AF/2A/2dA|\"G\"BG2|\"C\"cG/2c/2ec|\"D\"d2d3/2A/2|\"G\"GB/2d/2G|\"C\"ee/2g/2ag|\"D\"f/2a/2g/2f/2e/2d/2|\"G\"GBG|\"C\"cG/2c/2ec|\"D\"d/2f/2a/2g/2f/2e/2|\"G\"d/2g/2dBG|\"C\"c4|\"D\"AF/2A/2dA|\"G\"GB/2d/2G/2d/2B/2|\"C\"ee2e|\"D\"AF/2A/2dA|\"G\"GB/2d/2G/2d/2B/2|\"C\"ee/2g/2a|\"D\"f/2a/2g/2f/2e/2d/2|\"C\"ce3|\"D\"AF/2A/2dA|\"G\"GB/2d/2G/2d/2B/2|\"C\"ee/2g/2a/2g/2|\"D\"f/2a/2g/2f/2e/2d/2|\"G\"BGG|]\n",
      "\n",
      "M:4/\n",
      "---------------\n",
      "M:4/4L:1/4K:G|\"G\"|\"C\"|\"D\"|\"D\"|\"G\"|\"C\"|\"D\"|\"G\"|]\n",
      "\"G\"GBB|\"C\"ccc|\"D\"d/2c/2B/2A/2|\"D\"dD3/2D/2|\"G\"GBB|\"C\"c2G|\"D\"FAA|\"G\"G2|D|\"G\"GBd|\"C\"e2d|\"D\"A3/2B/2A|\"G\"GBd|\"C\"e2d|\"D\"A3/2B/2A|\"G\"GBd|\"C\"e2d|\"D\"A3/2B/2A|\"G\"GBd|\"C\"e2d|\"D\"A3/2B/2A|\"G\"G2|]\n",
      "\n",
      "M:3/4\n",
      "L:1/4\n",
      "K:A\n",
      "|\"A\"|\"A\"|\"B7\"|\"E7\"|\"A\"|\"E\"|\"A\"|\"E\"|\"A\"|\"D\"|\"A\"|\"Bm\"|\"E\"|\"A\"|\"E\"|\"A\"|\"D\"|\"A\"|\"Bm\"|\"E7\"|\"A\"|\"D\"|\"A\"|\"E\"|\"A\"|\"B7\"|\"E\"|\"E\"|\"A\"|]\n",
      "E|\"A\"c2c|\"A\"edc|\"B7\"B2D|\"E7\"EFG|\"A\"A3/2B/2c|\"E\"B2E|\"A\"c2c|\"A\"edc|\"E\"B2G|\"A\"A2E/2F/2|\"D\"GFF|\"A\"EAc|\"Bm\"B2A|\"E\"GFG|\"A\"A2E/2F/2|\"E\"GFE|\"A\"A2|E|\"A\"E2E/2F/2|\"Bm\"GFF|\"E7\"EBc|\"A\"A3/\n",
      "---------------\n",
      "M:4/4L:1/4K:G|\"G\"|\"C\"|\"D\"|\"D\"|\"G\"|\"C\"|\"D\"|\"G\"|]\n",
      "B/4F/2|\"G\"G/2B/2B/2A/2G/2|G/2B/2A/2GD|\"C\"E3/2E/2G/2E/2|\"D\"A/2D/2D2B/2|\"D\"A/2D/2E/2F/2D/2E/2|\"G\"G/2B/2B/2A/2G/2D/2|\"C\"E3/2E/2G/2E/2|\"D\"F/2D/2DD/2E/2|\"G\"GG/2G/2B/2B/2|\"C\"c/2B/2A/2B/2G/2A/2G/2|\"D\"F/2D/2DD/2E/2F/2|\"G\"GG/2G/2G|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:G\n",
      "|\"G\"|\"G\"\"C\"|\"G\"|\"G\"|\"C\"|\"D\"|\"G\"|\"C\"\"G\"|\"D\"|\"G\"|\"G\"|\"C\"|\"D\"|\"G\"|]\n",
      "G/2A/2|\"G\"BG/2A/2BG/2A/2|B/2c/2d/2G/2A/2B/2A/2G/2|\"G\"BG/2A/2\"C\"BG/2A/2|\"G\"B/2c/2d/2G/2A/2B/2c/2|\"G\"d/2e/2d/2c/2BG/2A/2|\"C\"BG/2A/2BG/2A/2|\"D\"B/2G/2A/2B/2c/2d/2e/2|\"G\"f/2e/2d/2c/2BG|\"C\"AG/2A/2B/2\n",
      "---------------\n",
      "M:4/4L:1/4K:G|\"G\"|\"C\"|\"D\"|\"D\"|\"G\"|\"C\"|\"D\"|\"G\"|]\n",
      "B/2c/2|\"G\"d3/2B/2GA/2B/2|c/2B/2A/2GF|\"C\"E3/2C/2ed/2e/2|\"D\"dFA3/2A/2|\"G\"GB/2G/2D/2G/2B/2|\"C\"c/2B/2A/2G/2F/2E/2G/2|\"D\"ADF/2E/2D/2F/2|\"G\"G3/2G/2BG/2|\"C\"c3/2c/2cd/2e/2|\"D\"f/2g/2e/2f/2e/2d/2c/2B/2|\"G\"dGG|\"C\"c/2B/2A/2G/2C/2E/2G/2|\"D\"A3/2A/2AB/2c/2|\"G\"BGG|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:G\n",
      "|\"G\"|\"C\"|\"D\"|\"Am\"\"D7\"|\"G\"|\"C\"|\"D\"|\"D7\"|\"G\"|\"Em\"|\"Am\"\"D7\"|\"G\"|\"C\"|\"D\"|\"D7\"|\"G\"|]\n",
      "G/2A/2|\"G\"BGGF/2G/2|\"C\"AGGF/2G/2|\"D\"ADDG/2A/2|\"Am\"B/2A/2G/2F/2\"D7\"ED|\"G\"G2GG|\"C\"eg/2f/2eg/2f/2|\"D\"eddc|\"D7\"d/2e/2f/2d/2c/2BA/2G/2|\"G\"BGG|B/2c/2|\"Em\"dgg\n",
      "---------------\n",
      "M:4/4L:1/4K:G|\"G\"|\"C\"|\"D\"|\"D\"|\"G\"|\"C\"|\"D\"|\"G\"|]\n",
      "Bc|\"G\"d2d2|\"C\"ee2e2|\"D\"d2A2|\"D\"A2B2|\"A7\"A2B2|\"D\"A2B2|\"D\"A4-|AD2|\"D\"A2B2|\"G\"BB2B2|\"C\"c2cd|\"D\"A2B2|\"G\"G2G2|\"D\"FFEF|\"G\"G4-|G4|z2Bc|\"D\"d2A2|\"G\"B2B2|\"C\"c2cd|\"G\"B2B2|\"D\"A2B2|\"G\"G2G2|\"D\"D2B2|\"G\"G4-|G4|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:C\n",
      "|\"C\"|\"C\"\"Dm\"|\"C\"|\"C\"|\"C\"\"D7\"|\"G\"|\"G7\"|\"C\"|\"C\"|\"C\"\"D7\"|\"G\"\"D7\"|\"G\"|\"G\"|\"G\"|\"G\"|\"G\"|\"G\"|\"G\"|\"G\"|\"G\"|\"G\"|\"G\"|\"C\"\"D7\"|\"G\"|\"G\"|\"G\"|\"G\"|\"G\"|\"G\"|\"G\"|\"G\"|\"G\"|\"C\"\"D7\"|\"G\"|]\n",
      "B/2c/2|\"C\"ge/2c/2eg/2e/2|\"C\"ce/2c/2\"Dm\"d/2c/2B/2A/2|\"C\"G/2F/2E/2F/2G/2c/2|\"C\"e/2c/2d/2e/2\"D7\"d/2c/2B/2A/2|\"G\"GBG|d/2c/2|\"G\n",
      "---------------\n",
      "M:4/4L:1/4K:G|\"G\"|\"C\"|\"D\"|\"D\"|\"G\"|\"C\"|\"D\"|\"G\"|]\n",
      "d/2|\"G\"d/2G/2B/2D/2d/2|\"C\"e/2c/2G/2e/2G/2e/2G/2|\"D\"d/2B/2D/2F/2d/2F/2d/2|\"D\"c/2A/2F/2G/2Dd/2c/2|\"G\"BG/2d/2G/2e/2G/2d/2|\"C\"e/2d/2e/2f/2g/2e/2d/2|\"D\"c/2A/2F/2G/2A/2B/2c/2|\"G\"dG/2d/2G/2e/2G/2d/2|\"C\"ee/2c/2d/2e/2F/2e/2|\"D\"ff/2d/2BA/2B/2|\"C\"c/2G/2E/2G/2cc/2d/2|\"D\"e/2d/2c/2B/2c/2d/2e/2|\"G\"f/2d/2B/2G/2\"D\"AB/2c/2|\"G\"d/2B/2G/2d/2G/2e/2G/2d/2|\"C\"e/2d/2e/2f/2g/2e/2d/2|\"D\"c/2A/2F/2G/2A/2B/2c/2|\"G\"d/2B/2G/2d/2G/2e/2G/2d/2|\"C\"e/2d/2e/2f/2ge/2f/2|\"D\"f/2e/2d/2c/2\"G\"B/2G/2B/2G/2|\"C\"cc/2G/2c/2G/2e/2G/2|\"D\"c/2A/2\n",
      "---------------\n",
      "M:4/4L:1/4K:G|\"G\"|\"C\"|\"D\"|\"D\"|\"G\"|\"C\"|\"D\"|\"G\"|]\n",
      "Bc|\"G\"dd2d3/2B/2|GB/2d/2GB|\"C\"ce2e3/2c/2|GE2G|\"D\"A2D3/2A/2|BF2F/2A/2|\"D\"ADEF|\"G\"G4-|GBc|\"G\"ddBG|\"C\"ccB3/2c/2|GE2G/2c/2|\"D\"A2D2|\"G\"GD3/2E/2|\"C\"CEGc|\"D\"D2D2|\"G\"B,D3/2E/2B|\"C\"CEGc|\"D\"D2D3/2A/2|\"G\"BAGF|\"D\"A2D2|\"G\"GD3/2E/2B|\"C\"CEGc|\"D\"D2D3/2E/2|\"G\"B,D3/2E/2|\"C\"CEGc|\"D\"D2D2|\"G\"B,D3/2E/2|\"D\"DD3|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:G\n",
      "|\"G\"|\"G\"\"D\"|\"G\"\"Em\"|\"Am\"\"D\"|\"G\"|\"G\"\"C\"|\"G\"\"Em\"|\"C\"\"Am\"|\"D\"\"G\"|\"G\"\"C\"|\"G\"\"D\"|\"G\"\"C\"|\"G\"\"D\"|\"G\"|]\n",
      "\"G\"GG/2A/2G/2F/2G/2E/2|D/2E/2G/2A/2B/2c/2|\"G\"d/2c/2B/2c/2\"D\"d/2c/2B/2A/2|\"G\"GG/2A/2G/2F/2E/2|\"G\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "!python3 sample.py --out_dir=out-abc-char --start='M:4/4L:1/4K:G|\"G\"|\"C\"|\"D\"|\"D\"|\"G\"|\"C\"|\"D\"|\"G\"|]'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test major key with low samples: C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding: out_dir = out-abc-char\n",
      "Overriding: start = M:4/4L:1/4K:C|\"C\"|\"F\"|\"G\"|\"G\"|\"C\"|\"F\"|\"G\"|\"C\"|]\n",
      "number of parameters: 14.77M\n",
      "abc_char\n",
      "Loading meta from data/abc_char/meta.pkl...\n",
      "M:4/4L:1/4K:C|\"C\"|\"F\"|\"G\"|\"G\"|\"C\"|\"F\"|\"G\"|\"C\"|]\n",
      "A/2|\"C\"G/2c/2e3/2G/2c/2e/2|g/2e/2c/2e/2d/2c/2e|\"F\"f/2G/2d/2G/2A/2B/2|\"G\"G/2c/2e3/2G/2c/2|\"G\"B/2d/2G/2c/2B/2d/2G/2|\"C\"e/2c/2e3/2G/2c/2|\"F\"f/2G/2d/2G/2A/2B/2c/2|\"G\"d/2G/2d/2G/2f/2G/2f/2|\"C\"e3/2c/2|e/2c/2G/2c/2e/2c/2|\"F\"A3/2A/2A/2B/2|\"F\"c/2A/2F/2A/2B/2c/2|\"G\"d/2G/2d/2G/2f/2B/2d/2G/2|\"C\"e/2c/2G/2c/2e/2c/2G/2c/2|\"F\"f3/2c/2A/2c/2f/2c/2|\"G\"e/2G/2d/2G/2f/2G/2f/2G/2|\"C\"e/2c/2G/2c/2e/2c/2G/2c/2|\"F\"f3/2c/2A/2c/2f/2c/2|\"F\"A3/2A/2AB/2c/2|\"G\"d/2G/2d/2G/2f/2G/2f/2G/2|\"C\"e/2c/2e/2c/2g/2c/2e/2c/2|\"F\"A3/2A/2AB/2\n",
      "---------------\n",
      "M:4/4L:1/4K:C|\"C\"|\"F\"|\"G\"|\"G\"|\"C\"|\"F\"|\"G\"|\"C\"|]\n",
      "GA|\"C\"c/2e/2-e/2c/2de|\"F\"fFA2|\"G\"GA/2B/2-B/2G/2F/2E/2D/2|\"C\"CE/2G/2-GA|\"F\"f/2e/2-e/2c/2de|\"G\"f/2d/2c/2B/2c/2d/2e/2d/2e/2|\"C\"c/2=B/2c/2d/2-\"F\"f/2A/2G/2=F/2|\"G\"G/2A/2B/2G/2F/2G/2A/2B/2c/2|\"G\"d/2^c/2d/2e/2f/2g/2a/2b/2g/2|\"F\"fc'/2a/2-a/2f/2c'|\"G\"g/2g/2g/2b/2-b/2g/2g|\"C\"c'/2g/2g/2g/2-\"F\"g/2a/2f/2|\"G\"B/2c/2d/2B/2c/2d/2e/2f/2|\"C\"c2c|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:D\n",
      "|\"D\"|\"G\"|\"A7\"|\"D\"|\"G\"|\"G\"|\"A7\"|\"D\"\"A7\"|\"D\"|\"D\"|\"D\"|\"A7\"|\"D\"|\"G\"|\"A7\"|\"D\"\"A7\"|\"D\"|]\n",
      "f/2e/2|\"D\"dA/2B/2AF/2G/2|\"G\"BG/2B/2BB/2B/2|\"A7\"A/2B/2c/2d/2e/2g/2f/2e\n",
      "---------------\n",
      "M:4/4L:1/4K:C|\"C\"|\"F\"|\"G\"|\"G\"|\"C\"|\"F\"|\"G\"|\"C\"|]\n",
      "efec|\"F\"fc2A2|FA/2c/2-c/2A2|\"G\"G2e2|\"G\"B/2d/2GB/2d/2G2|\"C\"G2ef|\"F\"A2cA2|\"G\"G2ef|\"C\"G2ef|\"F\"A2cA2|\"G\"G/2B/2d/2G/2B/2d/2G/2|\"C\"G/2B/2d/2G/2B/2d/2G/2|\"F\"A2cA2|\"G\"G2GA/2B/2|\"C\"c2c|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:A\n",
      "|\"A\"|\"A\"|\"A\"\"E7\"|\"A\"|\"A\"|\"A\"\"E7\"|\"A\"|\"A\"|\"A\"|\"D\"|\"E7\"\"A\"|\"A\"|\"A\"|\"A\"|\"A\"\"E7\"|\"A\"|]\n",
      "e/2d/2|\"A\"cAAB/2c/2|\"A\"c/2d/2c/2B/2A/2c/2d/2e/2|\"A\"cA\"E7\"Ac/2d/2|\"A\"e/2d/2c/2B/2Ae/2d/2|\"A\"cAAB/2c/2|\"A\"e/2f/2e/2d/2\"E7\"c/2e/2d/2c/2|\"A\"AcA|c/2d/2|\"A\"eaag/2e/2|\"A\"cc/2d/2ed/2c/2|\"D\"dfdB/2c/2|\"E7\"d/2c/2B/2c/2\"A\"AEA|\"A\"eaag\n",
      "---------------\n",
      "M:4/4L:1/4K:C|\"C\"|\"F\"|\"G\"|\"G\"|\"C\"|\"F\"|\"G\"|\"C\"|]\n",
      "\"C\"geG2|\"F\"Bc\"G\"G3/2G/2|\"G\"BdG2|\"C\"gec2|\"F\"AcA2|\"G\"dG2G|\"G\"Bdg2|\"C\"fec2|\"F\"AcA2|\"G\"GG2|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:C\n",
      "|\"C\"|\"C\"|\"G\"|\"G\"\"C\"|\"C\"|\"C\"|\"C\"|\"G7\"\"C\"|\"C\"|\"C\"|\"G\"\"C\"|\"C\"|\"C\"|\"F\"|\"C\"|\"F\"\"D7\"|\"G7\"\"C\"|]\n",
      "c/2d/2|\"C\"e/2c/2G/2c/2ec/2e/2|\"C\"g/2e/2c/2c/2ee/2c/2|\"G\"B/2G/2B/2d/2g/2d/2B/2d/2|\"G\"B/2G/2B/2d/2\"C\"c/2G/2c/2e/2|\"C\"g/2e/2c/2e/2g/2e/2c/2e/2|\"C\"g/2e/2c/2c/2ee/2c/2|\"G7\"B/2G/2B/2d/2\"C\"c|e/2d/2|\"C\"ec/2e/2gf/2e/2|\"G\"d/2B/2d/2^f/2g/2e/2d/2|\"G\"B/2G/2B/2d/2\"C\"c/2G/2c/2e/2|\"C\"g/2e/2c/2e/2g/2e/2c/2e/2|\"F\"f/2e/2d\n",
      "---------------\n",
      "M:4/4L:1/4K:C|\"C\"|\"F\"|\"G\"|\"G\"|\"C\"|\"F\"|\"G\"|\"C\"|]\n",
      "E/2G/2|\"C\"c/2c/2c/2A/2G/2|E/2G/2CE/2G/2|\"F\"A/2G/2C/2E/2G/2|\"G\"F/2F/2D/2F/2\"G\"DE/2D/2|\"C\"CCE/2G/2|\"F\"A/2G/2c/2A/2G/2F/2E/2|\"G\"D3/2D/2DE/2F/2|\"C\"G/2F/2E/2CE/2F/2|\"F\"G/2F/2A/2c/2fc/2A/2|\"G\"G/2F/2E/2D/2C/2B,/2D/2|\"C\"C3/2C/2C|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:F\n",
      "|\"F\"|\"F\"|\"F\"|\"F\"|\"C\"|\"C\"|\"F\"|\"F\"|\"C\"|\"C\"|\"F\"|\"F\"|\"F\"|\"F\"|\"C\"|\"C\"|\"C\"|\"F\"|\"F\"|\"F\"|\"F\"|\"C\"|\"F\"|\"F\"|\"C\"|\"F\"|\"F\"|\"C\"|\"F\"|]\n",
      "C/2D/2|\"F\"CF/2G/2A/2G/2F/2A/2|c/2A/2A/2G/2A/2G/2F/2E/2|F/2G/2A/2G/2F/2E/2D/2|\"F\"CF/2G/2A/2G/2F/2|\"F\"c/2A/2A/2G/2Ac/2d/2|\"C\"e/2c/2d/2e/2f/2g/\n",
      "---------------\n",
      "M:4/4L:1/4K:C|\"C\"|\"F\"|\"G\"|\"G\"|\"C\"|\"F\"|\"G\"|\"C\"|]\n",
      "g/4g/4f/4|\"C\"e/2g/2c/2e/2|\"F\"A/2c/2A/2F/2|\"G\"GG/2g/4g/4g/4|\"G\"f/2d/2e/2d/2|\"C\"c/2g/2c/2e/2|\"F\"A/2c/2A/2F/2|\"G\"GG/2g/4g/4g/2|\"G\"f/2d/2e/2d/2|\"C\"c/4B/4c/4c/2c/2|\"F\"A/2A/2F/2F/2|\"G\"GG/2g/4g/4g/4|f/2d/2B/2A/2|\"C\"G/2g/2f/2e/2|\"G\"d/2B/2G/2A/2|\"C\"G/2g/2f/2e/2|\"F\"A/2c/2A/2F/2|\"G\"G/2g/2f/2e/2|\"C\"c/4B/4c/4c/2|]\n",
      "\n",
      "M:6/8\n",
      "L:1/8\n",
      "K:C\n",
      "|\"C\"|\"C\"|\"F\"\"G\"|\"C\"\"Am\"|\"Dm\"\"G\"|\"C\"|\"F\"\"G\"|\"C\"|\"F\"\"G\"|\"C\"|\"F\"\"G\"|\"C\"|\"F\"\"G\"|\"C\"|]\n",
      "c/2d/2|\"C\"edcedc|\"F\"fd\"G\"gef|\"C\"edc\"Am\"dcA|\"Dm\"fdd\"G\"def|\"C\"edc\"Am\"gec|\"Dm\"fdd\"G\"dBG|\"C\"c3c2|a|\"C\n",
      "---------------\n",
      "M:4/4L:1/4K:C|\"C\"|\"F\"|\"G\"|\"G\"|\"C\"|\"F\"|\"G\"|\"C\"|]\n",
      "c/2f/2|\"C\"g3/2a/2g/2f/2e/2d/2c/2|\"F\"F/2f3/2c/2A/2F/2|\"G\"DG/2A/2G/2F/2G/2A/2|\"G\"B/2GA/2G/2F/2G/2A/2|\"C\"C/2c/2c/2c/2cd/2e/2c/2|\"F\"F/2f3/2c/2A/2F/2F/2|\"G\"DG/2A/2G/2F/2G/2A/2|\"C\"c/2d/2c/2B/2c3/2d/2|\"F\"e/2f/2e/2g/2f/2e/2d/2c/2|\"G\"BGG|G/2F/2|\"C\"EG/2C/2CG/2C/2E/2|\"F\"FA/2F/2C/2F/2A/2C/2F/2|\"G\"DG/2D/2G/2DG/2D/2|\"C\"EG/2C/2CG/2C/2E/2|\"F\"FA/2F/2C/2A/2C/2F/2|\"G\"AG/2D/2G/2F/2G/2A/2|\"C\"c/2d/2c/2d/2e/2f/2e/2d/2c/2|\"G\"BGG|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:C\n",
      "|\"C\"|\"C\"\"F\"|\"C\"|\"C\"|\"F\"\"G\"|\"C\"|\"C\"|\"C\"|\"G\"|\"C\"|\"C\"|\"F\"|\"C\"\"G\"|\"C\"|]\n",
      "e/2\n",
      "---------------\n",
      "M:4/4L:1/4K:C|\"C\"|\"F\"|\"G\"|\"G\"|\"C\"|\"F\"|\"G\"|\"C\"|]\n",
      "Gc|\"C\"e2d3/2c/2G/2c/2d/2e/2|\"F\"f3/2c/2A/2F/2A/2|\"G\"GB/2c/2d3/2B/2|\"G\"GB/2c/2de/2G/2|\"C\"e/2d/2c/2B/2c/2G/2c/2d/2|\"F\"e/2c/2d/2B/2cF/2A/2|\"G\"GB/2c/2d3/2B/2|\"C\"c2c2\"C\"gge/2d/2c/2d/2|\"F\"a/2f/2d/2c/2B/2A/2G/2F/2|\"G\"bb/2g/2a/2g/2f/2e/2d/2|\"C\"c2c2|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:F\n",
      "|\"F\"|\"F\"|\"F\"|\"C\"\"G\"|\"C\"|\"F\"|\"F\"|\"C\"\"G\"|\"C\"|\"F\"|\"F\"|\"F\"|\"G\"\"C\"|\"F\"|\"F\"|\"G7\"|\"C\"|]\n",
      "\"F\"c/2A/4A/4A/2A/2AB/2A/2|c/2A/2A/2A/2AB|\"F\"c/2A/2A/2A/2AB/2A/2|\"F\"c/2A/2A/2A/2AB/2A/2|\"C\"c/2G/2G/2A/2\"G\"G/2F/2E/2D/2|\"C\"CC/2E/2EC/2E/2|\"F\"F/2AA/2AF/2A/2|\"F\"c/\n",
      "---------------\n",
      "M:4/4L:1/4K:C|\"C\"|\"F\"|\"G\"|\"G\"|\"C\"|\"F\"|\"G\"|\"C\"|]\n",
      "\"C\"CE/2F/2G/2c/2G/2E/2|CE/2F/2G2|\"F\"AFD|\"G\"A/2^G/2B/2G/2A/2B/2G/2A/2|\"G\"BGD/2E/2F/2G/2|\"C\"CE/2F/2G/2c/2G/2E/2|\"F\"AFD/2E/2F/2A/2|\"G\"G/2A/2B/2c/2d/2c/2B/2|\"C\"c2cA/2B/2|\"F\"c2-\"G7\"cE/2F/2|\"C\"G/2A/2G/2e/2G/2d/2E/2F/2|\"C\"e/2c/2G/2e/2G/2d/2e/2G/2|\"F\"A/2c/2G/2e/2fA/2B/2|\"G\"c/2B/2A/2G/2\"C\"d/2c/2G/2E/2|\"F\"A/2B/2c/2G/2e/2F/2f/2e/2F/2|\"G\"G/2A/2B/2c/2d/2c/2B/2|\"C\"c2c|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:C\n",
      "|\"C\"|\"C\"\"Am\"|\"Dm\"\"G7\"|\"C\"|\"C\"\"Am\"|\"Dm\"\"G7\"|\"C\"|\"C\"|\"C\"\"Am\"|\"Dm\"\"G7\"|\"C\"|\"C\"|\"C\"\"Am\"|\"Dm\"\"G7\"|\"C\"|\"C\"|\"C\"\"Am\"|\"Dm\"\"G7\"|\"C\"|]\n",
      "---------------\n",
      "M:4/4L:1/4K:C|\"C\"|\"F\"|\"G\"|\"G\"|\"C\"|\"F\"|\"G\"|\"C\"|]\n",
      "ee/2d/2|\"C\"c/2ed/2cG|\"F\"Afe/2d/2c/2B/2|\"G\"GedB|\"G\"GedB|\"C\"c/2ed/2cG|\"F\"Afe/2d/2c/2|\"G\"B/2d/2c/2B/2A/2G|\"C\"e/2gf/2e/2d/2c/2|\"F\"Afe/2d/2c/2|\"G\"B/2GA/2BG|\"C\"e/2gf/2e/2d/2c/2|\"F\"Afef|\"G\"G3/2A/2BG|\"C\"e/2gf/2e/2d/2c/2|\"F\"Afe/2d/2c/2|\"G\"B/2GA/2BG|\"C\"e/2gf/2e/2d/2c/2|\"F\"Afe/2d/2c/2|\"G\"B/2GA/2BG|\"C\"c3|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:C\n",
      "|\"C\"|\"G\"|\"Am\"\"D\"|\"G\"\"C\"|\"C\"|\"G\"|\"Am\"\"D\"|\"G\"|\"C\"|\"F\"|\"G\"|\"C\"|\"C\"|\"F\"|\"F\"|\"G\"|]\n",
      "(3G/2F/2E/2|\"C\"C/2E/2G/2A/2B/2cG/2E/2|\"G\"F/2G/2G/2B/2A/2G/2F/2E/2|\"Am\"C/2E/2G/2A/2\"D\"B/2c/2B/2A/2|\"G\"G/2F/2G\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "!python3 sample.py --out_dir=out-abc-char --start='M:4/4L:1/4K:C|\"C\"|\"F\"|\"G\"|\"G\"|\"C\"|\"F\"|\"G\"|\"C\"|]'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test minor key with low samples: Am"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding: out_dir = out-abc-char\n",
      "Overriding: start = M:4/4L:1/4K:Am|\"Am\"|\"Dm\"|\"E\"|\"E\"|\"Am\"|\"Dm\"|\"E\"|\"Am\"|]\n",
      "number of parameters: 14.77M\n",
      "abc_char\n",
      "Loading meta from data/abc_char/meta.pkl...\n",
      "M:4/4L:1/4K:Am|\"Am\"|\"Dm\"|\"E\"|\"E\"|\"Am\"|\"Dm\"|\"E\"|\"Am\"|]\n",
      "A3/4B/4^c3/4d/4e3/4d/4c3/4B/4|\"Am\"c3/4B/4A3/4G/4F3/4E/4|\"Dm\"D3/4E/4F3/4G/4|\"E\"A2A3/2c/2|\"E\"B3/4G/4A3/4B/4^c3/4B/4|\"Am\"A4|\"Am\"e3/4f/4e3/4d/4c3/4B/4|\"Dm\"A3/4G/4F3/4E/4D3/4E/4|\"E\"F2E3/2d/2|\"Am\"c3/4B/4A3/4c/4|\"E\"B2B3/4d/4e3/4d/4|\"Am\"c3/4B/4A3/4c/4\"Dm\"B3/4A/4|\"E\"B2B3/4d/4|\"Am\"c3/4B/4A3/4g/4f3/4e/4|\"D\"d3/4c/4B3/4c/4d3/4e/4|\"Am\"c3/4B/4A3/4g/4f3/4e/4|\"D\"d3/4c/4B3/4A/4|\"E\"^G3/4F/4E3/4d/4e3/4d/4|\"Am\"c3/4B/4A3/4c/4f3/4e/4|\"D\"d3/4c/4B3/4A/4|\"E\"^G3/4B/4c3/4d/4e3/4d/4|\"Am\"c3/4B/4A3/4g/4f3/4e/4|\"D\"d3/4c/4B3/4\n",
      "---------------\n",
      "M:4/4L:1/4K:Am|\"Am\"|\"Dm\"|\"E\"|\"E\"|\"Am\"|\"Dm\"|\"E\"|\"Am\"|]\n",
      "\"Am\"EAA3/4^G/4A3/4B/4c3/4d/4|ede2|\"Dm\"A2A3/4B/4c3/4d/4|\"E\"edcB|\"E\"B3/4^A/4B3/4c/4d3/4e/4|\"Am\"fee2|\"Am\"EAA3/4B/4c3/4d/4|\"Dm\"edd2|\"E\"e2d3/4e/4f3/4g/4|\"Am\"e3/4d/4c3/4B/4A3/4|\"Dm\"d3/4c/4B3/4A/4d3/4e/4f3/4|\"E\"e3/4d/4c3/4B/4A/4^G3/4A/4|\"Am\"B2E3/4G/4A3/4B/4|\"Am\"c3/4e/4d3/4c3/4B/4A3/4|\"Dm\"d3/4c/4B3/4A/4\"Am\"G3/4A/4|\"E\"B2E3/4^G/4A3/4B/4|\"Am\"c3/4e/4d3/4c3/4B/4A3/4|\"Dm\"d3/4c/4B3/4A/4\"G\"G3/4a/4g3/4f/4|\"C\"e3/4d/4c3/4B/4A3/4G/4|\"Dm\"F3/4B/4A3/4c/4d3/4e/4f3/4d/4|\"E\"e3/4d/4c3/4B/4^A3/4B/4|\"Am\"c3/4e/4d3/4c/4\"D7\"d\n",
      "---------------\n",
      "M:4/4L:1/4K:Am|\"Am\"|\"Dm\"|\"E\"|\"E\"|\"Am\"|\"Dm\"|\"E\"|\"Am\"|]\n",
      "A3/2c/2B/2G/2E/2C/2|\"Dm\"D3/2E/2F3/2E/2D3/2F/2|\"E\"G3/2A/2B3/2G/2E3/2D/2|\"E\"E2E3/2A/2G3/2B/2|\"E\"G3/2B/2G3/2B/2E2(3cde|\"Dm\"d3/2e/2d3/2c/2B3/2A/2|\"E\"G3/2A/2B3/2G/2E3/2G/2|\"Am\"A3/2G/2F3/2E/2C3/2B/2|\"Dm\"D3/2E/2F3/2E/2D3/2F/2|\"E\"G3/2F/2E3/2G/2B3/2A/2G3/2B/2|\"Am\"c3/2B/2A3/2G/2F3/2E/2C3/2B,/2|\"Dm\"D3/2E/2F3/2E/2D3/2F/2|\"E\"G3/2F/2E3/2D/2B3/2A/2G3/2B/2|\"Am\"A3/2G/2F3/2E/2C3/2B,/2A3/2G/2|\"Dm\"D3/2E/2F3/2E/2D3/2F/2|\"E\"E2E2|]\n",
      "\n",
      "M:2/4\n",
      "L:1/4\n",
      "K:G\n",
      "|\"G\"|\"D7\"|\"G\"|\"G\"|\"D7\"|\"G\"|\"D7\"|\"G\"|\"G\"|\"D\"\"D7\"|\"G\"|\"D7\"|\"G\"|\"D\"|\"A7\"\n",
      "---------------\n",
      "M:4/4L:1/4K:Am|\"Am\"|\"Dm\"|\"E\"|\"E\"|\"Am\"|\"Dm\"|\"E\"|\"Am\"|]\n",
      "\"Am\"eA/2B/2B/2c/2B/2A/2|\"Dm\"F/2A/2D/2E/2F/2G/2A/2|\"E\"B/2c/2B/2A/2G/2E/2F/2G/2|\"E\"E/2D/2E/2F/2G/2A/2|\"E\"B/2c/2B/2A/2G/2E/2F/2G/2|\"Am\"AA|A|\"Am\"e/2a/2a/2b/2a/2b/2a/2g/2f/2|\"E\"e/2d/2e/2f/2gf/2e/2|\"Dm\"d/2f/2e/2g/2f/2e/2d/2c/2|\"E\"B/2c/2B/2A/2G/2E/2F/2G/2|\"Am\"AAA|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:G#\n",
      "|\"G\"|\"G\"|\"D7\"|\"G\"|\"G\"|\"D7\"|\"G\"|\"C\"|\"C\"|\"C\"|\"D7\"\"G\"|\"G\"|\"D7\"\"G\"|]\n",
      "\"G\"GBd2|\"G\"gg/2f/2e/2d/2B/2G|\"D7\"c/2B/2A/2G/2F/2E/2F/2|\"G\"GBd2|\"G\"gg/2f/2e/2d/2B/2G|\"D7\"A/2B/2c/2A/2\"G\"G2\"G\"g2g/2bc'/2b/2g/2e/2|dg/2b/2d/2b2|\"C\"c'd'/2c'/2b/2\n",
      "---------------\n",
      "M:4/4L:1/4K:Am|\"Am\"|\"Dm\"|\"E\"|\"E\"|\"Am\"|\"Dm\"|\"E\"|\"Am\"|]\n",
      "\"Am\"A3/2B/2c3/2A/2G3/2E/2D3/2E/2D3/2E/2|G/2E/2G3/2B/2c3/2B/2A3/2G/2|\"Dm\"F3/2A/2c3/2A/2d3/2e/2d3/2c/2|\"E\"B3/2d/2e3/2f/2e3/2d/2|\"E\"B2B3/2d/2c3/2B/2A3/2G/2|\"Am\"A3/2B/2A3/2G/2\"Dm\"F3/2E/2D3/2E/2|\"Dm\"D3/2E/2F3/2A/2c3/2A/2d3/2e/2|\"E\"e3/2f/2e3/2f/2g3/2f/2\"Am\"e3/2d/2c3/2e/2|\"Dm\"d3/2c/2d3/2e/2f3/2g/2f3/2e/2d3/2c/2|\"E\"B3/2^A/2B3/2d/2e3/2f/2e3/2d/2|\"Am\"c3/2e/2a3/2g/2f3/2e/2\"Dm\"f3/2e/2d3/2e/2f3/2|\"E\"d3/2e/2f3/2g/2f3/2\"Am\"e3/2d/2c3/2e/2|\"Dm\"d3/2c/2d3/2e/2f3/2a/2g3/2f/2|\"E\"e2e2e2d2|\"C\"c3/2B/2c3/2e/2d3/2c3/2e/\n",
      "---------------\n",
      "M:4/4L:1/4K:Am|\"Am\"|\"Dm\"|\"E\"|\"E\"|\"Am\"|\"Dm\"|\"E\"|\"Am\"|]\n",
      "\"Am\"EAA/2B/2c/2B/2A/2|\"Am\"EAA/2B/2c/2d/2|\"Dm\"e/2d/2c/2B/2A/2G/2F/2|\"E\"EAA/2B/2c/2d/2|\"E\"e/2d/2c/2B/2AG|\"Am\"AA/2B/2c/2B/2A/2|\"Am\"EAA/2B/2c/2d/2|\"Dm\"e/2d/2c/2B/2A/2G/2F/2|\"E\"E/2F/2G/2A/2BA/2G/2|\"Am\"AA/2B/2c/2B/2A/2|\"Am\"EAA/2B/2c/2B/2A/2|\"Dm\"e/2d/2c/2B/2AG|\"E\"E/2F/2G/2A/2BA/2G/2|\"Am\"AA/2B/2c/2B/2A/2G/2|\"Dm\"F/2G/2A/2B/2c/2B/2A/2G/2|\"E\"E/2F/2G/2A/2BA/2G/2|\"Am\"AA/2B/2c/2B/2A/2G/2|\"Am\"AA/2B/2c/2B/2A/2|\"Am\"EA/2B/2c/2B/2A/2G/2|\"E\"E/2F/2G/2A/2BA/2G/2|\"Am\"AA/2B/2c/2B/2A/2|\"Dm\"F/2G/2A/2B/2c/2B/2A/2G/2|\"E\"E\n",
      "---------------\n",
      "M:4/4L:1/4K:Am|\"Am\"|\"Dm\"|\"E\"|\"E\"|\"Am\"|\"Dm\"|\"E\"|\"Am\"|]\n",
      "\"Am\"EAA/2B/2c/2B/2A/2|\"Dm\"FAA/2B/2c/2B/2A/2|\"E\"^GEE2|\"Am\"EAA/2B/2c/2B/2A/2|\"E\"^GEE2|\"Am\"AAA/2B/2c/2B/2A/2|\"Dm\"FAA/2B/2c/2B/2A/2|\"E\"^GEE2|\"Am\"AAA/2B/2c/2B/2A/2|\"Dm\"FAA/2B/2c/2B/2A/2|\"E\"^GEE2|\"Am\"AAA/2B/2c/2B/2A/2|\"Dm\"FAA/2B/2c/2B/2A/2|\"E\"^GEE2|\"Am\"AAA/2B/2c/2B/2A/2|\"Am\"cAA/2B/2c/2B/2A/2|\"E\"^GEE2|\"Am\"AAA/2B/2c/2B/2A/2|\"Dm\"FAA/2B/2c/2B/2A/2|\"E\"^GEE2|\"Am\"AAA/2B/2c/2B/2A/2|\"Dm\"FAA/2B/2c/2B/2A/2|\"E\"^GE2|\"Am\"AA/2B/2c/2B/2A/2B/2^G/2|\"Dm\"FAA/2B/2c/2B/2A/2|\"E\"^GE2|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:D\n",
      "|\"D\"|\"D\"\"D7\"|\"G\"\"D\"|\"\n",
      "---------------\n",
      "M:4/4L:1/4K:Am|\"Am\"|\"Dm\"|\"E\"|\"E\"|\"Am\"|\"Dm\"|\"E\"|\"Am\"|]\n",
      "\"Am\"EeA/2B/2c/2B/2A/2|\"Dm\"E/2F/2A/2c/2e/2d/2c/2B/2A/2|\"E\"^G/2A/2B/2c/2d/2e/2d/2c/2B/2|\"E\"^G/2A/2B/2G/2EA/2B/2|\"Am\"(3c/2B/2A/2B/2c/2f/2B/2A/2(3c/2B/2A/2|\"Dm\"E/2F/2A/2c/2e/2d/2c/2B/2|\"E\"^G/2A/2B/2c/2d/2e/2c/2B/2^G/2|\"Am\"AA/2B/2c/2f/2e/2f/2|\"Dm\"g/2e/2d/2f/2\"E\"e/2^d/2e/2f/2|\"E\"^g/2e/2c/2d/2e/2d/2c/2B/2^G/2|\"Am\"AA/2B/2c/2f/2e/2f/2|\"Dm\"g/2e/2d/2f/2\"Am\"e/2c/2A/2c/2|\"Am\"AAA/2B/2c/2f/2e/2f/2|\"Dm\"g/2e/2d/2f/2\"E\"e/2^d/2e/2f/2|\"Am\"e/2d/2c/2A/2c/2e/2f/2e/2f/2|\"C\"g/2e/2d/2f/2g/2a/2g/2f/2e/2|\"Dm\"dAA/2B/2c/2A/\n",
      "---------------\n",
      "M:4/4L:1/4K:Am|\"Am\"|\"Dm\"|\"E\"|\"E\"|\"Am\"|\"Dm\"|\"E\"|\"Am\"|]\n",
      "\"Am\"EAA3/4F/4|E/2E/2F/2G/2|\"Dm\"A3/4G/4F/2E/2|\"E\"d/2e/2B/2A/2|\"E\"^G/2B/2E/2F/2G/2|\"Am\"E/2A/2C/2E/2|\"Dm\"B,/2DF/2A/2|\"E\"d/2e/2c/2A/2|\"Am\"AA/2c/2|\"Dm\"B/2A/2\"G\"G/2F/2|\"E\"E3/4F/4G/2B/2|\"Am\"c/2A/2c/2e/2|e/2d/2c/2A/2|\"Dm\"B/2A/2\"G\"G/2F/2|\"E\"E3/4F/4G/2B/2|\"Am\"c/2A/2c/2e/2|e/2d/2c/2A/2|\"Dm\"B/2A/2\"G\"G/2F/2|\"E\"E3/4F/4G/2B/2|\"Am\"c/2A/2c/2e/2|e/2d/2c/2A/2|\"Dm\"B/2A/2\"G\"G/2F/2|\"E\"E3/4F/4G/2B/2|\"Am\"c/2A/2c/2e/2|e/2d/2c/2A/2|\"Dm\"B/2A/2\"G\"G/2F/2|\"E\"E3/4F/4G/2B/2|\"Am\"c/2A/2c/2e/2|e/2d/2c/2A/2|\"Dm\"B/2A/2\"A\"A/2|\"G\"G/\n",
      "---------------\n",
      "M:4/4L:1/4K:Am|\"Am\"|\"Dm\"|\"E\"|\"E\"|\"Am\"|\"Dm\"|\"E\"|\"Am\"|]\n",
      "A3/4B/4A/2G/2E/2|G/2A/2B/2A/2|\"Am\"E3/4F/4G/2E/2|\"Dm\"F3/4G/4A/2F/2|\"E\"E/2G/2A/2B/2|\"Am\"c/2B/2A/2G/2|\"Dm\"F3/4G/4A/2F/2|\"E\"E/2G/2B/2E/2|\"Am\"C3/4D/4E/2G/2|\"Dm\"F/2E/2D/2F/2|AF/2A/2c/2|\"E\"B/2G/2E/2G/2|\"Am\"C/2E/2A/2G/2|\"Dm\"F3/4G/4A/2F/2|\"E\"E/2G/2B/2E/2|\"Am\"C/2E/2A/2G/2|\"Dm\"F3/4G/4A/2F/2|\"E\"E/2G/2c/2B/2|\"Am\"c/2E/2A/2G/2|\"Dm\"F/2E/2D/2F/2|\"E\"C/2D/2B,/2E/2|\"Am\"C/2E/2A/2G/2|\"Dm\"F/2E/2D/2F/2|F/2A,/2C/2C/2|\"C\"C/2E/2G/2c/2|\"C\"c/2d/2e/2c/2A/2|\"F\"A/2B/2c/2A/2|\"B\"B/2G/2E/2E/2|\"Am\"C/2E/2A/2G/2|\"Dm\"F/2E/2D/2F/2|\"E\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "!python3 sample.py --out_dir=out-abc-char --start='M:4/4L:1/4K:Am|\"Am\"|\"Dm\"|\"E\"|\"E\"|\"Am\"|\"Dm\"|\"E\"|\"Am\"|]'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test older checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding: out_dir = older_ckpt/m_voices\n",
      "Overriding: path_meta = older_ckpt/m_voices\n",
      "Overriding: start = M:4/4L:1/4K:G|\"G\"|\"C\"|\"D\"|\"D\"|\"G\"|\"C\"|\"D\"|\"G\"|]\n",
      "number of parameters: 14.18M\n",
      "shakespeare_char\n",
      "Loading meta from older_ckpt/m_voices/meta.pkl...\n",
      "M:4/4L:1/4K:G|\"G\"|\"C\"|\"D\"|\"D\"|\"G\"|\"C\"|\"D\"|\"G\"|]\n",
      "e/2|\"G\"gg/2a/2gd/2c/2|\"G\"B/2G/2A/2G/2[+GB][+GB][+GB]|\"D\"A/2G/2F/2E/2D/2F/2A/2c/2|\"G\"B/2[GB][G/2B/2][+GB]e/2f/2|\"G\"gg/2a/2g/2e/2d/2e/2|g/2f/2g/2a/2bb/2e/2|\"D\"f/2e/2f/2g/2a/2f/2a/2f/2|\"G\"g/2f/2g/2a/2\"C\"g/2e/2d/2e/2|\"G\"gg/2a/2g/2e/2d/2e/2|g/2f/2g/2a/2bb|\"D\"d'd'\"D7\"e/2f3/2|\"G\"g3/2a/2gG/2E/2|\"G\"D/2B,/2D/2G/2D/2B,/2D/2G/2|\"C\"E/2[CE][C/2E/2][CE][CE]|\"D\"A/2B/2A/2F/2D/2F/2A/2c/2|\"G\"B/2[GB][G/2B/2][GB]G/2E/2|\"G\"D/2B,/2D/2G/2D/2B,/2D/2G/2|\"C\"E/2[CE][C/2E/2][CE][CE]|\"D\"A/2B/2A/2F/2D/2F/2A/2c/2|\"G\"B/2[GB][G\n",
      "---------------\n",
      "M:4/4L:1/4K:G|\"G\"|\"C\"|\"D\"|\"D\"|\"G\"|\"C\"|\"D\"|\"G\"|]\n",
      "e/2|\"G\"gg/2a/2gd/2c/2|\"G\"B/2G/2A/2G/2[+GB][+GB]|\"D\"A/2G/2F/2E/2D/2F/2A/2c/2|\"G\"B/2[GB][G/2B/2][+GB]e/2f/2|\"G\"gg/2a/2g/2e/2d/2c/2|\"G\"B/2G/2A/2G/2[GB][GB]|\"D\"A/2G/2F/2E/2D/2F/2A/2c/2|\"G\"B/2[GB][G/2B/2][GB]|e/2f/2|\"G\"gg/2a/2g/2e/2d/2e/2|g/2f/2g/2a/2bb/2e/2|\"D\"f/2e/2f/2g/2a/2f/2a/2f/2|\"G\"g/2f/2g/2a/2\"C\"g/2e/2d/2e/2|\"G\"gg/2a/2g/2e/2d/2e/2|g/2f/2g/2a/2bb|\"D\"d'd'\"D7\"e/2f3/2|\"G\"g3/2a/2gG/2E/2|\"G\"D/2B,/2D/2G/2D/2B,/2D/2G/2|\"C\"E/2[CE][C/2E/2][CE][CE]|\"D\"A/2B/2A/2F/2D/2F/2A/2c/2|\"G\"B/2[GB][G/2B/2][GB]G/2E\n",
      "---------------\n",
      "M:4/4L:1/4K:G|\"G\"|\"C\"|\"D\"|\"D\"|\"G\"|\"C\"|\"D\"|\"G\"|]\n",
      "d2|\"G\"B3/2g/2d3/2B/2G3/2B/2d3/2B/2|\"D\"c3/2e/2a3/2g/2f3/2d/2e3/2f/2|\"G\"B3/2g/2d3/2B/2g3/2d/2B3/2d/2|\"D\"c3/2A/2d3/2A/2e3/2A/2f3/2A/2|\"G\"g3/2d/2B3/2d/2g3/2b/2a3/2g/2|\"D\"f3/2d/2A3/2d/2f3/2a/2g3/2f/2|\"C\"e3/2d/2c3/2B/2\"D\"c3/2e/2d3/2c/2|\"G\"B2G2G2d2|\"D\"ADBDcDdc|\"G\"BGcG^cGdG|\"D\"ADBDcDd2|\"G\"edd^cd4|\"D\"ADBDcDdc|\"G\"BGcG^cGd2|\"C\"ecgc\"D\"fcac|\"G\"g2b2g4|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:Eb\n",
      "|\"D\"|\"G\"\"A\"|\"D\"|\"Em\"\"A\"|\"D\"|\"G\"\"A\"|\"G\"\"A\"|\"D\"|\"G\"\"D\"|\"G\"\"D\"|\"G\"\"D\"|\"E\"\"A\"|\"G\"\"D\"|\"G\"\"D\"|\"G\"\"D\"|\"E\"\"A\"|\"A\"\"D\"|]\n",
      "A/2|\"D\"d/2c/2d/2e/2fA|\"G\"Be\"\n",
      "---------------\n",
      "M:4/4L:1/4K:G|\"G\"|\"C\"|\"D\"|\"D\"|\"G\"|\"C\"|\"D\"|\"G\"|]\n",
      "e/2f/2|\"G\"gg/2a/2gd/2c/2|\"G\"B/2G/2A/2G/2[+GB][+GB]|\"D\"A/2G/2F/2E/2D/2F/2A/2c/2|\"G\"B/2[GB][G/2B/2][+GB]e/2f/2|\"G\"gg/2a/2g/2e/2d/2c/2|\"G\"B/2G/2A/2G/2[GB][GB]|\"D\"A/2G/2F/2E/2D/2F/2A/2c/2|\"G\"B/2[GB][G/2B/2][GB]|e/2f/2|\"G\"gg/2a/2g/2e/2d/2e/2|g/2f/2g/2a/2bb/2e/2|\"D\"f/2e/2f/2g/2a/2f/2a/2f/2|\"G\"g/2f/2g/2a/2\"C\"g/2e/2d/2e/2|\"G\"gg/2a/2g/2e/2d/2e/2|g/2f/2g/2a/2bb|\"D\"d'd'\"D7\"e/2f3/2|\"G\"g3/2a/2gG/2E/2|\"G\"D/2B,/2D/2G/2D/2B,/2D/2G/2|\"C\"E/2[CE][C/2E/2][CE][CE][CE]|\"D\"A/2B/2A/2F/2D/2F/2A/2c/2|\"G\"B/2[GB][G/2B/2][\n",
      "---------------\n",
      "M:4/4L:1/4K:G|\"G\"|\"C\"|\"D\"|\"D\"|\"G\"|\"C\"|\"D\"|\"G\"|]\n",
      "d2|\"G\"G/2B/2d3/4d/4|\"C\"e/2e/2c/2A/2|\"D\"F/2A/2D/2|\"D\"F/2A/2D/2d/2|\"D\"=c/2d/2A/2^G/2A/2|\"G\"B/2G/2d3/2e/4|\"G\"d/2B/2G/2A/2|\"C\"E/2G/2D/2=CD/2|\"D\"F/2A/2d/4d/4f/4e/4|\"G\"g/2G/2G/2|\"C\"c/2G/2c/2e|\"D\"d/4c/4B/4A/2f/4|\"G\"g/2d/2B/2G/2|\"D\"F/2A/2D/2A/4d/4|\"G\"B/2G/2d/4d3/4d/4|\"D\"A/2d/4c/4B/4A/2|\"G\"G/2B/2G/2|\"C\"E/2G/2\"G\"G|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:B\n",
      "|\"A\"|\"A\"|\"Bm\"\"E\"|\"A\"|\"Bm\"\"E\"|\"A\"|\"A\"\"Bm\"|\"E\"\"A\"\"A\"|\"D\"|\"A\"|\"Bm\"\"E\"|\"A\"|\"Bm\"\"E\"|\"A\"|\"D\"|\"A\"\"Bm\"|\"E\"\"A\"|\"A\"\"Bm\"|\"E\"\"A\"|]\n",
      "z/2|\"A\"z/2A/2-A/2G/2AA|\"Bm\"B/2c/2B/2A/2\"E\"GE|\"A\"z/2A/2-\n",
      "---------------\n",
      "M:4/4L:1/4K:G|\"G\"|\"C\"|\"D\"|\"D\"|\"G\"|\"C\"|\"D\"|\"G\"|]\n",
      "d/2|\"G\"B3/2G/2G/2|\"C\"cBc|\"D\"d2d|\"G\"G2B/2c/2|\"G\"d3/2B/2GG|\"C\"cB\"D\"AB/2c/2|\"G\"d2\"C\"e/2c/2|\"G\"d2g3/2B/2|\"D\"cABc|\"G\"d2\"C\"e3/2d/2|\"D\"fzde/2f/2|\"G\"g3/2e/2d3/2e/2|g/2c/2B/2A/2G2|\"D\"A2g3/2f/2|\"G\"g3/2d/2B/2\"C\"cB|\"D\"Ad\"G\"g3/2f/2|\"Em\"g/2e/2\"A\"^c/2\"D\"d2|\"A\"e/2d/2c/2d/2eA/2c/2|\"D\"d2\"G\"B/2A/2G/2A/2|\"D\"FDD|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:G\n",
      "|\"A\"|\"E\"|\"E\"|\"A\"|\"E\"\"A\"|\"A\"|\"F#m\"|\"Bm\"\"E\"|\"Bm\"\"E\"|\"A\"\"Bm\"|\"A\"\"E\"|\"A\"\"A\"|]\n",
      "e|\"A\"a3/2g/2a/2e/2c/2A/2|\"E\"BGE3/2E/2|\"/2F/2A/2AB/2c/2|\"Bm\"dc\"E\"e3/2d/2|\"A\"c3/2B/2A\"E\"B/2^G/2E/2B/2d/2c/2B/2|\"A\"A\n",
      "---------------\n",
      "M:4/4L:1/4K:G|\"G\"|\"C\"|\"D\"|\"D\"|\"G\"|\"C\"|\"D\"|\"G\"|]\n",
      "e/2e/2|\"G\"g/2d/2B/2|GB/2A/2|\"G\"G/4A/4B/4c/4d/2g/2|d/2B/2B/2g/2|\"F\"=f/2A/2A/2B/2|cA\"G\"G/4A/4B/4c/4d/2B/2|\"C\"e/2d/4c/4\"G\"d/2B/2|\"G\"G/4A/4B/4c/4d/2B/2|dB|\"G\"G/4A/4B/4c/4d/2B/2|\"C\"e/2d/4c/4d/2e/2|\"F\"=f/2A/2A/2B/2|cA\"G\"g/2d/2B/4c/4d/4B/4|g/2d/2B/4c/4d/4B/4|\"G\"g/2d/2B/4c/4d/4B/4|gd|\"G\"g/2d/2B/4c/4d/4B/4|g/2d/2B/4c/4d/4B/4|\"D\"a/4d/4A/2A/2B/2|\"D7\"cA|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:F\n",
      "|\"D\"|\"C\"|\"D\"|\"D\"|\"G\"\"A\"|\"D\"|\"G\"|\"D\"|\"E\"|\"A\"|\"G\"|\"D\"|\"G\"\"A\"|\"D\"|]\n",
      "a/2g/2|\"D\"fdfd|f/2af/2ag/2f/2|\"C\"e=cec|e/2ge/2ga/2g/2|\"D\"fdfd|\"D\"f/2af/\n",
      "---------------\n",
      "M:4/4L:1/4K:G|\"G\"|\"C\"|\"D\"|\"D\"|\"G\"|\"C\"|\"D\"|\"G\"|]\n",
      "d2|\"G\"B3/2g/2d3/4e/4d/2B/2|\"G\"g/2d/2B/2G/2A/2|B3/4B/4B/2A/2|\"G\"G3/2|\"C\"E/2G/2D/2E/2G/2A/2|\"D\"B3/4B/4B/2A/2G/2|\"G\"G3/2B/2A/2G/2|\"C\"E/2G/2D/2E/2G/2A/2|\"D\"B3/2B/4B/4A/2G/2|\"G\"G2|]\n",
      "\n",
      "M:3/4\n",
      "L:1/4\n",
      "K:Gb\n",
      "|\"D\"|\"D\"|\"G\"|\"D\"|\"D\"|\"Em\"\"A\"|\"D\"|\"D\"|\"A\"|\"Bm\"|\"A7\"|\"D\"|\"A\"|\"E7\"|\"A\"|\"G\"|\"D\"|\"A\"|\"D\"|\"D\"|\"Em\"|\"A7\"|\"D\"|]\n",
      "|A|\"D\"f3/2e/2d|\"D\"AFA|\"G\"BGB|\"D\"AFA|\"D\"f3/2e/2d|\"D\"AFA|\"Em\"Be\"A\"c|\"D\"d2|A|\"D\"f2a|\"A\"e2a|\"Bm\"d3/2e/2d|\"A7\"cBA|\"D\"f2a|\"A\"e2a|\"E7\"^gfg|\"A\"a2a|\"G\"b2b|\"D\"a2a|\"A\"ggg|\"D\"fed|\"D\"f3/2e/2d|\"Em\"Bgf|\"A7\"edc|\"D\"d2\n",
      "---------------\n",
      "M:4/4L:1/4K:G|\"G\"|\"C\"|\"D\"|\"D\"|\"G\"|\"C\"|\"D\"|\"G\"|]\n",
      "e/2|\"G\"gg/2a/2gd/2c/2|\"G\"B/2G/2A/2G/2[+GB][+GB]|\"D\"A/2G/2F/2E/2D/2F/2A/2c/2|\"G\"B/2[GB][G/2B/2][+GB]e/2f/2|\"G\"gg/2a/2g/2e/2d/2c/2|\"G\"B/2G/2A/2G/2[GB][GB]|\"D\"A/2G/2F/2E/2D/2F/2A/2c/2|\"G\"B/2[GB][G/2B/2][GB]|e/2f/2|\"G\"gg/2a/2g/2e/2d/2e/2|g/2f/2g/2a/2bb/2e/2|\"D\"f/2e/2f/2g/2a/2f/2a/2f/2|\"G\"g/2f/2g/2a/2\"C\"g/2e/2d/2e/2|\"D\"f/2e/2f/2g/2a/2b/2|\"D\"c'/2a/2f/2dd|\"G\"g/2f/2g/2a/2b/2g/2e/2d/2|\"C\"c/2g/2f/2g/2a/2g/2e/2d/2c/2|\"G\"B/2g/2f/2g/2d/2g/2d/2B/2|\"D\"cAA|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:G\n",
      "|\"G\"|\"G\"|\"G\"\"D7\"|\"G\"\"D7\"|\"G\"|\"D\"|\"G\n",
      "---------------\n",
      "M:4/4L:1/4K:G|\"G\"|\"C\"|\"D\"|\"D\"|\"G\"|\"C\"|\"D\"|\"G\"|]\n",
      "e/2|\"G\"gg/2a/2gd/2c/2|\"G\"B/2G/2A/2G/2[+GB][+GB]|\"D\"A/2G/2F/2E/2D/2F/2A/2c/2|\"G\"B/2[GB][G/2B/2][+GB]e/2f/2|\"G\"gg/2a/2g/2e/2d/2c/2|\"G\"B/2G/2A/2G/2[GB][GB]|\"D\"A/2G/2F/2E/2D/2F/2A/2c/2|\"G\"B/2[GB][G/2B/2][GB]|e/2f/2|\"G\"gg/2a/2g/2e/2d/2e/2|g/2f/2g/2a/2bb/2e/2|\"D\"f/2e/2f/2g/2a/2f/2a/2f/2|\"G\"g/2f/2g/2a/2\"C\"g/2e/2d/2e/2|\"G\"gg/2a/2g/2e/2d/2e/2|g/2f/2g/2a/2bb|\"D\"d'd'\"D7\"e/2f3/2|\"G\"g3/2a/2gG/2E/2|\"G\"D/2B,/2D/2G/2D/2B,/2D/2G/2|\"C\"E/2[CE][C/2E/2][CE][CE][CE]|\"D\"A/2B/2A/2F/2D/2F/2A/2c/2|\"G\"B/2[GB][G/2B/2][GB]\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    " !python3 sample.py --out_dir=older_ckpt/m_voices --path_meta=older_ckpt/m_voices --start='M:4/4L:1/4K:G|\"G\"|\"C\"|\"D\"|\"D\"|\"G\"|\"C\"|\"D\"|\"G\"|]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat older_ckpt/m_voices/ckpt.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l older_ckpt/m_voices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l out-abc-char/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!id"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
