{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def load_dataframe(relative_path,dataframe_name):\n",
    "    df = pd.read_pickle(f'{relative_path}/{dataframe_name}.pkl')    \n",
    "    return df\n",
    "\n",
    "def read_file(relative_path,file_name):\n",
    "    text= \"\"\n",
    "    with open(f'{relative_path}/{file_name}.abc','r') as f:\n",
    "        text = f.read()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_path =\"notebooks/data/final_dataset\"\n",
    "filename_name = 'clean_augmented_data'\n",
    "#filename_name = 'clean_original_training_data'\n",
    "#relative_path =\"notebooks/data/original_dataset\"\n",
    "training_data_df = load_dataframe(relative_path,filename_name)\n",
    "training_data_df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_df[\"clean_header\"].str.len().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_df[\"clean_body\"].str.len().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bodies = \"\"\n",
    "silences = 0\n",
    "for body in training_data_df[\"clean_body\"]:\n",
    "    if 'z' in body:\n",
    "        silences +=1 \n",
    "    bodies += body+\"\\n\"\n",
    "chars = sorted(list(set(bodies)))\n",
    "vocab_size = len(chars)\n",
    "print('vocab: ',''.join(chars))\n",
    "print('vocab_size',vocab_size)\n",
    "print(\"silences \",silences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_text = read_file(relative_path,filename_name)\n",
    "\n",
    "print(\"number of chars:\",len(training_data_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(training_data_text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import tiktoken\n",
    "\n",
    "print(wandb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dockerfile  docker-compose.yaml  overrides.json\n",
      "README.md   notebooks\t\t requirements.txt\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import shlex\n",
    "import os\n",
    "nano_path = 'notebooks/nanoGPT'\n",
    "os.chdir(nano_path)\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LICENSE      assets\t      data\t  out-abc-char\twandb\n",
      "README.md    config\t      model.py\t  sample.py\n",
      "__pycache__  configurator.py  older_ckpt  train.py\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset with multiple voices present\n",
    "#length of dataset in characters: 4,149,703\n",
    "#all the unique characters: \n",
    "#\"#'()+,-/123456789:=ABCDEFGKLM[]^_abcdefgmz|~\n",
    "#vocab size: 46\n",
    "#train has 3,734,732 tokens\n",
    "#val has 414,971 tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Normal Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 data/abc_char/prepare.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 train.py config/train_abc_char.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Roman Numeral Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters: 4,051,724\n",
      "all the unique characters: \n",
      "\"#'(),-/12345689:=ABCDEFGIKLMV[]^_abcdefgimvz|~\n",
      "vocab size: 48\n",
      "train has 3,646,551 tokens\n",
      "val has 405,173 tokens\n"
     ]
    }
   ],
   "source": [
    "!python3 data/abc_roman_num_char/prepare.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_abc_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-abc-char'\n",
      "eval_interval = 10 # keep frequent because we'll overfit\n",
      "eval_iters = 500\n",
      "log_interval = 5 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = True # override via command line if you like\n",
      "wandb_project = 'abc-char'\n",
      "wandb_run_name = 'mini-char-gpt-hd-12-ly-12-rn-data'\n",
      "\n",
      "dataset = 'abc_roman_num_char'\n",
      "batch_size = 32\n",
      "block_size = 512 # context of up to 512 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 12\n",
      "n_head = 12\n",
      "n_embd = 384\n",
      "dropout = 0.2\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 5000\n",
      "lr_decay_iters = 5000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 5 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "found vocab_size = 48 (inside data/abc_roman_num_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 21.26M\n",
      "using fused AdamW: True\n",
      "compiling the model... (takes a ~minute)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdavidnogales\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.0 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.14.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/pt-env/notebooks/nanoGPT/wandb/run-20230430_120805-kh3zbkiv\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmini-char-gpt-hd-12-ly-12-rn-data\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/davidnogales/abc-char\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/davidnogales/abc-char/runs/kh3zbkiv\u001b[0m\n",
      "step 0: train loss 3.8568, val loss 3.8466\n",
      "[2023-04-30 12:09:13,865] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-30 12:09:14,162] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-30 12:09:14,486] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-30 12:09:14,668] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-30 12:09:14,921] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-30 12:09:15,104] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-30 12:09:15,363] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-30 12:09:15,549] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-30 12:09:15,805] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-30 12:09:15,994] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-30 12:09:16,267] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-30 12:09:16,458] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-30 12:09:16,825] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-30 12:09:17,015] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-30 12:09:17,286] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-30 12:09:17,475] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-30 12:09:17,740] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-30 12:09:17,928] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-30 12:09:18,191] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-30 12:09:18,380] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-30 12:09:18,653] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-30 12:09:18,840] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-30 12:09:19,110] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-30 12:09:19,293] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "iter 0: loss 3.8769, time 83483.70ms, mfu -100.00%\n",
      "iter 5: loss 3.0302, time 7682.81ms, mfu 4.26%\n",
      "step 10: train loss 2.7808, val loss 2.8480\n",
      "saving checkpoint to out-abc-char\n",
      "iter 10: loss 2.7730, time 65180.70ms, mfu 3.89%\n",
      "iter 15: loss 2.5879, time 7670.90ms, mfu 3.92%\n",
      "step 20: train loss 2.2494, val loss 2.3065\n",
      "saving checkpoint to out-abc-char\n",
      "iter 20: loss 2.3573, time 64725.82ms, mfu 3.58%\n",
      "iter 25: loss 2.2352, time 7663.40ms, mfu 3.65%\n",
      "step 30: train loss 2.0316, val loss 2.0900\n",
      "saving checkpoint to out-abc-char\n",
      "iter 30: loss 2.1156, time 65475.71ms, mfu 3.34%\n",
      "iter 35: loss 1.9505, time 7717.00ms, mfu 3.43%\n",
      "step 40: train loss 1.9185, val loss 1.9678\n",
      "saving checkpoint to out-abc-char\n",
      "iter 40: loss 1.9493, time 64932.96ms, mfu 3.13%\n",
      "iter 45: loss 1.8332, time 7601.44ms, mfu 3.25%\n",
      "step 50: train loss 1.8700, val loss 1.9310\n",
      "saving checkpoint to out-abc-char\n",
      "iter 50: loss 1.8826, time 64782.01ms, mfu 2.98%\n",
      "iter 55: loss 1.7763, time 7660.49ms, mfu 3.11%\n",
      "step 60: train loss 1.7711, val loss 1.8371\n",
      "saving checkpoint to out-abc-char\n",
      "iter 60: loss 1.7885, time 64395.66ms, mfu 2.85%\n",
      "iter 65: loss 1.8531, time 7657.56ms, mfu 2.99%\n",
      "step 70: train loss 1.6801, val loss 1.7323\n",
      "saving checkpoint to out-abc-char\n",
      "iter 70: loss 1.7212, time 65080.33ms, mfu 2.74%\n",
      "iter 75: loss 1.6981, time 7624.54ms, mfu 2.90%\n",
      "step 80: train loss 1.6181, val loss 1.6725\n",
      "saving checkpoint to out-abc-char\n",
      "iter 80: loss 1.7062, time 64437.97ms, mfu 2.66%\n",
      "iter 85: loss 1.6144, time 7608.09ms, mfu 2.82%\n",
      "step 90: train loss 1.5466, val loss 1.5612\n",
      "saving checkpoint to out-abc-char\n",
      "iter 90: loss 1.5988, time 64667.71ms, mfu 2.59%\n",
      "iter 95: loss 1.6057, time 7993.34ms, mfu 2.74%\n",
      "step 100: train loss 1.5229, val loss 1.5335\n",
      "saving checkpoint to out-abc-char\n",
      "iter 100: loss 1.5969, time 64713.45ms, mfu 2.52%\n",
      "iter 105: loss 1.5117, time 7594.24ms, mfu 2.70%\n",
      "step 110: train loss 1.4924, val loss 1.4950\n",
      "saving checkpoint to out-abc-char\n",
      "iter 110: loss 1.4496, time 64505.06ms, mfu 2.48%\n",
      "iter 115: loss 1.4878, time 7604.19ms, mfu 2.66%\n",
      "step 120: train loss 1.4557, val loss 1.4554\n",
      "saving checkpoint to out-abc-char\n",
      "iter 120: loss 1.4493, time 64019.29ms, mfu 2.45%\n",
      "iter 125: loss 1.4176, time 7577.79ms, mfu 2.63%\n",
      "step 130: train loss 1.4462, val loss 1.4420\n",
      "saving checkpoint to out-abc-char\n",
      "iter 130: loss 1.4399, time 64218.52ms, mfu 2.42%\n",
      "iter 135: loss 1.4397, time 7612.28ms, mfu 2.61%\n",
      "step 140: train loss 1.4372, val loss 1.4397\n",
      "saving checkpoint to out-abc-char\n",
      "iter 140: loss 1.4528, time 64134.21ms, mfu 2.40%\n",
      "iter 145: loss 1.4311, time 7593.01ms, mfu 2.59%\n",
      "step 150: train loss 1.4099, val loss 1.4016\n",
      "saving checkpoint to out-abc-char\n",
      "iter 150: loss 1.4344, time 64123.92ms, mfu 2.38%\n",
      "iter 155: loss 1.4401, time 7611.99ms, mfu 2.57%\n",
      "step 160: train loss 1.4009, val loss 1.3932\n",
      "saving checkpoint to out-abc-char\n",
      "iter 160: loss 1.5014, time 64169.43ms, mfu 2.37%\n",
      "iter 165: loss 1.4623, time 7635.80ms, mfu 2.56%\n",
      "step 170: train loss 1.3937, val loss 1.3904\n",
      "saving checkpoint to out-abc-char\n",
      "iter 170: loss 1.3345, time 63301.03ms, mfu 2.36%\n",
      "iter 175: loss 1.4048, time 7400.05ms, mfu 2.56%\n",
      "step 180: train loss 1.3884, val loss 1.3792\n",
      "saving checkpoint to out-abc-char\n",
      "iter 180: loss 1.4237, time 62365.06ms, mfu 2.36%\n",
      "iter 185: loss 1.4367, time 7398.71ms, mfu 2.57%\n",
      "step 190: train loss 1.3910, val loss 1.3761\n",
      "saving checkpoint to out-abc-char\n",
      "iter 190: loss 1.4584, time 62332.34ms, mfu 2.36%\n",
      "iter 195: loss 1.3595, time 7399.36ms, mfu 2.57%\n",
      "step 200: train loss 1.3679, val loss 1.3544\n",
      "saving checkpoint to out-abc-char\n",
      "iter 200: loss 1.2831, time 64377.48ms, mfu 2.36%\n",
      "iter 205: loss 1.3523, time 8122.83ms, mfu 2.53%\n",
      "step 210: train loss 1.3774, val loss 1.3685\n",
      "iter 210: loss 1.4123, time 64652.51ms, mfu 2.33%\n",
      "iter 215: loss 1.3442, time 7671.99ms, mfu 2.52%\n",
      "step 220: train loss 1.3498, val loss 1.3400\n",
      "saving checkpoint to out-abc-char\n",
      "iter 220: loss 1.3821, time 64638.37ms, mfu 2.32%\n",
      "iter 225: loss 1.4311, time 7645.64ms, mfu 2.52%\n",
      "step 230: train loss 1.3599, val loss 1.3448\n",
      "iter 230: loss 1.3092, time 65114.43ms, mfu 2.31%\n",
      "iter 235: loss 1.3249, time 7630.16ms, mfu 2.51%\n",
      "step 240: train loss 1.3419, val loss 1.3372\n",
      "saving checkpoint to out-abc-char\n",
      "iter 240: loss 1.3532, time 64331.31ms, mfu 2.31%\n",
      "iter 245: loss 1.3172, time 7726.83ms, mfu 2.50%\n",
      "step 250: train loss 1.3417, val loss 1.3398\n",
      "iter 250: loss 1.3602, time 64804.46ms, mfu 2.30%\n",
      "iter 255: loss 1.3004, time 7611.99ms, mfu 2.50%\n",
      "step 260: train loss 1.2982, val loss 1.2975\n",
      "saving checkpoint to out-abc-char\n",
      "iter 260: loss 1.3702, time 64356.59ms, mfu 2.30%\n",
      "iter 265: loss 1.2661, time 7674.89ms, mfu 2.50%\n",
      "step 270: train loss 1.2775, val loss 1.2719\n",
      "saving checkpoint to out-abc-char\n",
      "iter 270: loss 1.2630, time 64672.00ms, mfu 2.30%\n",
      "iter 275: loss 1.3087, time 7626.63ms, mfu 2.50%\n",
      "step 280: train loss 1.2566, val loss 1.2541\n",
      "saving checkpoint to out-abc-char\n",
      "iter 280: loss 1.3061, time 64975.65ms, mfu 2.30%\n",
      "iter 285: loss 1.2648, time 7575.42ms, mfu 2.50%\n",
      "step 290: train loss 1.2234, val loss 1.2291\n",
      "saving checkpoint to out-abc-char\n",
      "iter 290: loss 1.2904, time 62443.20ms, mfu 2.31%\n",
      "iter 295: loss 1.2425, time 7440.79ms, mfu 2.51%\n",
      "step 300: train loss 1.2132, val loss 1.2208\n",
      "saving checkpoint to out-abc-char\n",
      "iter 300: loss 1.2758, time 64087.10ms, mfu 2.31%\n",
      "iter 305: loss 1.2001, time 7409.55ms, mfu 2.52%\n",
      "step 310: train loss 1.1978, val loss 1.2055\n",
      "saving checkpoint to out-abc-char\n",
      "iter 310: loss 1.2661, time 63512.07ms, mfu 2.32%\n",
      "iter 315: loss 1.2012, time 7424.26ms, mfu 2.53%\n",
      "step 320: train loss 1.1845, val loss 1.1942\n",
      "saving checkpoint to out-abc-char\n",
      "iter 320: loss 1.2344, time 62534.50ms, mfu 2.33%\n",
      "iter 325: loss 1.1549, time 7404.18ms, mfu 2.54%\n",
      "step 330: train loss 1.1540, val loss 1.1548\n",
      "saving checkpoint to out-abc-char\n",
      "iter 330: loss 1.2070, time 62560.14ms, mfu 2.34%\n",
      "iter 335: loss 1.2290, time 7455.59ms, mfu 2.54%\n",
      "step 340: train loss 1.1423, val loss 1.1461\n",
      "saving checkpoint to out-abc-char\n",
      "iter 340: loss 1.2411, time 62731.28ms, mfu 2.34%\n",
      "iter 345: loss 1.1545, time 7413.09ms, mfu 2.55%\n",
      "step 350: train loss 1.1201, val loss 1.1207\n",
      "saving checkpoint to out-abc-char\n",
      "iter 350: loss 1.0883, time 62524.79ms, mfu 2.35%\n",
      "iter 355: loss 1.1019, time 7459.48ms, mfu 2.55%\n",
      "step 360: train loss 1.1106, val loss 1.1063\n",
      "saving checkpoint to out-abc-char\n",
      "iter 360: loss 1.0891, time 62737.54ms, mfu 2.35%\n",
      "iter 365: loss 1.1186, time 7472.52ms, mfu 2.55%\n",
      "step 370: train loss 1.0933, val loss 1.0983\n",
      "saving checkpoint to out-abc-char\n",
      "iter 370: loss 1.1996, time 63192.80ms, mfu 2.35%\n",
      "iter 375: loss 1.1077, time 7624.68ms, mfu 2.54%\n",
      "step 380: train loss 1.0864, val loss 1.0829\n",
      "saving checkpoint to out-abc-char\n",
      "iter 380: loss 1.0884, time 64147.17ms, mfu 2.34%\n",
      "iter 385: loss 1.1098, time 7626.33ms, mfu 2.54%\n",
      "step 390: train loss 1.0828, val loss 1.0802\n",
      "saving checkpoint to out-abc-char\n",
      "iter 390: loss 1.1660, time 64191.24ms, mfu 2.33%\n",
      "iter 395: loss 1.1167, time 7631.01ms, mfu 2.53%\n",
      "step 400: train loss 1.0585, val loss 1.0617\n",
      "saving checkpoint to out-abc-char\n",
      "iter 400: loss 1.0963, time 64127.59ms, mfu 2.33%\n",
      "iter 405: loss 1.0812, time 7625.45ms, mfu 2.52%\n",
      "step 410: train loss 1.0668, val loss 1.0669\n",
      "iter 410: loss 1.0676, time 63863.14ms, mfu 2.32%\n",
      "iter 415: loss 1.1089, time 7411.71ms, mfu 2.53%\n",
      "step 420: train loss 1.0375, val loss 1.0402\n",
      "saving checkpoint to out-abc-char\n",
      "iter 420: loss 1.0785, time 62450.20ms, mfu 2.33%\n",
      "iter 425: loss 1.0692, time 7410.99ms, mfu 2.54%\n",
      "step 430: train loss 1.0317, val loss 1.0379\n",
      "saving checkpoint to out-abc-char\n",
      "iter 430: loss 1.0233, time 62450.34ms, mfu 2.34%\n",
      "iter 435: loss 1.0005, time 7413.27ms, mfu 2.55%\n",
      "step 440: train loss 1.0109, val loss 1.0173\n",
      "saving checkpoint to out-abc-char\n",
      "iter 440: loss 1.0575, time 62430.38ms, mfu 2.34%\n",
      "iter 445: loss 0.9969, time 7414.94ms, mfu 2.55%\n",
      "step 450: train loss 1.0132, val loss 1.0248\n",
      "iter 450: loss 1.0066, time 62185.65ms, mfu 2.35%\n",
      "iter 455: loss 1.0119, time 7414.46ms, mfu 2.56%\n",
      "step 460: train loss 1.0046, val loss 1.0071\n",
      "saving checkpoint to out-abc-char\n",
      "iter 460: loss 1.0678, time 62430.16ms, mfu 2.35%\n",
      "iter 465: loss 1.0710, time 7416.25ms, mfu 2.56%\n",
      "step 470: train loss 0.9841, val loss 0.9946\n",
      "saving checkpoint to out-abc-char\n",
      "iter 470: loss 1.0107, time 62418.78ms, mfu 2.36%\n",
      "iter 475: loss 0.9549, time 7417.17ms, mfu 2.56%\n",
      "step 480: train loss 0.9810, val loss 0.9862\n",
      "saving checkpoint to out-abc-char\n",
      "iter 480: loss 1.0013, time 62430.99ms, mfu 2.36%\n",
      "iter 485: loss 1.0143, time 7414.26ms, mfu 2.56%\n",
      "step 490: train loss 0.9732, val loss 0.9804\n",
      "saving checkpoint to out-abc-char\n",
      "iter 490: loss 0.9413, time 62438.19ms, mfu 2.36%\n",
      "iter 495: loss 1.0077, time 7410.19ms, mfu 2.57%\n",
      "step 500: train loss 0.9634, val loss 0.9696\n",
      "saving checkpoint to out-abc-char\n",
      "iter 500: loss 0.9847, time 62421.74ms, mfu 2.36%\n",
      "iter 505: loss 0.9504, time 7414.14ms, mfu 2.57%\n",
      "step 510: train loss 0.9511, val loss 0.9589\n",
      "saving checkpoint to out-abc-char\n",
      "iter 510: loss 0.9795, time 62482.94ms, mfu 2.36%\n",
      "iter 515: loss 0.9748, time 7413.33ms, mfu 2.57%\n",
      "step 520: train loss 0.9455, val loss 0.9576\n",
      "saving checkpoint to out-abc-char\n",
      "iter 520: loss 0.9142, time 62438.26ms, mfu 2.36%\n",
      "iter 525: loss 0.9497, time 7413.73ms, mfu 2.57%\n",
      "step 530: train loss 0.9338, val loss 0.9467\n",
      "saving checkpoint to out-abc-char\n",
      "iter 530: loss 0.9574, time 62428.12ms, mfu 2.36%\n",
      "iter 535: loss 0.9923, time 7415.68ms, mfu 2.57%\n",
      "step 540: train loss 0.9320, val loss 0.9448\n",
      "saving checkpoint to out-abc-char\n",
      "iter 540: loss 0.9192, time 62430.54ms, mfu 2.37%\n",
      "iter 545: loss 0.9642, time 7414.51ms, mfu 2.57%\n",
      "step 550: train loss 0.9149, val loss 0.9270\n",
      "saving checkpoint to out-abc-char\n",
      "iter 550: loss 0.9030, time 62435.98ms, mfu 2.37%\n",
      "iter 555: loss 0.9513, time 7414.23ms, mfu 2.57%\n",
      "step 560: train loss 0.9107, val loss 0.9243\n",
      "saving checkpoint to out-abc-char\n",
      "iter 560: loss 0.9585, time 62392.08ms, mfu 2.37%\n",
      "iter 565: loss 0.9040, time 7412.27ms, mfu 2.57%\n",
      "step 570: train loss 0.8920, val loss 0.9077\n",
      "saving checkpoint to out-abc-char\n",
      "iter 570: loss 0.9111, time 62427.00ms, mfu 2.37%\n",
      "iter 575: loss 0.9669, time 7415.24ms, mfu 2.57%\n",
      "step 580: train loss 0.8827, val loss 0.9013\n",
      "saving checkpoint to out-abc-char\n",
      "iter 580: loss 0.9034, time 62421.33ms, mfu 2.37%\n",
      "iter 585: loss 0.8958, time 7415.60ms, mfu 2.57%\n",
      "step 590: train loss 0.8982, val loss 0.9140\n",
      "iter 590: loss 0.9211, time 62193.54ms, mfu 2.37%\n",
      "iter 595: loss 0.8945, time 7414.73ms, mfu 2.57%\n",
      "step 600: train loss 0.8699, val loss 0.8883\n",
      "saving checkpoint to out-abc-char\n",
      "iter 600: loss 0.8423, time 62445.56ms, mfu 2.37%\n",
      "iter 605: loss 0.9212, time 7413.28ms, mfu 2.57%\n",
      "step 610: train loss 0.8682, val loss 0.8840\n",
      "saving checkpoint to out-abc-char\n",
      "iter 610: loss 0.8223, time 62413.46ms, mfu 2.37%\n",
      "iter 615: loss 0.9432, time 7413.21ms, mfu 2.57%\n",
      "step 620: train loss 0.8570, val loss 0.8732\n",
      "saving checkpoint to out-abc-char\n",
      "iter 620: loss 0.9006, time 62430.46ms, mfu 2.37%\n",
      "iter 625: loss 0.8409, time 7416.02ms, mfu 2.57%\n",
      "step 630: train loss 0.8435, val loss 0.8659\n",
      "saving checkpoint to out-abc-char\n",
      "iter 630: loss 0.8862, time 62430.40ms, mfu 2.37%\n",
      "iter 635: loss 0.8363, time 7413.31ms, mfu 2.57%\n",
      "step 640: train loss 0.8291, val loss 0.8566\n",
      "saving checkpoint to out-abc-char\n",
      "iter 640: loss 0.8533, time 64208.80ms, mfu 2.37%\n",
      "iter 645: loss 0.8475, time 7665.08ms, mfu 2.56%\n",
      "step 650: train loss 0.8164, val loss 0.8503\n",
      "saving checkpoint to out-abc-char\n",
      "iter 650: loss 0.7657, time 64176.34ms, mfu 2.35%\n",
      "iter 655: loss 0.8074, time 7628.90ms, mfu 2.55%\n",
      "step 660: train loss 0.8055, val loss 0.8298\n",
      "saving checkpoint to out-abc-char\n",
      "iter 660: loss 0.8405, time 64402.50ms, mfu 2.34%\n",
      "iter 665: loss 0.8077, time 7599.66ms, mfu 2.54%\n",
      "step 670: train loss 0.8023, val loss 0.8319\n",
      "iter 670: loss 0.8290, time 63952.97ms, mfu 2.34%\n",
      "iter 675: loss 0.7695, time 7643.77ms, mfu 2.53%\n",
      "step 680: train loss 0.7850, val loss 0.8148\n",
      "saving checkpoint to out-abc-char\n",
      "iter 680: loss 0.8258, time 64425.84ms, mfu 2.33%\n",
      "iter 685: loss 0.8264, time 7596.60ms, mfu 2.53%\n",
      "step 690: train loss 0.7656, val loss 0.7950\n",
      "saving checkpoint to out-abc-char\n",
      "iter 690: loss 0.7609, time 64141.48ms, mfu 2.33%\n",
      "iter 695: loss 0.7856, time 7617.29ms, mfu 2.52%\n",
      "step 700: train loss 0.7592, val loss 0.7947\n",
      "saving checkpoint to out-abc-char\n",
      "iter 700: loss 0.8181, time 64218.16ms, mfu 2.32%\n",
      "iter 705: loss 0.8126, time 7599.83ms, mfu 2.52%\n",
      "step 710: train loss 0.7472, val loss 0.7811\n",
      "saving checkpoint to out-abc-char\n",
      "iter 710: loss 0.8000, time 64110.42ms, mfu 2.32%\n",
      "iter 715: loss 0.7745, time 7436.23ms, mfu 2.53%\n",
      "step 720: train loss 0.7290, val loss 0.7611\n",
      "saving checkpoint to out-abc-char\n",
      "iter 720: loss 0.7727, time 62930.16ms, mfu 2.33%\n",
      "iter 725: loss 0.7650, time 7477.40ms, mfu 2.53%\n",
      "step 730: train loss 0.7198, val loss 0.7555\n",
      "saving checkpoint to out-abc-char\n",
      "iter 730: loss 0.7219, time 62954.63ms, mfu 2.33%\n",
      "iter 735: loss 0.7615, time 7422.93ms, mfu 2.54%\n",
      "step 740: train loss 0.7032, val loss 0.7401\n",
      "saving checkpoint to out-abc-char\n",
      "iter 740: loss 0.7211, time 62882.67ms, mfu 2.34%\n",
      "iter 745: loss 0.7752, time 7556.60ms, mfu 2.54%\n",
      "step 750: train loss 0.6929, val loss 0.7315\n",
      "saving checkpoint to out-abc-char\n",
      "iter 750: loss 0.7277, time 62920.17ms, mfu 2.33%\n",
      "iter 755: loss 0.7220, time 7458.72ms, mfu 2.54%\n",
      "step 760: train loss 0.6863, val loss 0.7233\n",
      "saving checkpoint to out-abc-char\n",
      "iter 760: loss 0.7104, time 62920.94ms, mfu 2.34%\n",
      "iter 765: loss 0.6975, time 7452.08ms, mfu 2.54%\n",
      "step 770: train loss 0.6684, val loss 0.7083\n",
      "saving checkpoint to out-abc-char\n",
      "iter 770: loss 0.7343, time 62807.70ms, mfu 2.34%\n",
      "iter 775: loss 0.6972, time 7474.89ms, mfu 2.55%\n",
      "step 780: train loss 0.6543, val loss 0.6867\n",
      "saving checkpoint to out-abc-char\n",
      "iter 780: loss 0.6791, time 62807.16ms, mfu 2.34%\n",
      "iter 785: loss 0.6359, time 7467.17ms, mfu 2.55%\n",
      "step 790: train loss 0.6427, val loss 0.6787\n",
      "saving checkpoint to out-abc-char\n",
      "iter 790: loss 0.6654, time 62670.95ms, mfu 2.34%\n",
      "iter 795: loss 0.6972, time 7455.74ms, mfu 2.55%\n",
      "step 800: train loss 0.6376, val loss 0.6767\n",
      "saving checkpoint to out-abc-char\n",
      "iter 800: loss 0.6255, time 62734.33ms, mfu 2.35%\n",
      "iter 805: loss 0.6379, time 7443.11ms, mfu 2.55%\n",
      "step 810: train loss 0.6265, val loss 0.6645\n",
      "saving checkpoint to out-abc-char\n",
      "iter 810: loss 0.6463, time 62882.29ms, mfu 2.35%\n",
      "iter 815: loss 0.6476, time 7501.20ms, mfu 2.55%\n",
      "step 820: train loss 0.6116, val loss 0.6519\n",
      "saving checkpoint to out-abc-char\n",
      "iter 820: loss 0.6431, time 62943.61ms, mfu 2.35%\n",
      "iter 825: loss 0.6252, time 7494.29ms, mfu 2.55%\n",
      "step 830: train loss 0.6015, val loss 0.6419\n",
      "saving checkpoint to out-abc-char\n",
      "iter 830: loss 0.6465, time 62782.10ms, mfu 2.35%\n",
      "iter 835: loss 0.6170, time 7458.63ms, mfu 2.55%\n",
      "step 840: train loss 0.5842, val loss 0.6297\n",
      "saving checkpoint to out-abc-char\n",
      "iter 840: loss 0.6241, time 62788.78ms, mfu 2.35%\n",
      "iter 845: loss 0.6126, time 7412.63ms, mfu 2.56%\n",
      "step 850: train loss 0.5828, val loss 0.6288\n",
      "saving checkpoint to out-abc-char\n",
      "iter 850: loss 0.6281, time 62812.17ms, mfu 2.35%\n",
      "iter 855: loss 0.6399, time 7491.33ms, mfu 2.55%\n",
      "step 860: train loss 0.5766, val loss 0.6208\n",
      "saving checkpoint to out-abc-char\n",
      "iter 860: loss 0.5914, time 62608.66ms, mfu 2.35%\n",
      "iter 865: loss 0.6123, time 7433.08ms, mfu 2.56%\n",
      "step 870: train loss 0.5576, val loss 0.6069\n",
      "saving checkpoint to out-abc-char\n",
      "iter 870: loss 0.5793, time 63849.88ms, mfu 2.35%\n",
      "iter 875: loss 0.5649, time 7432.33ms, mfu 2.56%\n",
      "step 880: train loss 0.5567, val loss 0.6033\n",
      "saving checkpoint to out-abc-char\n",
      "iter 880: loss 0.5577, time 62901.24ms, mfu 2.35%\n",
      "iter 885: loss 0.5657, time 7569.15ms, mfu 2.55%\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 293, in <module>\n",
      "    scaler.scale(loss).backward()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\", line 487, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\", line 200, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[31m(failed 255).\u001b[0m Press Control-C to abort syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: / 0.028 MB of 0.028 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       iter ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        mfu ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/loss ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val/loss ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       iter 880\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr 0.00093\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        mfu 2.55706\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/loss 0.55668\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val/loss 0.6033\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mmini-char-gpt-hd-12-ly-12-rn-data\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/davidnogales/abc-char/runs/kh3zbkiv\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230430_120805-kh3zbkiv/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 train.py config/train_abc_char.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./older_ckpt/hd-12-ly-12-rn-data'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_name = 'hd-12-ly-12-rn-data'\n",
    "examples_folder = f'./older_ckpt/{folder_name}'\n",
    "examples_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_start = {\n",
    "    'G':'M:4/4L:1/4K:G|\"G\"|\"C\"|\"D\"|\"D\"|\"G\"|\"C\"|\"D\"|\"G\"|]',\n",
    "    'C':'M:4/4L:1/4K:C|\"C\"|\"F\"|\"G\"|\"G\"|\"C\"|\"F\"|\"G\"|\"C\"|]',\n",
    "    'Am':'M:4/4L:1/4K:Am|\"Am\"|\"Dm\"|\"E\"|\"E\"|\"Am\"|\"Dm\"|\"E\"|\"Am\"|]'\n",
    "    }\n",
    "\n",
    "songs_roman_start = {\n",
    "    'G':'M:4/4L:1/4K:G|\"I\"|\"IV\"|\"V\"|\"V\"|\"I\"|\"IV\"|\"V\"|\"I\"|]',\n",
    "    'C':'M:4/4L:1/4K:C|\"I\"|\"IV\"|\"V\"|\"V\"|\"I\"|\"IV\"|\"V\"|\"I\"|]',\n",
    "    'Am':'M:4/4L:1/4K:Am|\"i\"|\"iv\"|\"V\"|\"V\"|\"i\"|\"iv\"|\"V\"|\"i\"|]'\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test key with most occurrences: G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'M:4/4L:1/4K:G|\"I\"|\"IV\"|\"V\"|\"V\"|\"I\"|\"IV\"|\"V\"|\"I\"|]'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song_start = songs_roman_start['G']\n",
    "song_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 sample.py --out_dir=out-abc-char --start={shlex.quote(song_start)} > {examples_folder}/examples_G.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test major key with low samples: C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'M:4/4L:1/4K:C|\"I\"|\"IV\"|\"V\"|\"V\"|\"I\"|\"IV\"|\"V\"|\"I\"|]'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song_start = songs_roman_start['C']\n",
    "song_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 sample.py --out_dir=out-abc-char --start={shlex.quote(song_start)} > {examples_folder}/examples_C.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test minor key with low samples: Am"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'M:4/4L:1/4K:Am|\"i\"|\"iv\"|\"V\"|\"V\"|\"i\"|\"iv\"|\"V\"|\"i\"|]'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song_start = songs_roman_start['Am']\n",
    "song_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 sample.py --out_dir=out-abc-char --start={shlex.quote(song_start)} > {examples_folder}/examples_Am.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move checkpoint files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = './data/abc_roman_num_char/meta.pkl'\n",
    "target_folder = examples_folder\n",
    "!mv {source} {target_folder}/meta.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = './out-abc-char/ckpt.pt'\n",
    "!mv {source} {target_folder}/ckpt.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = './config/train_abc_char.py'\n",
    "!cp {source} {target_folder}/config.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test older checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_start = songs_start['Am']\n",
    "!echo {shlex.quote(song_start)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " song_start = songs_start['Am']\n",
    " !python3 sample.py --out_dir=older_ckpt/m_voices --path_meta=older_ckpt/m_voices --start={shlex.quote(current_start)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 --version\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
