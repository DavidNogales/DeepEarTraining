{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def load_dataframe(relative_path,dataframe_name):\n",
    "    df = pd.read_pickle(f'{relative_path}/{dataframe_name}.pkl')    \n",
    "    return df\n",
    "\n",
    "def read_file(relative_path,file_name):\n",
    "    text= \"\"\n",
    "    with open(f'{relative_path}/{file_name}.abc','r') as f:\n",
    "        text = f.read()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_path =\"notebooks/data/final_dataset\"\n",
    "filename_name = 'clean_augmented_data'\n",
    "#filename_name = 'clean_original_training_data'\n",
    "#relative_path =\"notebooks/data/original_dataset\"\n",
    "training_data_df = load_dataframe(relative_path,filename_name)\n",
    "training_data_df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_df[\"clean_header\"].str.len().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_df[\"clean_body\"].str.len().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bodies = \"\"\n",
    "silences = 0\n",
    "for body in training_data_df[\"clean_body\"]:\n",
    "    if 'z' in body:\n",
    "        silences +=1 \n",
    "    bodies += body+\"\\n\"\n",
    "chars = sorted(list(set(bodies)))\n",
    "vocab_size = len(chars)\n",
    "print('vocab: ',''.join(chars))\n",
    "print('vocab_size',vocab_size)\n",
    "print(\"silences \",silences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_text = read_file(relative_path,filename_name)\n",
    "\n",
    "print(\"number of chars:\",len(training_data_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(training_data_text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import tiktoken\n",
    "\n",
    "print(wandb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "nano_path = 'notebooks/nanoGPT'\n",
    "os.chdir(nano_path)\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset with multiple voices present\n",
    "#length of dataset in characters: 4,149,703\n",
    "#all the unique characters: \n",
    "#\"#'()+,-/123456789:=ABCDEFGKLM[]^_abcdefgmz|~\n",
    "#vocab size: 46\n",
    "#train has 3,734,732 tokens\n",
    "#val has 414,971 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters: 4,062,773\n",
      "all the unique characters: \n",
      "\"#'(),-/123456789:=ABCDEFGKLM[]^_abcdefgmz|~\n",
      "vocab size: 45\n",
      "train has 3,656,495 tokens\n",
      "val has 406,278 tokens\n"
     ]
    }
   ],
   "source": [
    "!python3 data/abc_char/prepare.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_abc_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-abc-char'\n",
      "eval_interval = 10 # keep frequent because we'll overfit\n",
      "eval_iters = 500\n",
      "log_interval = 5 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = True # override via command line if you like\n",
      "wandb_project = 'abc-char'\n",
      "wandb_run_name = 'mini-char-gpt-hd-8-ly-12-bt-1'\n",
      "\n",
      "dataset = 'abc_char'\n",
      "batch_size = 1\n",
      "block_size = 512 # context of up to 512 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 12\n",
      "n_head = 8\n",
      "n_embd = 384\n",
      "dropout = 0.2\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 5000\n",
      "lr_decay_iters = 5000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 5 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "found vocab_size = 45 (inside data/abc_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 21.26M\n",
      "using fused AdamW: True\n",
      "compiling the model... (takes a ~minute)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdavidnogales\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.0 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.14.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/pt-env/notebooks/nanoGPT/wandb/run-20230426_213604-l4doe6wv\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmini-char-gpt-hd-8-ly-12-bt-1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/davidnogales/abc-char\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/davidnogales/abc-char/runs/l4doe6wv\u001b[0m\n",
      "step 0: train loss 3.9184, val loss 3.9154\n",
      "[2023-04-26 21:36:18,886] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-26 21:36:19,253] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-26 21:36:19,775] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-26 21:36:19,961] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-26 21:36:20,201] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-26 21:36:20,379] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-26 21:36:20,628] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-26 21:36:20,811] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-26 21:36:21,052] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-26 21:36:21,226] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-26 21:36:21,469] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-26 21:36:21,668] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-26 21:36:21,911] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-26 21:36:22,189] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-26 21:36:22,433] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-26 21:36:22,613] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-26 21:36:22,850] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-26 21:36:23,028] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-26 21:36:23,268] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-26 21:36:23,447] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-26 21:36:23,696] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-26 21:36:23,886] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-26 21:36:24,132] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-26 21:36:24,317] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "iter 0: loss 3.9437, time 22766.24ms, mfu -100.00%\n",
      "iter 5: loss 3.2510, time 812.51ms, mfu 1.26%\n",
      "step 10: train loss 2.7402, val loss 2.7481\n",
      "saving checkpoint to out-abc-char\n",
      "iter 10: loss 2.7255, time 7403.49ms, mfu 1.15%\n",
      "iter 15: loss 2.1813, time 874.40ms, mfu 1.15%\n",
      "step 20: train loss 2.4088, val loss 2.4167\n",
      "saving checkpoint to out-abc-char\n",
      "iter 20: loss 2.1503, time 7462.81ms, mfu 1.05%\n",
      "iter 25: loss 2.3864, time 1045.32ms, mfu 1.04%\n",
      "step 30: train loss 2.2073, val loss 2.2538\n",
      "saving checkpoint to out-abc-char\n",
      "iter 30: loss 1.8130, time 7962.66ms, mfu 0.95%\n",
      "iter 35: loss 2.0414, time 1006.21ms, mfu 0.96%\n",
      "step 40: train loss 2.1023, val loss 2.1713\n",
      "saving checkpoint to out-abc-char\n",
      "iter 40: loss 1.8120, time 7840.04ms, mfu 0.87%\n",
      "iter 45: loss 2.2632, time 1004.25ms, mfu 0.89%\n",
      "step 50: train loss 2.0259, val loss 2.0761\n",
      "saving checkpoint to out-abc-char\n",
      "iter 50: loss 2.3624, time 8019.66ms, mfu 0.81%\n",
      "iter 55: loss 2.1078, time 918.11ms, mfu 0.84%\n",
      "step 60: train loss 1.9553, val loss 1.9682\n",
      "saving checkpoint to out-abc-char\n",
      "iter 60: loss 2.0590, time 7674.75ms, mfu 0.77%\n",
      "iter 65: loss 1.8925, time 1002.28ms, mfu 0.80%\n",
      "step 70: train loss 1.9054, val loss 1.9350\n",
      "saving checkpoint to out-abc-char\n",
      "iter 70: loss 2.0413, time 7679.58ms, mfu 0.73%\n",
      "iter 75: loss 2.1807, time 897.10ms, mfu 0.77%\n",
      "step 80: train loss 1.8512, val loss 1.8671\n",
      "saving checkpoint to out-abc-char\n",
      "iter 80: loss 1.9511, time 7708.40ms, mfu 0.71%\n",
      "iter 85: loss 1.2992, time 826.29ms, mfu 0.76%\n",
      "step 90: train loss 1.8261, val loss 1.8331\n",
      "saving checkpoint to out-abc-char\n",
      "iter 90: loss 1.7146, time 7774.46ms, mfu 0.70%\n",
      "iter 95: loss 1.7205, time 811.49ms, mfu 0.75%\n",
      "step 100: train loss 1.7831, val loss 1.8300\n",
      "saving checkpoint to out-abc-char\n",
      "iter 100: loss 1.7618, time 7413.09ms, mfu 0.69%\n",
      "iter 105: loss 1.9268, time 856.90ms, mfu 0.74%\n",
      "step 110: train loss 1.7426, val loss 1.7728\n",
      "saving checkpoint to out-abc-char\n",
      "iter 110: loss 1.7616, time 7555.79ms, mfu 0.68%\n",
      "iter 115: loss 2.2120, time 895.85ms, mfu 0.73%\n",
      "step 120: train loss 1.7678, val loss 1.7811\n",
      "iter 120: loss 1.5993, time 7067.45ms, mfu 0.67%\n",
      "iter 125: loss 2.1504, time 806.10ms, mfu 0.73%\n",
      "step 130: train loss 1.7146, val loss 1.7327\n",
      "saving checkpoint to out-abc-char\n",
      "iter 130: loss 1.8977, time 7279.38ms, mfu 0.67%\n",
      "iter 135: loss 1.7320, time 943.53ms, mfu 0.71%\n",
      "step 140: train loss 1.7000, val loss 1.7470\n",
      "iter 140: loss 1.9952, time 7401.51ms, mfu 0.65%\n",
      "iter 145: loss 1.9895, time 910.91ms, mfu 0.70%\n",
      "step 150: train loss 1.6873, val loss 1.7065\n",
      "saving checkpoint to out-abc-char\n",
      "iter 150: loss 1.5401, time 7872.24ms, mfu 0.64%\n",
      "iter 155: loss 1.9226, time 925.03ms, mfu 0.69%\n",
      "step 160: train loss 1.6560, val loss 1.7343\n",
      "iter 160: loss 1.3678, time 7150.91ms, mfu 0.64%\n",
      "iter 165: loss 1.8458, time 877.80ms, mfu 0.69%\n",
      "step 170: train loss 1.6361, val loss 1.7067\n",
      "iter 170: loss 1.3467, time 7334.74ms, mfu 0.63%\n",
      "iter 175: loss 1.2374, time 851.75ms, mfu 0.69%\n",
      "step 180: train loss 1.6216, val loss 1.6270\n",
      "saving checkpoint to out-abc-char\n",
      "iter 180: loss 1.4172, time 8003.00ms, mfu 0.63%\n",
      "iter 185: loss 2.0507, time 926.09ms, mfu 0.68%\n",
      "step 190: train loss 1.6035, val loss 1.6320\n",
      "iter 190: loss 1.2237, time 7512.61ms, mfu 0.63%\n",
      "iter 195: loss 1.6902, time 904.19ms, mfu 0.68%\n",
      "step 200: train loss 1.6053, val loss 1.6463\n",
      "iter 200: loss 1.6452, time 7721.28ms, mfu 0.62%\n",
      "iter 205: loss 1.5262, time 870.03ms, mfu 0.68%\n",
      "step 210: train loss 1.6150, val loss 1.6104\n",
      "saving checkpoint to out-abc-char\n",
      "iter 210: loss 1.8132, time 7713.67ms, mfu 0.62%\n",
      "iter 215: loss 1.9715, time 818.55ms, mfu 0.69%\n",
      "step 220: train loss 1.5652, val loss 1.5869\n",
      "saving checkpoint to out-abc-char\n",
      "iter 220: loss 1.7622, time 7589.90ms, mfu 0.63%\n",
      "iter 225: loss 1.5193, time 860.98ms, mfu 0.69%\n",
      "step 230: train loss 1.5302, val loss 1.5904\n",
      "iter 230: loss 1.7658, time 7202.94ms, mfu 0.63%\n",
      "iter 235: loss 1.3774, time 880.03ms, mfu 0.69%\n",
      "step 240: train loss 1.5236, val loss 1.5407\n",
      "saving checkpoint to out-abc-char\n",
      "iter 240: loss 1.3334, time 7428.97ms, mfu 0.63%\n",
      "iter 245: loss 1.3584, time 820.29ms, mfu 0.69%\n",
      "step 250: train loss 1.5504, val loss 1.5590\n",
      "iter 250: loss 1.3413, time 7375.00ms, mfu 0.64%\n",
      "iter 255: loss 1.6353, time 882.73ms, mfu 0.69%\n",
      "step 260: train loss 1.5338, val loss 1.5443\n",
      "iter 260: loss 1.2752, time 7383.97ms, mfu 0.63%\n",
      "iter 265: loss 1.6064, time 886.03ms, mfu 0.69%\n",
      "step 270: train loss 1.5243, val loss 1.5545\n",
      "iter 270: loss 1.7980, time 7290.77ms, mfu 0.63%\n",
      "iter 275: loss 1.4270, time 852.57ms, mfu 0.69%\n",
      "step 280: train loss 1.5129, val loss 1.5273\n",
      "saving checkpoint to out-abc-char\n",
      "iter 280: loss 1.3381, time 7761.05ms, mfu 0.63%\n",
      "iter 285: loss 1.1848, time 886.41ms, mfu 0.68%\n",
      "step 290: train loss 1.5017, val loss 1.5196\n",
      "saving checkpoint to out-abc-char\n",
      "iter 290: loss 1.3254, time 7951.57ms, mfu 0.63%\n",
      "iter 295: loss 1.5596, time 824.62ms, mfu 0.69%\n",
      "step 300: train loss 1.4886, val loss 1.4960\n",
      "saving checkpoint to out-abc-char\n",
      "iter 300: loss 1.5135, time 7591.63ms, mfu 0.63%\n",
      "iter 305: loss 1.8867, time 810.27ms, mfu 0.70%\n",
      "step 310: train loss 1.4796, val loss 1.5038\n",
      "iter 310: loss 1.3318, time 7298.66ms, mfu 0.64%\n",
      "iter 315: loss 1.6547, time 803.51ms, mfu 0.71%\n",
      "step 320: train loss 1.4766, val loss 1.5058\n",
      "iter 320: loss 1.6234, time 7233.34ms, mfu 0.65%\n",
      "iter 325: loss 1.4628, time 830.05ms, mfu 0.71%\n",
      "step 330: train loss 1.4614, val loss 1.5176\n",
      "iter 330: loss 1.1836, time 7525.35ms, mfu 0.65%\n",
      "iter 335: loss 1.2014, time 848.42ms, mfu 0.71%\n",
      "step 340: train loss 1.4765, val loss 1.5043\n",
      "iter 340: loss 1.0043, time 7245.87ms, mfu 0.65%\n",
      "iter 345: loss 1.5009, time 820.94ms, mfu 0.71%\n",
      "step 350: train loss 1.4770, val loss 1.4680\n",
      "saving checkpoint to out-abc-char\n",
      "iter 350: loss 1.5478, time 7539.78ms, mfu 0.65%\n",
      "iter 355: loss 1.7343, time 988.73ms, mfu 0.69%\n",
      "step 360: train loss 1.4598, val loss 1.4661\n",
      "saving checkpoint to out-abc-char\n",
      "iter 360: loss 1.2904, time 7243.13ms, mfu 0.64%\n",
      "iter 365: loss 1.1479, time 810.08ms, mfu 0.70%\n",
      "step 370: train loss 1.4585, val loss 1.4616\n",
      "saving checkpoint to out-abc-char\n",
      "iter 370: loss 1.1660, time 7404.39ms, mfu 0.64%\n",
      "iter 375: loss 1.6177, time 879.35ms, mfu 0.69%\n",
      "step 380: train loss 1.4468, val loss 1.4881\n",
      "iter 380: loss 1.6714, time 7287.85ms, mfu 0.64%\n",
      "iter 385: loss 1.7650, time 846.12ms, mfu 0.70%\n",
      "step 390: train loss 1.4389, val loss 1.4547\n",
      "saving checkpoint to out-abc-char\n",
      "iter 390: loss 1.4733, time 7784.25ms, mfu 0.64%\n",
      "iter 395: loss 1.6212, time 824.99ms, mfu 0.70%\n",
      "step 400: train loss 1.4358, val loss 1.4551\n",
      "iter 400: loss 1.3099, time 7174.70ms, mfu 0.64%\n",
      "iter 405: loss 1.4963, time 840.08ms, mfu 0.70%\n",
      "step 410: train loss 1.4222, val loss 1.4303\n",
      "saving checkpoint to out-abc-char\n",
      "iter 410: loss 1.2004, time 7657.90ms, mfu 0.64%\n",
      "iter 415: loss 1.4925, time 902.12ms, mfu 0.69%\n",
      "step 420: train loss 1.4324, val loss 1.4287\n",
      "saving checkpoint to out-abc-char\n",
      "iter 420: loss 1.6319, time 7657.79ms, mfu 0.64%\n",
      "iter 425: loss 1.1500, time 793.86ms, mfu 0.70%\n",
      "step 430: train loss 1.4028, val loss 1.4166\n",
      "saving checkpoint to out-abc-char\n",
      "iter 430: loss 0.9982, time 7717.97ms, mfu 0.65%\n",
      "iter 435: loss 1.3909, time 853.90ms, mfu 0.70%\n",
      "step 440: train loss 1.3883, val loss 1.3848\n",
      "saving checkpoint to out-abc-char\n",
      "iter 440: loss 0.9871, time 8396.97ms, mfu 0.64%\n",
      "iter 445: loss 0.9317, time 873.64ms, mfu 0.70%\n",
      "step 450: train loss 1.3580, val loss 1.3672\n",
      "saving checkpoint to out-abc-char\n",
      "iter 450: loss 1.6484, time 7612.20ms, mfu 0.64%\n",
      "iter 455: loss 1.5049, time 827.89ms, mfu 0.70%\n",
      "step 460: train loss 1.3392, val loss 1.3699\n",
      "iter 460: loss 1.2396, time 7444.15ms, mfu 0.64%\n",
      "iter 465: loss 0.9781, time 863.35ms, mfu 0.70%\n",
      "step 470: train loss 1.3185, val loss 1.3437\n",
      "saving checkpoint to out-abc-char\n",
      "iter 470: loss 1.2285, time 7849.61ms, mfu 0.64%\n",
      "iter 475: loss 1.1038, time 871.86ms, mfu 0.69%\n",
      "step 480: train loss 1.3027, val loss 1.2974\n",
      "saving checkpoint to out-abc-char\n",
      "iter 480: loss 1.2330, time 7732.97ms, mfu 0.64%\n",
      "iter 485: loss 1.2975, time 854.66ms, mfu 0.69%\n",
      "step 490: train loss 1.2750, val loss 1.2937\n",
      "saving checkpoint to out-abc-char\n",
      "iter 490: loss 1.4014, time 7682.39ms, mfu 0.64%\n",
      "iter 495: loss 1.4110, time 829.21ms, mfu 0.70%\n",
      "step 500: train loss 1.2614, val loss 1.2830\n",
      "saving checkpoint to out-abc-char\n",
      "iter 500: loss 1.5530, time 7816.72ms, mfu 0.64%\n",
      "iter 505: loss 1.0952, time 890.51ms, mfu 0.69%\n",
      "step 510: train loss 1.2409, val loss 1.2489\n",
      "saving checkpoint to out-abc-char\n",
      "iter 510: loss 1.5234, time 7739.74ms, mfu 0.64%\n",
      "iter 515: loss 0.9286, time 900.34ms, mfu 0.69%\n",
      "step 520: train loss 1.2287, val loss 1.2484\n",
      "saving checkpoint to out-abc-char\n",
      "iter 520: loss 1.5801, time 7836.81ms, mfu 0.63%\n",
      "iter 525: loss 1.1553, time 856.49ms, mfu 0.69%\n",
      "step 530: train loss 1.1974, val loss 1.2207\n",
      "saving checkpoint to out-abc-char\n",
      "iter 530: loss 1.3577, time 8051.33ms, mfu 0.63%\n",
      "iter 535: loss 1.0574, time 885.21ms, mfu 0.68%\n",
      "step 540: train loss 1.1915, val loss 1.2037\n",
      "saving checkpoint to out-abc-char\n",
      "iter 540: loss 1.0651, time 7928.22ms, mfu 0.63%\n",
      "iter 545: loss 1.1889, time 940.34ms, mfu 0.67%\n",
      "step 550: train loss 1.1937, val loss 1.1980\n",
      "saving checkpoint to out-abc-char\n",
      "iter 550: loss 1.1356, time 8011.81ms, mfu 0.62%\n",
      "iter 555: loss 1.4148, time 856.14ms, mfu 0.68%\n",
      "step 560: train loss 1.1817, val loss 1.1959\n",
      "saving checkpoint to out-abc-char\n",
      "iter 560: loss 1.3461, time 7513.53ms, mfu 0.62%\n",
      "iter 565: loss 1.1948, time 900.23ms, mfu 0.67%\n",
      "step 570: train loss 1.1507, val loss 1.1851\n",
      "saving checkpoint to out-abc-char\n",
      "iter 570: loss 1.0989, time 7633.65ms, mfu 0.62%\n",
      "iter 575: loss 1.2191, time 831.63ms, mfu 0.68%\n",
      "step 580: train loss 1.1535, val loss 1.1720\n",
      "saving checkpoint to out-abc-char\n",
      "iter 580: loss 0.9681, time 7465.73ms, mfu 0.63%\n",
      "iter 585: loss 1.0099, time 803.53ms, mfu 0.69%\n",
      "step 590: train loss 1.1663, val loss 1.1729\n",
      "iter 590: loss 0.9873, time 7126.60ms, mfu 0.64%\n",
      "iter 595: loss 1.1710, time 828.49ms, mfu 0.70%\n",
      "step 600: train loss 1.1340, val loss 1.1688\n",
      "saving checkpoint to out-abc-char\n",
      "iter 600: loss 1.3604, time 7386.94ms, mfu 0.64%\n",
      "iter 605: loss 1.4165, time 767.89ms, mfu 0.71%\n",
      "step 610: train loss 1.1177, val loss 1.1288\n",
      "saving checkpoint to out-abc-char\n",
      "iter 610: loss 1.0640, time 7544.10ms, mfu 0.65%\n",
      "iter 615: loss 0.9602, time 857.97ms, mfu 0.71%\n",
      "step 620: train loss 1.1097, val loss 1.1303\n",
      "iter 620: loss 1.2250, time 7482.56ms, mfu 0.65%\n",
      "iter 625: loss 1.7979, time 843.15ms, mfu 0.71%\n",
      "step 630: train loss 1.1259, val loss 1.1399\n",
      "iter 630: loss 1.0005, time 7374.29ms, mfu 0.65%\n",
      "iter 635: loss 0.9109, time 818.55ms, mfu 0.71%\n",
      "step 640: train loss 1.1146, val loss 1.1552\n",
      "iter 640: loss 1.0757, time 7307.74ms, mfu 0.65%\n",
      "iter 645: loss 1.1767, time 873.03ms, mfu 0.70%\n",
      "step 650: train loss 1.0997, val loss 1.1259\n",
      "saving checkpoint to out-abc-char\n",
      "iter 650: loss 0.9178, time 7681.95ms, mfu 0.65%\n",
      "iter 655: loss 1.0913, time 879.28ms, mfu 0.70%\n",
      "step 660: train loss 1.1045, val loss 1.1145\n",
      "saving checkpoint to out-abc-char\n",
      "iter 660: loss 1.3873, time 7886.26ms, mfu 0.64%\n",
      "iter 665: loss 0.8587, time 875.71ms, mfu 0.69%\n",
      "step 670: train loss 1.0885, val loss 1.1035\n",
      "saving checkpoint to out-abc-char\n",
      "iter 670: loss 0.9462, time 7551.31ms, mfu 0.64%\n",
      "iter 675: loss 1.2747, time 795.22ms, mfu 0.70%\n",
      "step 680: train loss 1.0745, val loss 1.1039\n",
      "iter 680: loss 1.0447, time 7598.49ms, mfu 0.65%\n",
      "iter 685: loss 1.1102, time 895.56ms, mfu 0.70%\n",
      "step 690: train loss 1.0746, val loss 1.0994\n",
      "saving checkpoint to out-abc-char\n",
      "iter 690: loss 1.3451, time 7804.07ms, mfu 0.64%\n",
      "iter 695: loss 1.1004, time 866.60ms, mfu 0.69%\n",
      "step 700: train loss 1.0595, val loss 1.0915\n",
      "saving checkpoint to out-abc-char\n",
      "iter 700: loss 1.0521, time 7973.61ms, mfu 0.64%\n",
      "iter 705: loss 1.0919, time 946.22ms, mfu 0.68%\n",
      "step 710: train loss 1.0540, val loss 1.0768\n",
      "saving checkpoint to out-abc-char\n",
      "iter 710: loss 0.9996, time 7791.72ms, mfu 0.63%\n",
      "iter 715: loss 1.3238, time 836.04ms, mfu 0.69%\n",
      "step 720: train loss 1.0506, val loss 1.0843\n",
      "iter 720: loss 0.8887, time 7448.31ms, mfu 0.63%\n",
      "iter 725: loss 1.2630, time 914.85ms, mfu 0.68%\n",
      "step 730: train loss 1.0394, val loss 1.0764\n",
      "saving checkpoint to out-abc-char\n",
      "iter 730: loss 0.6990, time 8009.69ms, mfu 0.62%\n",
      "iter 735: loss 1.5947, time 855.98ms, mfu 0.68%\n",
      "step 740: train loss 1.0404, val loss 1.0629\n",
      "saving checkpoint to out-abc-char\n",
      "iter 740: loss 0.7599, time 8048.02ms, mfu 0.63%\n",
      "iter 745: loss 0.9796, time 929.52ms, mfu 0.67%\n",
      "step 750: train loss 1.0355, val loss 1.0465\n",
      "saving checkpoint to out-abc-char\n",
      "iter 750: loss 1.2133, time 7916.51ms, mfu 0.62%\n",
      "iter 755: loss 0.8549, time 816.20ms, mfu 0.68%\n",
      "step 760: train loss 1.0143, val loss 1.0467\n",
      "iter 760: loss 0.8825, time 7506.31ms, mfu 0.63%\n",
      "iter 765: loss 0.7788, time 858.63ms, mfu 0.68%\n",
      "step 770: train loss 0.9923, val loss 1.0437\n",
      "saving checkpoint to out-abc-char\n",
      "iter 770: loss 1.2233, time 7879.21ms, mfu 0.63%\n",
      "iter 775: loss 0.9329, time 956.58ms, mfu 0.67%\n",
      "step 780: train loss 1.0093, val loss 1.0397\n",
      "saving checkpoint to out-abc-char\n",
      "iter 780: loss 1.0573, time 7811.10ms, mfu 0.62%\n",
      "iter 785: loss 1.4583, time 896.28ms, mfu 0.67%\n",
      "step 790: train loss 1.0046, val loss 1.0399\n",
      "iter 790: loss 1.1479, time 7664.64ms, mfu 0.62%\n",
      "iter 795: loss 0.8034, time 894.07ms, mfu 0.67%\n",
      "step 800: train loss 0.9724, val loss 0.9933\n",
      "saving checkpoint to out-abc-char\n",
      "iter 800: loss 1.1482, time 7747.81ms, mfu 0.62%\n",
      "iter 805: loss 0.9145, time 862.17ms, mfu 0.67%\n",
      "step 810: train loss 0.9655, val loss 1.0049\n",
      "iter 810: loss 0.7204, time 7604.37ms, mfu 0.62%\n",
      "iter 815: loss 1.2690, time 864.17ms, mfu 0.68%\n",
      "step 820: train loss 0.9599, val loss 0.9946\n",
      "iter 820: loss 0.9439, time 7335.20ms, mfu 0.62%\n",
      "iter 825: loss 0.7379, time 940.03ms, mfu 0.67%\n",
      "step 830: train loss 0.9604, val loss 0.9859\n",
      "saving checkpoint to out-abc-char\n",
      "iter 830: loss 1.3410, time 7434.11ms, mfu 0.62%\n",
      "iter 835: loss 1.1750, time 768.36ms, mfu 0.69%\n",
      "step 840: train loss 0.9562, val loss 0.9853\n",
      "saving checkpoint to out-abc-char\n",
      "iter 840: loss 0.9868, time 7721.72ms, mfu 0.63%\n",
      "iter 845: loss 0.7889, time 914.33ms, mfu 0.68%\n",
      "step 850: train loss 0.9438, val loss 0.9787\n",
      "saving checkpoint to out-abc-char\n",
      "iter 850: loss 0.8186, time 7496.24ms, mfu 0.63%\n",
      "iter 855: loss 0.9693, time 970.87ms, mfu 0.67%\n",
      "step 860: train loss 0.9341, val loss 0.9652\n",
      "saving checkpoint to out-abc-char\n",
      "iter 860: loss 0.7454, time 7731.66ms, mfu 0.62%\n",
      "iter 865: loss 1.2836, time 840.95ms, mfu 0.68%\n",
      "step 870: train loss 0.9185, val loss 0.9719\n",
      "iter 870: loss 0.7057, time 7555.39ms, mfu 0.62%\n",
      "iter 875: loss 0.9715, time 835.53ms, mfu 0.68%\n",
      "step 880: train loss 0.9208, val loss 0.9459\n",
      "saving checkpoint to out-abc-char\n",
      "iter 880: loss 0.8923, time 7807.36ms, mfu 0.63%\n",
      "iter 885: loss 0.7603, time 833.29ms, mfu 0.69%\n",
      "step 890: train loss 0.9008, val loss 0.9304\n",
      "saving checkpoint to out-abc-char\n",
      "iter 890: loss 0.8919, time 7844.35ms, mfu 0.63%\n",
      "iter 895: loss 0.9668, time 900.92ms, mfu 0.68%\n",
      "step 900: train loss 0.9003, val loss 0.9444\n",
      "iter 900: loss 0.9219, time 7706.95ms, mfu 0.63%\n",
      "iter 905: loss 1.0736, time 884.21ms, mfu 0.68%\n",
      "step 910: train loss 0.8870, val loss 0.9347\n",
      "iter 910: loss 0.7803, time 7591.08ms, mfu 0.63%\n",
      "iter 915: loss 0.9765, time 950.93ms, mfu 0.67%\n",
      "step 920: train loss 0.8909, val loss 0.9279\n",
      "saving checkpoint to out-abc-char\n",
      "iter 920: loss 0.6727, time 8048.89ms, mfu 0.62%\n",
      "iter 925: loss 0.9869, time 970.32ms, mfu 0.66%\n",
      "step 930: train loss 0.8792, val loss 0.9238\n",
      "saving checkpoint to out-abc-char\n",
      "iter 930: loss 0.8726, time 7972.19ms, mfu 0.61%\n",
      "iter 935: loss 0.8335, time 869.34ms, mfu 0.66%\n",
      "step 940: train loss 0.8797, val loss 0.8998\n",
      "saving checkpoint to out-abc-char\n",
      "iter 940: loss 1.2781, time 7927.37ms, mfu 0.61%\n",
      "iter 945: loss 1.0714, time 854.68ms, mfu 0.67%\n",
      "step 950: train loss 0.8606, val loss 0.8935\n",
      "saving checkpoint to out-abc-char\n",
      "iter 950: loss 0.9294, time 7727.02ms, mfu 0.62%\n",
      "iter 955: loss 0.9065, time 826.77ms, mfu 0.68%\n",
      "step 960: train loss 0.8629, val loss 0.9023\n",
      "iter 960: loss 0.8388, time 7536.97ms, mfu 0.62%\n",
      "iter 965: loss 0.7786, time 834.43ms, mfu 0.68%\n",
      "step 970: train loss 0.8516, val loss 0.8959\n",
      "iter 970: loss 0.9975, time 7511.47ms, mfu 0.63%\n",
      "iter 975: loss 1.0575, time 820.20ms, mfu 0.69%\n",
      "step 980: train loss 0.8521, val loss 0.8817\n",
      "saving checkpoint to out-abc-char\n",
      "iter 980: loss 0.6669, time 7700.77ms, mfu 0.63%\n",
      "iter 985: loss 0.7668, time 873.50ms, mfu 0.69%\n",
      "step 990: train loss 0.8383, val loss 0.8718\n",
      "saving checkpoint to out-abc-char\n",
      "iter 990: loss 1.0416, time 7795.50ms, mfu 0.63%\n",
      "iter 995: loss 0.9830, time 909.60ms, mfu 0.68%\n",
      "step 1000: train loss 0.8308, val loss 0.8743\n",
      "iter 1000: loss 1.1540, time 7482.62ms, mfu 0.63%\n",
      "iter 1005: loss 0.6525, time 903.76ms, mfu 0.68%\n",
      "step 1010: train loss 0.8302, val loss 0.8642\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1010: loss 0.8038, time 8046.63ms, mfu 0.62%\n",
      "iter 1015: loss 0.7799, time 914.96ms, mfu 0.67%\n",
      "step 1020: train loss 0.8156, val loss 0.8490\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1020: loss 0.6207, time 7879.61ms, mfu 0.62%\n",
      "iter 1025: loss 0.8219, time 829.88ms, mfu 0.68%\n",
      "step 1030: train loss 0.8083, val loss 0.8468\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1030: loss 0.7957, time 7864.97ms, mfu 0.62%\n",
      "iter 1035: loss 0.6812, time 874.02ms, mfu 0.68%\n",
      "step 1040: train loss 0.8012, val loss 0.8445\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1040: loss 1.0247, time 7868.65ms, mfu 0.62%\n",
      "iter 1045: loss 0.7988, time 901.98ms, mfu 0.68%\n",
      "step 1050: train loss 0.7952, val loss 0.8301\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1050: loss 0.6209, time 7400.21ms, mfu 0.62%\n",
      "iter 1055: loss 0.9337, time 1024.32ms, mfu 0.66%\n",
      "step 1060: train loss 0.7845, val loss 0.8272\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1060: loss 0.8094, time 7374.15ms, mfu 0.61%\n",
      "iter 1065: loss 0.9481, time 819.80ms, mfu 0.67%\n",
      "step 1070: train loss 0.7809, val loss 0.8340\n",
      "iter 1070: loss 1.0593, time 7114.74ms, mfu 0.62%\n",
      "iter 1075: loss 0.9010, time 804.95ms, mfu 0.68%\n",
      "step 1080: train loss 0.7797, val loss 0.8101\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1080: loss 0.7174, time 7401.01ms, mfu 0.63%\n",
      "iter 1085: loss 0.5885, time 859.13ms, mfu 0.69%\n",
      "step 1090: train loss 0.7750, val loss 0.7925\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1090: loss 0.9320, time 7447.27ms, mfu 0.63%\n",
      "iter 1095: loss 0.6419, time 813.57ms, mfu 0.69%\n",
      "step 1100: train loss 0.7784, val loss 0.7946\n",
      "iter 1100: loss 0.4749, time 7164.66ms, mfu 0.64%\n",
      "iter 1105: loss 0.8981, time 876.32ms, mfu 0.69%\n",
      "step 1110: train loss 0.7527, val loss 0.7958\n",
      "iter 1110: loss 0.7229, time 7198.02ms, mfu 0.64%\n",
      "iter 1115: loss 0.7581, time 879.84ms, mfu 0.69%\n",
      "step 1120: train loss 0.7547, val loss 0.7907\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1120: loss 0.6614, time 7211.38ms, mfu 0.63%\n",
      "iter 1125: loss 0.5698, time 832.97ms, mfu 0.69%\n",
      "step 1130: train loss 0.7659, val loss 0.8052\n",
      "iter 1130: loss 0.6700, time 6893.41ms, mfu 0.64%\n",
      "iter 1135: loss 0.7815, time 824.02ms, mfu 0.70%\n",
      "step 1140: train loss 0.7500, val loss 0.7629\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1140: loss 0.6829, time 7288.97ms, mfu 0.64%\n",
      "iter 1145: loss 0.5886, time 802.04ms, mfu 0.71%\n",
      "step 1150: train loss 0.7353, val loss 0.8022\n",
      "iter 1150: loss 1.0056, time 7318.94ms, mfu 0.65%\n",
      "iter 1155: loss 0.6846, time 818.99ms, mfu 0.71%\n",
      "step 1160: train loss 0.7344, val loss 0.7777\n",
      "iter 1160: loss 0.7748, time 7089.36ms, mfu 0.65%\n",
      "iter 1165: loss 0.7990, time 810.30ms, mfu 0.71%\n",
      "step 1170: train loss 0.7265, val loss 0.7925\n",
      "iter 1170: loss 0.6722, time 7038.05ms, mfu 0.66%\n",
      "iter 1175: loss 0.6693, time 857.89ms, mfu 0.71%\n",
      "step 1180: train loss 0.7147, val loss 0.7494\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1180: loss 0.5887, time 7744.43ms, mfu 0.65%\n",
      "iter 1185: loss 0.6288, time 857.97ms, mfu 0.71%\n",
      "step 1190: train loss 0.7158, val loss 0.7506\n",
      "iter 1190: loss 0.9015, time 7415.90ms, mfu 0.65%\n",
      "iter 1195: loss 0.5862, time 858.85ms, mfu 0.70%\n",
      "step 1200: train loss 0.7137, val loss 0.7530\n",
      "iter 1200: loss 0.9972, time 7226.03ms, mfu 0.65%\n",
      "iter 1205: loss 0.8400, time 816.17ms, mfu 0.71%\n",
      "step 1210: train loss 0.7190, val loss 0.7548\n",
      "iter 1210: loss 0.8750, time 7131.04ms, mfu 0.65%\n",
      "iter 1215: loss 0.6040, time 854.77ms, mfu 0.71%\n",
      "step 1220: train loss 0.6994, val loss 0.7557\n",
      "iter 1220: loss 0.8042, time 7367.85ms, mfu 0.65%\n",
      "iter 1225: loss 0.7804, time 808.87ms, mfu 0.71%\n",
      "step 1230: train loss 0.6937, val loss 0.7546\n",
      "iter 1230: loss 0.7703, time 7343.30ms, mfu 0.65%\n",
      "iter 1235: loss 0.6917, time 866.53ms, mfu 0.71%\n",
      "step 1240: train loss 0.7017, val loss 0.7368\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1240: loss 0.5561, time 7552.97ms, mfu 0.65%\n",
      "iter 1245: loss 0.5084, time 905.93ms, mfu 0.70%\n",
      "step 1250: train loss 0.6956, val loss 0.7320\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1250: loss 0.6763, time 7545.39ms, mfu 0.64%\n",
      "iter 1255: loss 0.7845, time 842.81ms, mfu 0.70%\n",
      "step 1260: train loss 0.6906, val loss 0.7307\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1260: loss 0.9314, time 7862.44ms, mfu 0.64%\n",
      "iter 1265: loss 0.8997, time 868.23ms, mfu 0.70%\n",
      "step 1270: train loss 0.6818, val loss 0.7389\n",
      "iter 1270: loss 0.7464, time 7350.74ms, mfu 0.64%\n",
      "iter 1275: loss 0.7036, time 844.34ms, mfu 0.70%\n",
      "step 1280: train loss 0.6784, val loss 0.7316\n",
      "iter 1280: loss 0.8097, time 7295.63ms, mfu 0.64%\n",
      "iter 1285: loss 0.7098, time 875.94ms, mfu 0.69%\n",
      "step 1290: train loss 0.6697, val loss 0.7171\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1290: loss 0.7792, time 7529.06ms, mfu 0.64%\n",
      "iter 1295: loss 0.7752, time 945.31ms, mfu 0.68%\n",
      "step 1300: train loss 0.6875, val loss 0.7190\n",
      "iter 1300: loss 0.9730, time 7036.12ms, mfu 0.63%\n",
      "iter 1305: loss 0.8837, time 831.42ms, mfu 0.69%\n",
      "step 1310: train loss 0.6714, val loss 0.7189\n",
      "iter 1310: loss 0.8455, time 6669.74ms, mfu 0.64%\n",
      "iter 1315: loss 0.7257, time 794.48ms, mfu 0.70%\n",
      "step 1320: train loss 0.6546, val loss 0.7043\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1320: loss 0.7692, time 7064.00ms, mfu 0.65%\n",
      "iter 1325: loss 0.7249, time 823.14ms, mfu 0.70%\n",
      "step 1330: train loss 0.6678, val loss 0.7060\n",
      "iter 1330: loss 0.8963, time 7466.22ms, mfu 0.65%\n",
      "iter 1335: loss 0.8997, time 892.19ms, mfu 0.70%\n",
      "step 1340: train loss 0.6581, val loss 0.7016\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1340: loss 0.8769, time 7706.10ms, mfu 0.64%\n",
      "iter 1345: loss 0.6361, time 813.36ms, mfu 0.70%\n",
      "step 1350: train loss 0.6596, val loss 0.7108\n",
      "iter 1350: loss 0.5797, time 7063.96ms, mfu 0.65%\n",
      "iter 1355: loss 0.6857, time 839.91ms, mfu 0.70%\n",
      "step 1360: train loss 0.6511, val loss 0.7019\n",
      "iter 1360: loss 0.6829, time 7089.97ms, mfu 0.65%\n",
      "iter 1365: loss 0.5816, time 825.43ms, mfu 0.71%\n",
      "step 1370: train loss 0.6468, val loss 0.6998\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1370: loss 0.7261, time 7412.80ms, mfu 0.65%\n",
      "iter 1375: loss 0.7292, time 834.90ms, mfu 0.71%\n",
      "step 1380: train loss 0.6505, val loss 0.6837\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1380: loss 0.5113, time 7438.42ms, mfu 0.65%\n",
      "iter 1385: loss 0.7457, time 846.03ms, mfu 0.71%\n",
      "step 1390: train loss 0.6480, val loss 0.6951\n",
      "iter 1390: loss 0.7381, time 7133.04ms, mfu 0.65%\n",
      "iter 1395: loss 0.5587, time 823.95ms, mfu 0.71%\n",
      "step 1400: train loss 0.6476, val loss 0.6994\n",
      "iter 1400: loss 0.5348, time 7200.17ms, mfu 0.65%\n",
      "iter 1405: loss 0.7516, time 809.64ms, mfu 0.71%\n",
      "step 1410: train loss 0.6414, val loss 0.6857\n",
      "iter 1410: loss 0.6481, time 7354.26ms, mfu 0.66%\n",
      "iter 1415: loss 0.4979, time 951.45ms, mfu 0.70%\n",
      "step 1420: train loss 0.6420, val loss 0.6636\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1420: loss 0.6021, time 7686.54ms, mfu 0.64%\n",
      "iter 1425: loss 0.5138, time 837.90ms, mfu 0.70%\n",
      "step 1430: train loss 0.6350, val loss 0.6690\n",
      "iter 1430: loss 0.6740, time 7160.29ms, mfu 0.64%\n",
      "iter 1435: loss 0.5226, time 825.19ms, mfu 0.70%\n",
      "step 1440: train loss 0.6244, val loss 0.6669\n",
      "iter 1440: loss 0.6542, time 7322.52ms, mfu 0.65%\n",
      "iter 1445: loss 0.5478, time 866.93ms, mfu 0.70%\n",
      "step 1450: train loss 0.6326, val loss 0.6791\n",
      "iter 1450: loss 0.4712, time 7096.83ms, mfu 0.64%\n",
      "iter 1455: loss 0.5818, time 839.47ms, mfu 0.70%\n",
      "step 1460: train loss 0.6238, val loss 0.6824\n",
      "iter 1460: loss 0.6067, time 7367.93ms, mfu 0.65%\n",
      "iter 1465: loss 0.7258, time 846.18ms, mfu 0.70%\n",
      "step 1470: train loss 0.6235, val loss 0.6731\n",
      "iter 1470: loss 0.7084, time 7337.69ms, mfu 0.65%\n",
      "iter 1475: loss 0.7645, time 832.78ms, mfu 0.70%\n",
      "step 1480: train loss 0.6159, val loss 0.6669\n",
      "iter 1480: loss 0.4593, time 7188.96ms, mfu 0.65%\n",
      "iter 1485: loss 0.4452, time 848.03ms, mfu 0.70%\n",
      "step 1490: train loss 0.6180, val loss 0.6691\n",
      "iter 1490: loss 0.4491, time 7399.81ms, mfu 0.65%\n",
      "iter 1495: loss 0.6114, time 873.94ms, mfu 0.70%\n",
      "step 1500: train loss 0.6192, val loss 0.6577\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1500: loss 0.5058, time 7476.23ms, mfu 0.64%\n",
      "iter 1505: loss 0.5725, time 798.16ms, mfu 0.71%\n",
      "step 1510: train loss 0.6150, val loss 0.6684\n",
      "iter 1510: loss 0.7510, time 6977.83ms, mfu 0.65%\n",
      "iter 1515: loss 0.4640, time 773.45ms, mfu 0.72%\n",
      "step 1520: train loss 0.6078, val loss 0.6583\n",
      "iter 1520: loss 0.6612, time 7301.82ms, mfu 0.66%\n",
      "iter 1525: loss 0.6731, time 879.30ms, mfu 0.71%\n",
      "step 1530: train loss 0.6104, val loss 0.6702\n",
      "iter 1530: loss 0.5218, time 6912.97ms, mfu 0.65%\n",
      "iter 1535: loss 0.4568, time 835.24ms, mfu 0.71%\n",
      "step 1540: train loss 0.6008, val loss 0.6430\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1540: loss 0.4401, time 6722.96ms, mfu 0.66%\n",
      "iter 1545: loss 0.6770, time 845.79ms, mfu 0.71%\n",
      "step 1550: train loss 0.6106, val loss 0.6648\n",
      "iter 1550: loss 0.6193, time 6883.13ms, mfu 0.65%\n",
      "iter 1555: loss 0.6576, time 828.39ms, mfu 0.71%\n",
      "step 1560: train loss 0.6058, val loss 0.6459\n",
      "iter 1560: loss 0.3774, time 6782.58ms, mfu 0.66%\n",
      "iter 1565: loss 0.4049, time 783.89ms, mfu 0.72%\n",
      "step 1570: train loss 0.6070, val loss 0.6538\n",
      "iter 1570: loss 0.4665, time 7392.20ms, mfu 0.66%\n",
      "iter 1575: loss 0.5803, time 839.61ms, mfu 0.72%\n",
      "step 1580: train loss 0.6025, val loss 0.6499\n",
      "iter 1580: loss 0.7384, time 7343.26ms, mfu 0.66%\n",
      "iter 1585: loss 0.8538, time 946.78ms, mfu 0.70%\n",
      "step 1590: train loss 0.5960, val loss 0.6482\n",
      "iter 1590: loss 0.5255, time 7818.06ms, mfu 0.65%\n",
      "iter 1595: loss 0.6037, time 824.19ms, mfu 0.71%\n",
      "step 1600: train loss 0.5900, val loss 0.6458\n",
      "iter 1600: loss 0.6267, time 7356.90ms, mfu 0.65%\n",
      "iter 1605: loss 0.3721, time 873.43ms, mfu 0.70%\n",
      "step 1610: train loss 0.5990, val loss 0.6317\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1610: loss 0.7061, time 7233.98ms, mfu 0.64%\n",
      "iter 1615: loss 0.6830, time 824.28ms, mfu 0.70%\n",
      "step 1620: train loss 0.5881, val loss 0.6312\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1620: loss 0.5196, time 7134.80ms, mfu 0.65%\n",
      "iter 1625: loss 0.6741, time 794.31ms, mfu 0.71%\n",
      "step 1630: train loss 0.5841, val loss 0.6407\n",
      "iter 1630: loss 0.5691, time 7135.86ms, mfu 0.66%\n",
      "iter 1635: loss 0.6622, time 839.08ms, mfu 0.71%\n",
      "step 1640: train loss 0.5697, val loss 0.6332\n",
      "iter 1640: loss 0.5941, time 7036.27ms, mfu 0.66%\n",
      "iter 1645: loss 0.8698, time 770.89ms, mfu 0.72%\n",
      "step 1650: train loss 0.5791, val loss 0.6297\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1650: loss 0.5791, time 7909.15ms, mfu 0.66%\n",
      "iter 1655: loss 0.5490, time 853.71ms, mfu 0.72%\n",
      "step 1660: train loss 0.5747, val loss 0.6402\n",
      "iter 1660: loss 0.6412, time 7579.49ms, mfu 0.66%\n",
      "iter 1665: loss 0.4949, time 881.23ms, mfu 0.71%\n",
      "step 1670: train loss 0.5804, val loss 0.6399\n",
      "iter 1670: loss 0.5099, time 7970.70ms, mfu 0.65%\n",
      "iter 1675: loss 0.6556, time 826.15ms, mfu 0.71%\n",
      "step 1680: train loss 0.5815, val loss 0.6315\n",
      "iter 1680: loss 0.7144, time 7166.38ms, mfu 0.65%\n",
      "iter 1685: loss 0.6310, time 859.97ms, mfu 0.71%\n",
      "step 1690: train loss 0.5910, val loss 0.6270\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1690: loss 0.4923, time 7464.14ms, mfu 0.65%\n",
      "iter 1695: loss 0.5327, time 857.13ms, mfu 0.70%\n",
      "step 1700: train loss 0.5745, val loss 0.6257\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1700: loss 0.6327, time 7780.33ms, mfu 0.65%\n",
      "iter 1705: loss 0.4588, time 974.61ms, mfu 0.69%\n",
      "step 1710: train loss 0.5655, val loss 0.6323\n",
      "iter 1710: loss 0.6856, time 7728.06ms, mfu 0.63%\n",
      "iter 1715: loss 0.5142, time 987.47ms, mfu 0.67%\n",
      "step 1720: train loss 0.5703, val loss 0.6264\n",
      "iter 1720: loss 0.5723, time 7927.71ms, mfu 0.62%\n",
      "iter 1725: loss 0.5987, time 961.36ms, mfu 0.66%\n",
      "step 1730: train loss 0.5561, val loss 0.6172\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1730: loss 0.4737, time 7618.87ms, mfu 0.61%\n",
      "iter 1735: loss 0.7348, time 956.88ms, mfu 0.66%\n",
      "step 1740: train loss 0.5761, val loss 0.6105\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1740: loss 0.5803, time 7680.16ms, mfu 0.60%\n",
      "iter 1745: loss 0.6052, time 882.00ms, mfu 0.66%\n",
      "step 1750: train loss 0.5673, val loss 0.6282\n",
      "iter 1750: loss 0.5858, time 7386.56ms, mfu 0.61%\n",
      "iter 1755: loss 0.6797, time 853.94ms, mfu 0.67%\n",
      "step 1760: train loss 0.5583, val loss 0.6182\n",
      "iter 1760: loss 0.6275, time 7341.28ms, mfu 0.61%\n",
      "iter 1765: loss 0.5931, time 816.27ms, mfu 0.68%\n",
      "step 1770: train loss 0.5597, val loss 0.6279\n",
      "iter 1770: loss 0.5216, time 6646.14ms, mfu 0.63%\n",
      "iter 1775: loss 0.5533, time 884.64ms, mfu 0.68%\n",
      "step 1780: train loss 0.5645, val loss 0.6103\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1780: loss 0.5299, time 6845.13ms, mfu 0.63%\n",
      "iter 1785: loss 0.5267, time 871.52ms, mfu 0.68%\n",
      "step 1790: train loss 0.5542, val loss 0.6191\n",
      "iter 1790: loss 0.6853, time 6657.53ms, mfu 0.63%\n",
      "iter 1795: loss 0.4729, time 779.77ms, mfu 0.70%\n",
      "step 1800: train loss 0.5525, val loss 0.6052\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1800: loss 0.6648, time 7034.06ms, mfu 0.64%\n",
      "iter 1805: loss 0.5438, time 869.96ms, mfu 0.69%\n",
      "step 1810: train loss 0.5545, val loss 0.6038\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1810: loss 0.6793, time 7599.72ms, mfu 0.64%\n",
      "iter 1815: loss 0.4004, time 894.51ms, mfu 0.69%\n",
      "step 1820: train loss 0.5530, val loss 0.6112\n",
      "iter 1820: loss 0.5761, time 7287.04ms, mfu 0.63%\n",
      "iter 1825: loss 0.6402, time 790.21ms, mfu 0.70%\n",
      "step 1830: train loss 0.5591, val loss 0.5958\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1830: loss 0.4603, time 7673.24ms, mfu 0.64%\n",
      "iter 1835: loss 0.6533, time 833.32ms, mfu 0.70%\n",
      "step 1840: train loss 0.5461, val loss 0.6082\n",
      "iter 1840: loss 0.5482, time 7428.61ms, mfu 0.65%\n",
      "iter 1845: loss 0.5425, time 776.35ms, mfu 0.71%\n",
      "step 1850: train loss 0.5430, val loss 0.6079\n",
      "iter 1850: loss 0.7677, time 6897.14ms, mfu 0.66%\n",
      "iter 1855: loss 0.6348, time 786.87ms, mfu 0.72%\n",
      "step 1860: train loss 0.5433, val loss 0.5925\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1860: loss 0.4746, time 7225.90ms, mfu 0.66%\n",
      "iter 1865: loss 0.6268, time 780.97ms, mfu 0.73%\n",
      "step 1870: train loss 0.5562, val loss 0.6108\n",
      "iter 1870: loss 0.6601, time 7364.17ms, mfu 0.67%\n",
      "iter 1875: loss 0.6100, time 822.67ms, mfu 0.73%\n",
      "step 1880: train loss 0.5307, val loss 0.6003\n",
      "iter 1880: loss 0.7111, time 7286.45ms, mfu 0.67%\n",
      "iter 1885: loss 0.3834, time 772.22ms, mfu 0.73%\n",
      "step 1890: train loss 0.5391, val loss 0.5951\n",
      "iter 1890: loss 0.5141, time 7386.99ms, mfu 0.67%\n",
      "iter 1895: loss 0.5454, time 924.41ms, mfu 0.72%\n",
      "step 1900: train loss 0.5443, val loss 0.5857\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1900: loss 0.5497, time 7560.63ms, mfu 0.66%\n",
      "iter 1905: loss 0.5827, time 791.83ms, mfu 0.72%\n",
      "step 1910: train loss 0.5376, val loss 0.5976\n",
      "iter 1910: loss 0.6033, time 7283.98ms, mfu 0.66%\n",
      "iter 1915: loss 0.3943, time 791.23ms, mfu 0.73%\n",
      "step 1920: train loss 0.5346, val loss 0.5909\n",
      "iter 1920: loss 0.7816, time 7271.21ms, mfu 0.67%\n",
      "iter 1925: loss 0.6699, time 834.77ms, mfu 0.72%\n",
      "step 1930: train loss 0.5332, val loss 0.5839\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1930: loss 0.4764, time 7803.73ms, mfu 0.66%\n",
      "iter 1935: loss 0.5241, time 770.88ms, mfu 0.73%\n",
      "step 1940: train loss 0.5315, val loss 0.5847\n",
      "iter 1940: loss 0.4345, time 7406.74ms, mfu 0.67%\n",
      "iter 1945: loss 0.5667, time 835.76ms, mfu 0.73%\n",
      "step 1950: train loss 0.5175, val loss 0.5911\n",
      "iter 1950: loss 0.7907, time 7631.18ms, mfu 0.67%\n",
      "iter 1955: loss 0.5375, time 868.63ms, mfu 0.72%\n",
      "step 1960: train loss 0.5233, val loss 0.5823\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1960: loss 0.6692, time 7882.11ms, mfu 0.66%\n",
      "iter 1965: loss 0.5268, time 898.76ms, mfu 0.71%\n",
      "step 1970: train loss 0.5300, val loss 0.5749\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1970: loss 0.4791, time 8271.47ms, mfu 0.65%\n",
      "iter 1975: loss 0.3951, time 909.94ms, mfu 0.70%\n",
      "step 1980: train loss 0.5264, val loss 0.5738\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1980: loss 0.7408, time 7559.33ms, mfu 0.64%\n",
      "iter 1985: loss 0.4818, time 887.52ms, mfu 0.69%\n",
      "step 1990: train loss 0.5109, val loss 0.5809\n",
      "iter 1990: loss 0.3285, time 7268.97ms, mfu 0.64%\n",
      "iter 1995: loss 0.6010, time 833.78ms, mfu 0.70%\n",
      "step 2000: train loss 0.5264, val loss 0.5724\n",
      "saving checkpoint to out-abc-char\n",
      "iter 2000: loss 0.6264, time 7629.57ms, mfu 0.64%\n",
      "iter 2005: loss 0.4185, time 755.75ms, mfu 0.71%\n",
      "step 2010: train loss 0.5239, val loss 0.5789\n",
      "iter 2010: loss 0.4557, time 6584.26ms, mfu 0.66%\n",
      "iter 2015: loss 0.4606, time 837.54ms, mfu 0.71%\n",
      "step 2020: train loss 0.5227, val loss 0.5818\n",
      "iter 2020: loss 0.6535, time 6534.21ms, mfu 0.66%\n",
      "iter 2025: loss 0.5799, time 788.20ms, mfu 0.72%\n",
      "step 2030: train loss 0.5152, val loss 0.5756\n",
      "iter 2030: loss 0.6896, time 7433.52ms, mfu 0.66%\n",
      "iter 2035: loss 0.5607, time 800.07ms, mfu 0.72%\n",
      "step 2040: train loss 0.5181, val loss 0.5766\n",
      "iter 2040: loss 0.5086, time 6938.81ms, mfu 0.67%\n",
      "iter 2045: loss 0.4711, time 925.00ms, mfu 0.71%\n",
      "step 2050: train loss 0.5176, val loss 0.5769\n",
      "iter 2050: loss 0.6999, time 7484.28ms, mfu 0.65%\n",
      "iter 2055: loss 0.5881, time 869.00ms, mfu 0.71%\n",
      "step 2060: train loss 0.5024, val loss 0.5656\n",
      "saving checkpoint to out-abc-char\n",
      "iter 2060: loss 0.5285, time 7741.14ms, mfu 0.65%\n",
      "iter 2065: loss 0.5931, time 858.68ms, mfu 0.70%\n",
      "step 2070: train loss 0.5178, val loss 0.5690\n",
      "iter 2070: loss 0.4789, time 7601.70ms, mfu 0.65%\n",
      "iter 2075: loss 0.4969, time 792.97ms, mfu 0.71%\n",
      "step 2080: train loss 0.5104, val loss 0.5752\n",
      "iter 2080: loss 0.4281, time 7324.34ms, mfu 0.65%\n",
      "iter 2085: loss 0.4824, time 874.10ms, mfu 0.70%\n",
      "step 2090: train loss 0.5109, val loss 0.5725\n",
      "iter 2090: loss 0.7023, time 7679.30ms, mfu 0.65%\n",
      "iter 2095: loss 0.4775, time 852.48ms, mfu 0.70%\n",
      "step 2100: train loss 0.5127, val loss 0.5657\n",
      "iter 2100: loss 0.4970, time 7598.99ms, mfu 0.65%\n",
      "iter 2105: loss 0.6034, time 868.35ms, mfu 0.70%\n",
      "step 2110: train loss 0.5124, val loss 0.5535\n",
      "saving checkpoint to out-abc-char\n",
      "iter 2110: loss 0.4315, time 7812.29ms, mfu 0.64%\n",
      "iter 2115: loss 0.5059, time 924.22ms, mfu 0.69%\n",
      "step 2120: train loss 0.5002, val loss 0.5732\n",
      "iter 2120: loss 0.5090, time 7934.80ms, mfu 0.63%\n",
      "iter 2125: loss 0.5017, time 926.94ms, mfu 0.68%\n",
      "step 2130: train loss 0.4976, val loss 0.5457\n",
      "saving checkpoint to out-abc-char\n",
      "iter 2130: loss 0.5654, time 7784.30ms, mfu 0.63%\n",
      "iter 2135: loss 0.6378, time 874.57ms, mfu 0.68%\n",
      "step 2140: train loss 0.5036, val loss 0.5562\n",
      "iter 2140: loss 0.5588, time 7364.62ms, mfu 0.63%\n",
      "iter 2145: loss 0.4129, time 915.73ms, mfu 0.67%\n",
      "step 2150: train loss 0.5052, val loss 0.5671\n",
      "iter 2150: loss 0.4961, time 7809.87ms, mfu 0.62%\n",
      "iter 2155: loss 0.4369, time 897.56ms, mfu 0.67%\n",
      "step 2160: train loss 0.4979, val loss 0.5621\n",
      "iter 2160: loss 0.4713, time 7554.70ms, mfu 0.62%\n",
      "iter 2165: loss 0.5051, time 865.10ms, mfu 0.68%\n",
      "step 2170: train loss 0.4997, val loss 0.5632\n",
      "iter 2170: loss 0.6769, time 7314.06ms, mfu 0.62%\n",
      "iter 2175: loss 0.4286, time 805.92ms, mfu 0.69%\n",
      "step 2180: train loss 0.4952, val loss 0.5626\n",
      "iter 2180: loss 0.5186, time 7590.70ms, mfu 0.63%\n",
      "iter 2185: loss 0.5829, time 828.58ms, mfu 0.69%\n",
      "step 2190: train loss 0.4958, val loss 0.5651\n",
      "iter 2190: loss 0.5495, time 7824.85ms, mfu 0.64%\n",
      "iter 2195: loss 0.5873, time 924.60ms, mfu 0.68%\n",
      "step 2200: train loss 0.4905, val loss 0.5603\n",
      "iter 2200: loss 0.6645, time 7762.42ms, mfu 0.63%\n",
      "iter 2205: loss 0.5130, time 875.08ms, mfu 0.68%\n",
      "step 2210: train loss 0.4863, val loss 0.5629\n",
      "iter 2210: loss 0.6132, time 8024.05ms, mfu 0.63%\n",
      "iter 2215: loss 0.4894, time 897.79ms, mfu 0.68%\n",
      "step 2220: train loss 0.4969, val loss 0.5635\n",
      "iter 2220: loss 0.4619, time 7703.94ms, mfu 0.62%\n",
      "iter 2225: loss 0.4628, time 814.80ms, mfu 0.69%\n",
      "step 2230: train loss 0.4899, val loss 0.5507\n",
      "iter 2230: loss 0.5103, time 7949.64ms, mfu 0.63%\n",
      "iter 2235: loss 0.6061, time 849.22ms, mfu 0.69%\n",
      "step 2240: train loss 0.4890, val loss 0.5623\n",
      "iter 2240: loss 0.5258, time 6899.43ms, mfu 0.63%\n",
      "iter 2245: loss 0.6685, time 879.67ms, mfu 0.69%\n",
      "step 2250: train loss 0.4836, val loss 0.5514\n",
      "iter 2250: loss 0.5215, time 6870.15ms, mfu 0.63%\n",
      "iter 2255: loss 0.4950, time 861.61ms, mfu 0.69%\n",
      "step 2260: train loss 0.4858, val loss 0.5505\n",
      "iter 2260: loss 0.5969, time 7046.69ms, mfu 0.63%\n",
      "iter 2265: loss 0.3775, time 901.47ms, mfu 0.68%\n",
      "step 2270: train loss 0.4798, val loss 0.5468\n",
      "iter 2270: loss 0.3590, time 7610.28ms, mfu 0.63%\n",
      "iter 2275: loss 0.3879, time 798.87ms, mfu 0.69%\n",
      "step 2280: train loss 0.4746, val loss 0.5530\n",
      "iter 2280: loss 0.5405, time 7186.41ms, mfu 0.64%\n",
      "iter 2285: loss 0.5347, time 906.84ms, mfu 0.69%\n",
      "step 2290: train loss 0.4801, val loss 0.5540\n",
      "iter 2290: loss 0.6349, time 7519.45ms, mfu 0.63%\n",
      "iter 2295: loss 0.3954, time 830.03ms, mfu 0.69%\n",
      "step 2300: train loss 0.4732, val loss 0.5523\n",
      "iter 2300: loss 0.3185, time 7343.61ms, mfu 0.64%\n",
      "iter 2305: loss 0.6977, time 897.62ms, mfu 0.69%\n",
      "step 2310: train loss 0.4779, val loss 0.5428\n",
      "saving checkpoint to out-abc-char\n",
      "iter 2310: loss 0.5680, time 8037.87ms, mfu 0.63%\n",
      "iter 2315: loss 0.4824, time 878.12ms, mfu 0.69%\n",
      "step 2320: train loss 0.4708, val loss 0.5399\n",
      "saving checkpoint to out-abc-char\n",
      "iter 2320: loss 0.4894, time 7503.36ms, mfu 0.63%\n",
      "iter 2325: loss 0.4104, time 783.62ms, mfu 0.70%\n",
      "step 2330: train loss 0.4672, val loss 0.5417\n",
      "iter 2330: loss 0.4191, time 7430.34ms, mfu 0.64%\n",
      "iter 2335: loss 0.4258, time 775.23ms, mfu 0.71%\n",
      "step 2340: train loss 0.4650, val loss 0.5336\n",
      "saving checkpoint to out-abc-char\n",
      "iter 2340: loss 0.5410, time 7567.73ms, mfu 0.65%\n",
      "iter 2345: loss 0.4129, time 830.81ms, mfu 0.71%\n",
      "step 2350: train loss 0.4748, val loss 0.5532\n",
      "iter 2350: loss 0.3717, time 7824.21ms, mfu 0.65%\n",
      "iter 2355: loss 0.4729, time 894.66ms, mfu 0.70%\n",
      "step 2360: train loss 0.4728, val loss 0.5303\n",
      "saving checkpoint to out-abc-char\n",
      "iter 2360: loss 0.3552, time 7664.45ms, mfu 0.64%\n",
      "iter 2365: loss 0.7091, time 914.70ms, mfu 0.69%\n",
      "step 2370: train loss 0.4642, val loss 0.5432\n",
      "iter 2370: loss 0.4451, time 7409.22ms, mfu 0.64%\n",
      "iter 2375: loss 0.3958, time 980.53ms, mfu 0.68%\n",
      "step 2380: train loss 0.4704, val loss 0.5297\n",
      "saving checkpoint to out-abc-char\n",
      "iter 2380: loss 0.5779, time 7817.63ms, mfu 0.62%\n",
      "iter 2385: loss 0.4248, time 858.55ms, mfu 0.68%\n",
      "step 2390: train loss 0.4684, val loss 0.5378\n",
      "iter 2390: loss 0.4224, time 7476.74ms, mfu 0.63%\n",
      "iter 2395: loss 0.7514, time 864.25ms, mfu 0.68%\n",
      "step 2400: train loss 0.4681, val loss 0.5401\n",
      "iter 2400: loss 0.4160, time 7500.09ms, mfu 0.63%\n",
      "iter 2405: loss 0.4067, time 824.67ms, mfu 0.69%\n",
      "step 2410: train loss 0.4497, val loss 0.5499\n",
      "iter 2410: loss 0.6151, time 7641.59ms, mfu 0.63%\n",
      "iter 2415: loss 0.5128, time 820.24ms, mfu 0.69%\n",
      "step 2420: train loss 0.4710, val loss 0.5396\n",
      "iter 2420: loss 0.5591, time 7712.03ms, mfu 0.64%\n",
      "iter 2425: loss 0.4611, time 880.27ms, mfu 0.69%\n",
      "step 2430: train loss 0.4610, val loss 0.5314\n",
      "iter 2430: loss 0.3980, time 7782.24ms, mfu 0.63%\n",
      "iter 2435: loss 0.5222, time 879.93ms, mfu 0.69%\n",
      "step 2440: train loss 0.4640, val loss 0.5292\n",
      "saving checkpoint to out-abc-char\n",
      "iter 2440: loss 0.4344, time 7904.25ms, mfu 0.63%\n",
      "iter 2445: loss 0.5257, time 846.01ms, mfu 0.69%\n",
      "step 2450: train loss 0.4567, val loss 0.5389\n",
      "iter 2450: loss 0.5151, time 7775.27ms, mfu 0.63%\n",
      "iter 2455: loss 0.4939, time 822.10ms, mfu 0.69%\n",
      "step 2460: train loss 0.4542, val loss 0.5232\n",
      "saving checkpoint to out-abc-char\n",
      "iter 2460: loss 0.3561, time 8051.49ms, mfu 0.64%\n",
      "iter 2465: loss 0.6228, time 860.70ms, mfu 0.69%\n",
      "step 2470: train loss 0.4489, val loss 0.5172\n",
      "saving checkpoint to out-abc-char\n",
      "iter 2470: loss 0.4961, time 7843.34ms, mfu 0.64%\n",
      "iter 2475: loss 0.7549, time 790.10ms, mfu 0.70%\n",
      "step 2480: train loss 0.4585, val loss 0.5323\n",
      "iter 2480: loss 0.4070, time 6983.47ms, mfu 0.65%\n",
      "iter 2485: loss 0.3211, time 789.50ms, mfu 0.71%\n",
      "step 2490: train loss 0.4411, val loss 0.5315\n",
      "iter 2490: loss 0.5564, time 6871.86ms, mfu 0.66%\n",
      "iter 2495: loss 0.4533, time 829.89ms, mfu 0.71%\n",
      "step 2500: train loss 0.4533, val loss 0.5258\n",
      "iter 2500: loss 0.5109, time 7048.44ms, mfu 0.66%\n",
      "iter 2505: loss 0.5360, time 902.40ms, mfu 0.70%\n",
      "step 2510: train loss 0.4587, val loss 0.5382\n",
      "iter 2510: loss 0.4741, time 7060.40ms, mfu 0.65%\n",
      "iter 2515: loss 0.4729, time 786.01ms, mfu 0.71%\n",
      "step 2520: train loss 0.4473, val loss 0.5219\n",
      "iter 2520: loss 0.6042, time 7411.74ms, mfu 0.66%\n",
      "iter 2525: loss 0.5665, time 821.86ms, mfu 0.71%\n",
      "step 2530: train loss 0.4484, val loss 0.5285\n",
      "iter 2530: loss 0.6334, time 7335.67ms, mfu 0.66%\n",
      "iter 2535: loss 0.5338, time 773.52ms, mfu 0.72%\n",
      "step 2540: train loss 0.4547, val loss 0.5267\n",
      "iter 2540: loss 0.7592, time 7509.79ms, mfu 0.67%\n",
      "iter 2545: loss 0.4464, time 842.92ms, mfu 0.72%\n",
      "step 2550: train loss 0.4345, val loss 0.5187\n",
      "iter 2550: loss 0.3972, time 7535.48ms, mfu 0.66%\n",
      "iter 2555: loss 0.4435, time 796.37ms, mfu 0.72%\n",
      "step 2560: train loss 0.4448, val loss 0.5290\n",
      "iter 2560: loss 0.3454, time 7046.47ms, mfu 0.67%\n",
      "iter 2565: loss 0.4851, time 812.03ms, mfu 0.73%\n",
      "step 2570: train loss 0.4418, val loss 0.5229\n",
      "iter 2570: loss 0.5773, time 7261.93ms, mfu 0.67%\n",
      "iter 2575: loss 0.4652, time 782.96ms, mfu 0.73%\n",
      "step 2580: train loss 0.4421, val loss 0.5339\n",
      "iter 2580: loss 0.3215, time 7364.77ms, mfu 0.67%\n",
      "iter 2585: loss 0.3830, time 830.58ms, mfu 0.73%\n",
      "step 2590: train loss 0.4410, val loss 0.5127\n",
      "saving checkpoint to out-abc-char\n",
      "iter 2590: loss 0.7287, time 7935.32ms, mfu 0.67%\n",
      "iter 2595: loss 0.3896, time 818.32ms, mfu 0.73%\n",
      "step 2600: train loss 0.4422, val loss 0.5220\n",
      "iter 2600: loss 0.3415, time 7432.81ms, mfu 0.67%\n",
      "iter 2605: loss 0.6229, time 872.15ms, mfu 0.72%\n",
      "step 2610: train loss 0.4359, val loss 0.5154\n",
      "iter 2610: loss 0.4348, time 7533.75ms, mfu 0.66%\n",
      "iter 2615: loss 0.5245, time 898.11ms, mfu 0.71%\n",
      "step 2620: train loss 0.4327, val loss 0.5120\n",
      "saving checkpoint to out-abc-char\n",
      "iter 2620: loss 0.4139, time 8112.68ms, mfu 0.65%\n",
      "iter 2625: loss 0.3801, time 872.96ms, mfu 0.70%\n",
      "step 2630: train loss 0.4324, val loss 0.5203\n",
      "iter 2630: loss 0.6222, time 7824.36ms, mfu 0.64%\n",
      "iter 2635: loss 0.2323, time 913.81ms, mfu 0.69%\n",
      "step 2640: train loss 0.4322, val loss 0.5050\n",
      "saving checkpoint to out-abc-char\n",
      "iter 2640: loss 0.3830, time 7948.60ms, mfu 0.64%\n",
      "iter 2645: loss 0.3633, time 798.25ms, mfu 0.70%\n",
      "step 2650: train loss 0.4277, val loss 0.5092\n",
      "iter 2650: loss 0.4771, time 7587.99ms, mfu 0.64%\n",
      "iter 2655: loss 0.3209, time 835.73ms, mfu 0.70%\n",
      "step 2660: train loss 0.4278, val loss 0.5103\n",
      "iter 2660: loss 0.5013, time 7500.14ms, mfu 0.65%\n",
      "iter 2665: loss 0.3999, time 844.48ms, mfu 0.70%\n",
      "step 2670: train loss 0.4272, val loss 0.5167\n",
      "iter 2670: loss 0.4832, time 7344.33ms, mfu 0.65%\n",
      "iter 2675: loss 0.4645, time 865.85ms, mfu 0.70%\n",
      "step 2680: train loss 0.4183, val loss 0.5105\n",
      "iter 2680: loss 0.4114, time 7382.94ms, mfu 0.64%\n",
      "iter 2685: loss 0.3654, time 818.77ms, mfu 0.70%\n",
      "step 2690: train loss 0.4251, val loss 0.5191\n",
      "iter 2690: loss 0.4639, time 7329.15ms, mfu 0.65%\n",
      "iter 2695: loss 0.4238, time 836.48ms, mfu 0.71%\n",
      "step 2700: train loss 0.4207, val loss 0.5014\n",
      "saving checkpoint to out-abc-char\n",
      "iter 2700: loss 0.3495, time 7592.40ms, mfu 0.65%\n",
      "iter 2705: loss 0.5557, time 834.01ms, mfu 0.71%\n",
      "step 2710: train loss 0.4157, val loss 0.5102\n",
      "iter 2710: loss 0.5155, time 7501.17ms, mfu 0.65%\n",
      "iter 2715: loss 0.5552, time 816.91ms, mfu 0.71%\n",
      "step 2720: train loss 0.4161, val loss 0.5077\n",
      "iter 2720: loss 0.4305, time 6524.49ms, mfu 0.65%\n",
      "iter 2725: loss 0.5562, time 755.77ms, mfu 0.72%\n",
      "step 2730: train loss 0.4173, val loss 0.5087\n",
      "iter 2730: loss 0.3632, time 6801.19ms, mfu 0.67%\n",
      "iter 2735: loss 0.4256, time 787.00ms, mfu 0.73%\n",
      "step 2740: train loss 0.4229, val loss 0.5093\n",
      "iter 2740: loss 0.4488, time 7043.07ms, mfu 0.67%\n",
      "iter 2745: loss 0.6374, time 765.79ms, mfu 0.74%\n",
      "step 2750: train loss 0.4223, val loss 0.5062\n",
      "iter 2750: loss 0.4521, time 7243.32ms, mfu 0.68%\n",
      "iter 2755: loss 0.5204, time 799.76ms, mfu 0.74%\n",
      "step 2760: train loss 0.4155, val loss 0.5151\n",
      "iter 2760: loss 0.4407, time 7534.92ms, mfu 0.68%\n",
      "iter 2765: loss 0.4193, time 978.21ms, mfu 0.71%\n",
      "step 2770: train loss 0.4080, val loss 0.5031\n",
      "iter 2770: loss 0.6943, time 7529.45ms, mfu 0.66%\n",
      "iter 2775: loss 0.4740, time 920.80ms, mfu 0.70%\n",
      "step 2780: train loss 0.4068, val loss 0.5059\n",
      "iter 2780: loss 0.6603, time 7580.42ms, mfu 0.65%\n",
      "iter 2785: loss 0.5226, time 865.55ms, mfu 0.70%\n",
      "step 2790: train loss 0.4091, val loss 0.5051\n",
      "iter 2790: loss 0.4227, time 7343.60ms, mfu 0.64%\n",
      "iter 2795: loss 0.3760, time 806.70ms, mfu 0.71%\n",
      "step 2800: train loss 0.4093, val loss 0.4933\n",
      "saving checkpoint to out-abc-char\n",
      "iter 2800: loss 0.6747, time 7267.96ms, mfu 0.65%\n",
      "iter 2805: loss 0.5049, time 812.10ms, mfu 0.71%\n",
      "step 2810: train loss 0.4090, val loss 0.5095\n",
      "iter 2810: loss 0.5599, time 7275.65ms, mfu 0.65%\n",
      "iter 2815: loss 0.6092, time 816.58ms, mfu 0.71%\n",
      "step 2820: train loss 0.4043, val loss 0.5080\n",
      "iter 2820: loss 0.2786, time 7184.26ms, mfu 0.66%\n",
      "iter 2825: loss 0.6075, time 846.10ms, mfu 0.71%\n",
      "step 2830: train loss 0.4062, val loss 0.4979\n",
      "iter 2830: loss 0.3867, time 7308.22ms, mfu 0.65%\n",
      "iter 2835: loss 0.4635, time 857.09ms, mfu 0.71%\n",
      "step 2840: train loss 0.3993, val loss 0.5020\n",
      "iter 2840: loss 0.4935, time 7235.09ms, mfu 0.65%\n",
      "iter 2845: loss 0.4698, time 840.65ms, mfu 0.71%\n",
      "step 2850: train loss 0.3981, val loss 0.4995\n",
      "iter 2850: loss 0.6616, time 7419.20ms, mfu 0.65%\n",
      "iter 2855: loss 0.3490, time 897.81ms, mfu 0.70%\n",
      "step 2860: train loss 0.4046, val loss 0.5055\n",
      "iter 2860: loss 0.4499, time 7973.85ms, mfu 0.64%\n",
      "iter 2865: loss 0.2988, time 984.96ms, mfu 0.68%\n",
      "step 2870: train loss 0.3994, val loss 0.5011\n",
      "iter 2870: loss 0.3506, time 7479.17ms, mfu 0.63%\n",
      "iter 2875: loss 0.4590, time 847.67ms, mfu 0.69%\n",
      "step 2880: train loss 0.4003, val loss 0.4933\n",
      "iter 2880: loss 0.4570, time 7066.21ms, mfu 0.63%\n",
      "iter 2885: loss 0.4328, time 857.56ms, mfu 0.69%\n",
      "step 2890: train loss 0.4006, val loss 0.4982\n",
      "iter 2890: loss 0.4354, time 7096.94ms, mfu 0.63%\n",
      "iter 2895: loss 0.5271, time 866.45ms, mfu 0.69%\n",
      "step 2900: train loss 0.3923, val loss 0.4935\n",
      "iter 2900: loss 0.4044, time 7637.33ms, mfu 0.63%\n",
      "iter 2905: loss 0.3103, time 895.45ms, mfu 0.68%\n",
      "step 2910: train loss 0.3892, val loss 0.5014\n",
      "iter 2910: loss 0.6741, time 7822.80ms, mfu 0.63%\n",
      "iter 2915: loss 0.3540, time 890.98ms, mfu 0.68%\n",
      "step 2920: train loss 0.3911, val loss 0.4971\n",
      "iter 2920: loss 0.3090, time 7645.01ms, mfu 0.63%\n",
      "iter 2925: loss 0.4286, time 953.60ms, mfu 0.67%\n",
      "step 2930: train loss 0.3932, val loss 0.4859\n",
      "saving checkpoint to out-abc-char\n",
      "iter 2930: loss 0.2648, time 7928.62ms, mfu 0.62%\n",
      "iter 2935: loss 0.3863, time 840.04ms, mfu 0.68%\n",
      "step 2940: train loss 0.3839, val loss 0.4887\n",
      "iter 2940: loss 0.4594, time 7194.22ms, mfu 0.62%\n",
      "iter 2945: loss 0.3998, time 840.75ms, mfu 0.68%\n",
      "step 2950: train loss 0.3933, val loss 0.4849\n",
      "saving checkpoint to out-abc-char\n",
      "iter 2950: loss 0.3670, time 7496.57ms, mfu 0.63%\n",
      "iter 2955: loss 0.4439, time 805.52ms, mfu 0.69%\n",
      "step 2960: train loss 0.3867, val loss 0.4892\n",
      "iter 2960: loss 0.3608, time 6676.84ms, mfu 0.64%\n",
      "iter 2965: loss 0.4115, time 802.30ms, mfu 0.70%\n",
      "step 2970: train loss 0.3864, val loss 0.4800\n",
      "saving checkpoint to out-abc-char\n",
      "iter 2970: loss 0.3868, time 7070.89ms, mfu 0.65%\n",
      "iter 2975: loss 0.3462, time 842.09ms, mfu 0.70%\n",
      "step 2980: train loss 0.3771, val loss 0.4809\n",
      "iter 2980: loss 0.4414, time 6909.70ms, mfu 0.65%\n",
      "iter 2985: loss 0.3841, time 839.70ms, mfu 0.70%\n",
      "step 2990: train loss 0.3873, val loss 0.4854\n",
      "iter 2990: loss 0.3022, time 6661.23ms, mfu 0.65%\n",
      "iter 2995: loss 0.4633, time 929.24ms, mfu 0.69%\n",
      "step 3000: train loss 0.3886, val loss 0.4915\n",
      "iter 3000: loss 0.3906, time 7530.88ms, mfu 0.64%\n",
      "iter 3005: loss 0.4411, time 959.66ms, mfu 0.68%\n",
      "step 3010: train loss 0.3814, val loss 0.4907\n",
      "iter 3010: loss 0.3026, time 7242.88ms, mfu 0.63%\n",
      "iter 3015: loss 0.4701, time 966.71ms, mfu 0.67%\n",
      "step 3020: train loss 0.3790, val loss 0.4741\n",
      "saving checkpoint to out-abc-char\n",
      "iter 3020: loss 0.3980, time 7693.82ms, mfu 0.62%\n",
      "iter 3025: loss 0.3584, time 810.81ms, mfu 0.68%\n",
      "step 3030: train loss 0.3777, val loss 0.4859\n",
      "iter 3030: loss 0.5519, time 7337.26ms, mfu 0.63%\n",
      "iter 3035: loss 0.2814, time 862.25ms, mfu 0.68%\n",
      "step 3040: train loss 0.3688, val loss 0.4826\n",
      "iter 3040: loss 0.3379, time 7072.67ms, mfu 0.63%\n",
      "iter 3045: loss 0.3622, time 826.17ms, mfu 0.69%\n",
      "step 3050: train loss 0.3724, val loss 0.4830\n",
      "iter 3050: loss 0.3719, time 7286.55ms, mfu 0.64%\n",
      "iter 3055: loss 0.3747, time 877.13ms, mfu 0.69%\n",
      "step 3060: train loss 0.3693, val loss 0.4818\n",
      "iter 3060: loss 0.3316, time 7724.74ms, mfu 0.63%\n",
      "iter 3065: loss 0.3844, time 860.41ms, mfu 0.69%\n",
      "step 3070: train loss 0.3730, val loss 0.4763\n",
      "iter 3070: loss 0.4276, time 7798.79ms, mfu 0.63%\n",
      "iter 3075: loss 0.2243, time 837.93ms, mfu 0.69%\n",
      "step 3080: train loss 0.3747, val loss 0.4798\n",
      "iter 3080: loss 0.4268, time 7272.23ms, mfu 0.64%\n",
      "iter 3085: loss 0.4341, time 814.87ms, mfu 0.70%\n",
      "step 3090: train loss 0.3741, val loss 0.4818\n",
      "iter 3090: loss 0.3366, time 7143.51ms, mfu 0.64%\n",
      "iter 3095: loss 0.3057, time 845.25ms, mfu 0.70%\n",
      "step 3100: train loss 0.3626, val loss 0.4737\n",
      "saving checkpoint to out-abc-char\n",
      "iter 3100: loss 0.4438, time 8080.35ms, mfu 0.64%\n",
      "iter 3105: loss 0.3639, time 890.47ms, mfu 0.69%\n",
      "step 3110: train loss 0.3717, val loss 0.4715\n",
      "saving checkpoint to out-abc-char\n",
      "iter 3110: loss 0.4899, time 7917.77ms, mfu 0.64%\n",
      "iter 3115: loss 0.4194, time 866.23ms, mfu 0.69%\n",
      "step 3120: train loss 0.3655, val loss 0.4773\n",
      "iter 3120: loss 0.4470, time 7423.96ms, mfu 0.64%\n",
      "iter 3125: loss 0.4262, time 840.68ms, mfu 0.69%\n",
      "step 3130: train loss 0.3655, val loss 0.4715\n",
      "iter 3130: loss 0.3509, time 7767.05ms, mfu 0.64%\n",
      "iter 3135: loss 0.3009, time 855.37ms, mfu 0.69%\n",
      "step 3140: train loss 0.3657, val loss 0.4789\n",
      "iter 3140: loss 0.5606, time 7513.95ms, mfu 0.64%\n",
      "iter 3145: loss 0.3509, time 850.55ms, mfu 0.69%\n",
      "step 3150: train loss 0.3528, val loss 0.4737\n",
      "iter 3150: loss 0.4256, time 7433.49ms, mfu 0.64%\n",
      "iter 3155: loss 0.1774, time 876.50ms, mfu 0.69%\n",
      "step 3160: train loss 0.3577, val loss 0.4742\n",
      "iter 3160: loss 0.3665, time 7446.91ms, mfu 0.64%\n",
      "iter 3165: loss 0.3218, time 874.78ms, mfu 0.69%\n",
      "^C\n",
      "Process ForkProcess-20:\n",
      "Process ForkProcess-2:\n",
      "Process ForkProcess-4:\n",
      "Process ForkProcess-7:\n",
      "Process ForkProcess-13:\n",
      "Process ForkProcess-12:\n",
      "Process ForkProcess-11:\n",
      "Process ForkProcess-17:\n",
      "Process ForkProcess-19:\n",
      "Process ForkProcess-15:\n",
      "Process ForkProcess-6:\n",
      "Process ForkProcess-18:\n",
      "Process ForkProcess-14:\n",
      "Process ForkProcess-16:\n",
      "Process ForkProcess-10:\n",
      "Process ForkProcess-1:\n",
      "Process ForkProcess-5:\n",
      "Process ForkProcess-8:\n",
      "Process ForkProcess-3:\n",
      "Process ForkProcess-9:\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 289, in <module>\n",
      "    logits, loss = model(X, Y)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/_dynamo/eval_frame.py\", line 82, in forward\n",
      "    return self.dynamo_ctx(self._orig_mod.forward)(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/_dynamo/eval_frame.py\", line 209, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/pt-env/notebooks/nanoGPT/model.py\", line 188, in forward\n",
      "    x = block(x)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/pt-env/notebooks/nanoGPT/model.py\", line 111, in forward\n",
      "    x = x + self.attn(self.ln_1(x))\n",
      "  File \"/pt-env/notebooks/nanoGPT/model.py\", line 111, in <graph break in forward>\n",
      "    x = x + self.attn(self.ln_1(x))\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/_dynamo/eval_frame.py\", line 209, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py\", line 2819, in forward\n",
      "    return compiled_fn(full_args)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py\", line 1222, in g\n",
      "    return f(*args)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py\", line 2386, in debug_compiled_function\n",
      "    return compiled_function(*args)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py\", line 1898, in runtime_wrapper\n",
      "    all_outs = call_func_with_args(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py\", line 1247, in call_func_with_args\n",
      "    out = normalize_as_list(f(args))\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py\", line 1222, in g\n",
      "    return f(*args)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/autograd/function.py\", line 506, in apply\n",
      "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py\", line 2151, in forward\n",
      "    fw_outs = call_func_with_args(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py\", line 1247, in call_func_with_args\n",
      "    out = normalize_as_list(f(args))\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/_inductor/compile_fx.py\", line 248, in run\n",
      "    return model(new_inputs)\n",
      "  File \"/tmp/torchinductor_root/5i/c5iz7g7int5bpp6yhwfwxa6l6z3eg26zbq3jxrc6st5r3b2qhx2p.py\", line 210, in call\n",
      "    triton__1.run(primals_2, buf5, 589824, grid=grid(589824), stream=stream0)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/_inductor/triton_ops/autotune.py\", line 190, in run\n",
      "    result = launcher(\n",
      "  File \"<string>\", line 6, in launcher\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/concurrent/futures/process.py\", line 233, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.8/concurrent/futures/process.py\", line 233, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 96, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 96, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/concurrent/futures/process.py\", line 233, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/concurrent/futures/process.py\", line 233, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 96, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/concurrent/futures/process.py\", line 233, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.8/concurrent/futures/process.py\", line 233, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 96, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 96, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 96, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.8/concurrent/futures/process.py\", line 233, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 96, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/concurrent/futures/process.py\", line 233, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 96, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/concurrent/futures/process.py\", line 233, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 96, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/concurrent/futures/process.py\", line 233, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 96, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/concurrent/futures/process.py\", line 233, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 96, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/concurrent/futures/process.py\", line 233, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 97, in get\n",
      "    res = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.8/concurrent/futures/process.py\", line 233, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 96, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/concurrent/futures/process.py\", line 233, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 96, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/concurrent/futures/process.py\", line 233, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 96, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/concurrent/futures/process.py\", line 233, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 96, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/concurrent/futures/process.py\", line 233, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 96, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/concurrent/futures/process.py\", line 233, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/concurrent/futures/process.py\", line 233, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 96, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 96, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/concurrent/futures/process.py\", line 233, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 96, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[31m(failed 255).\u001b[0m Press Control-C to abort syncing.\n"
     ]
    }
   ],
   "source": [
    "!python3 train.py config/train_abc_char.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test key with most occurrences: G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding: out_dir = out-abc-char\n",
      "Overriding: start = M:4/4L:1/4K:G|\"G\"|\"C\"|\"D\"|\"D\"|\"G\"|\"C\"|\"D\"|\"G\"|]\n",
      "number of parameters: 21.26M\n",
      "abc_char\n",
      "Loading meta from data/abc_char/meta.pkl...\n",
      "M:4/4L:1/4K:G|\"G\"|\"C\"|\"D\"|\"D\"|\"G\"|\"C\"|\"D\"|\"G\"|]\n",
      "B/2c/2|\"G\"ddb|\"C\"a/2g/2e/2gg/2a/2|\"D\"ff/2e/2d/2e/2f/2|\"D\"F/2A/2B/2G/2AB/2c/2|\"G\"ddb|\"C\"a/2g/2e/2g/2f/2g3/2e/2|\"D\"d/2c/2B/2A/2G/2A/2c/2B/2A/2|\"G\"GBG|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:G\n",
      "|\"G\"|\"G\"|\"Am\"|\"D\"|\"G\"|\"G\"|\"D\"|\"G\"|\"G\"|\"Am\"|\"D\"|\"G\"|\"G\"|\"G\"|\"Am\"|\"D\"|\"G\"|]\n",
      "\"G\"G3/2B/2d3/2B/2|\"G\"d3/2B/2g3/2B/2|\"Am\"e3/2A/2b3/2a/2|\"D\"g3/2e/2f3/2e/2|\"G\"d3/2c/2B3/2d/2|\"D\"A/2G/2F/2A/2d3/2B/2|\"G\"d3/2B/2g3/2B/2|\"G\"d3/2B/2g3/2B/2|\"Am\"e3/2A/2b3/2a/2|\"D\"d3/2c/2a3/2g/2|\"G\"g3/2d/2B3/2g/2|\"G\"d3/2B/2g3/2B/2|\"Am\"A3/2G/2F3/2A/2|\"D\"f3/2e/2f3/2g/\n",
      "---------------\n",
      "M:4/4L:1/4K:G|\"G\"|\"C\"|\"D\"|\"D\"|\"G\"|\"C\"|\"D\"|\"G\"|]\n",
      "D|\"G\"G/2B/2B/2d/2|\"C\"e/2d/2c/2d/2|\"D\"DF|\"G\"G/2B/2B/2d/2|\"C\"ece/2d/2|\"D\"f/2e/2d/2c/2|\"G\"B3|B/2c/2|\"G\"d/2d/2d/2d/2B/2|\"C\"ecA|\"D\"A/2B/2c/2A/2|\"G\"B/2A/2G/2F/2G/2|\"C\"EGG/2B/2|\"D\"A/2G/2F/2E/2|\"G\"DGG/2A/2|\"C\"B/2A/2G/2F/2E/2|\"D\"DFB|\"G\"G/2B/2B/2c/2|\"C\"dc/2d/2|\"D\"c/2B/2A/2G/2F/2|\"G\"G3|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:D\n",
      "|\"D\"|\"D\"|\"D\"|\"Em\"\"A7\"|\"D\"|\"D\"|\"D\"|\"D\"|\"G\"|\"Em\"\"A7\"|\"D\"|\"D\"|\"D\"|\"Em\"\"A7\"|\"D\"|]\n",
      "e|\"D\"fdAF|\"D\"fdAF|\"D\"fdAF|\"Em\"eG\"A7\"AE|\"D\"fdAF|\"D\"fdAF|\"D\"fdAF|\"D\"fdAF|\"G\"BdGB|\"Em\"eG\"A7\"AG|\"D\"FAd/2d/2d/2d/2d/2|\"D\"fdAF|\"D\"fd\n",
      "---------------\n",
      "M:4/4L:1/4K:G|\"G\"|\"C\"|\"D\"|\"D\"|\"G\"|\"C\"|\"D\"|\"G\"|]\n",
      "B/2c/2|\"G\"d3/2B/2d3/2d/2|\"C\"de3/2d/2|\"D\"c3/2d/2e/2f/2d3/2c/2|\"D\"d3/2c/2A3/2d/2|\"G\"d3/2B/2d3/2d/2|\"C\"e/2d/2e3/2f/2e3/2d/2|\"D\"c3/2A/2G3/2F/2|\"G\"G3/2B/2d3/2d/2|\"C\"e/2d/2c/2B/2\"D\"Ad|\"G\"G3|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:F\n",
      "|\"F\"\"C7\"|\"F\"\"C7\"|\"F\"\"C7\"|\"F\"\"C7\"|\"F\"|\"Dm\"|\"Gm\"|\"C\"|\"C\"|\"F\"|\"Gm\"|\"C\"|\"F\"\"B\"|\"F\"\"B\"|\"B\"|\"F\"\"B\"|\"B\"|\"F\"\"B\"|\"B\"|\"B\"|\"F\"\"B\"|\"F\"\"B\"|\"B\"|\"F\"\"B\"|\"B\"|\"B\"|\"F\"\"B\"|\"B\"\"C7\"|\"F\"|\"B\"|\"F\"\"B\"|\"B\"|\"F\"\"B\"|\"B\"|]\n",
      "C|\"F\"F\"C7\"D^E|\"F\"F3/2F/2\"C7\"CD|\"F\"CF\"C7\"D^E|\"F\"F3/2F/2\"C7\"CD|\"F\"CFAB|\"Dm\"A3/2A/2FG|\"Gm\"F/2E/2D/2C/2B,C|\"\n",
      "---------------\n",
      "M:4/4L:1/4K:G|\"G\"|\"C\"|\"D\"|\"D\"|\"G\"|\"C\"|\"D\"|\"G\"|]\n",
      "\"G\"ge|\"G\"dBB|\"C\"cee|\"D\"d3|\"G\"dBB|\"C\"cee|\"D\"d3|\"G\"dBB|\"C\"cee|\"D\"d3|\"G\"dBB|\"C\"cee|\"D\"d3|\"G\"dBB|\"C\"cee|\"D\"d3/2c/2BA|\"G\"Gdd|\"C\"g2|]\n",
      "\n",
      "M:3/2\n",
      "L:1/8\n",
      "K:Gb\n",
      "|\"G\"|\"G\"|\"Am\"|\"D\"|\"G\"|\"Em\"\"D\"|\"G\"|\"D\"|\"Em\"|\"D\"|\"G\"|\"G\"|\"D\"|\"Em\"|\"D\"|\"Em\"|\"D\"|\"G\"|\"G\"|\"D\"|\"G\"|\"G\"|\"G\"|\"D\"|\"Em\"|\"D\"|\"G\"|\"Em\"|\"D\"|\"G\"|\"G\"|]\n",
      "d/2e/2|\"G\"d3/2d/2Bc|\"G\"ddge|\"Am\"c3/2c/2AB|\"D\"cdef|\"G\"dBGB|\"Em\"\"Bm\"d3/2d/2Bc|\"D\"d3/2d/2cB|\"Em\"A3/2A/2Bc|\"D\"defe|\"Em\"d3|A3/2A/2|\"D\"A2de|\"Em\"B2ed|\"D\"A2de|\"G\"d3/2d/2Bc|\"D\"d2ef|\"Em\"e2ed|\"D\"A2de|\"G\"d2d|\"D\"A2AB|\"Em\"d3/2d/2F\n",
      "---------------\n",
      "M:4/4L:1/4K:G|\"G\"|\"C\"|\"D\"|\"D\"|\"G\"|\"C\"|\"D\"|\"G\"|]\n",
      "d|\"G\"B/2d/2G/2B/2d/2B/2g/2|\"C\"e/2d/2e/2d/2c/2d/2e/2|\"D\"f/2d/2A/2c/2d/2e/2f/2|\"G\"g/2d/2B/2G/2d/2B/2g/2|\"C\"e/2d/2e/2d/2c/2d/2e/2|\"D\"f/2d/2A/2c/2d/2e/2f/2|\"G\"g/2e/2d/2e/2f/2g/2e/2|\"C\"g/2e/2d/2c/2d/2e/2f/2|\"D\"g/2e/2d/2c/2d3/2e/2|\"G\"f/2g/2e/2d/2c/2d/2e/2f/2|\"C\"g/2e/2d/2c/2d/2e/2f/2|\"D\"g/2e/2d/2c/2B/2A/2c/2d/2|\"G\"e/2d/2c/2B/2Ad/2e/2|\"D\"f/2d/2A/2c/2d/2e/2f/2|\"G\"g/2e/2d/2c/2d/2e/2f/2g/2|\"C\"e/2g/2d/2e/2f/2g/2e/2|\"D\"d/2c/2B/2A/2c/2d/2e/2f/2|\"G\"g/2e/2d/2c/2B/2d/2c/2d/2|\"C\"e/2d/2c/2B/2A/2c/2d/2e/2|\"D7\"f/2d\n",
      "---------------\n",
      "M:4/4L:1/4K:G|\"G\"|\"C\"|\"D\"|\"D\"|\"G\"|\"C\"|\"D\"|\"G\"|]\n",
      "g/2f/2|\"G\"e/2d/2B/2GB/2d/2|\"C\"_cc/2e/2d/2e/2_c/2|\"D\"d/2g/2f/2e/2d/2B/2d/2|\"D\"F/2A/2D/2F/2A/2d/2|\"G\"B/2G/2B/2d/2_B/2G/2A/2|\"C\"_cc/2e/2d/2e/2_c/2|\"D\"d/2B/2D/2F/2A/2d/2|\"G\"B/2G/2B/2d/2_B/2G/2B/2d/2|\"C\"_fe/2f/2d/2e/2_c/2|\"D\"F/2A/2D/2F/2A/2d/2|\"G\"B/2G/2B/2d/2=B/2G/2B/2d/2|\"C\"_f/2e/2f/2d/2e/2_c/2|\"D\"d/2f/2e/2f/2g/2a/2_f/2d/2|\"G\"B/2G/2B/2d/2_B/2G/2B/2d/2|\"C\"_f/2e/2f/2d/2e/2_c/2|\"D\"F/2A/2D/2F/2A/2d/2|\"G\"B/2G/2B/2d/2g/2=B/2G/2B/2d/2|\"C\"_f/2e/2f/2d/2e/2_c/2e/2|\"D\"f/2^e/2f/2a/2f/2g/2a/2=f/2d/2|\"G\"B/2G/2B/\n",
      "---------------\n",
      "M:4/4L:1/4K:G|\"G\"|\"C\"|\"D\"|\"D\"|\"G\"|\"C\"|\"D\"|\"G\"|]\n",
      "D|\"G\"DG/2A/2B/2A/2|B/2A/2G/2B/2A/2|\"C\"A/2G/2F/2E/2|\"D\"F/2D/2E/2F/2A/2|\"G\"B/2A/2B/2c/2B/2A/2|\"C\"A/2G/2F/2E/2|\"D\"DG/2A/2B/2A/2|A/2G/2F/2E/2D/2A/2|\"G\"B/2A/2B/2c/2B/2A/2|\"C\"B/2A/2G/2F/2E/2|\"D\"DG/2A/2B/2A/2|\"G\"B/2A/2G/2F/2E/2D/2|\"C\"G/2E/2c/2e/2d/2c/2B/2|\"D\"A/2G/2F/2D/2E/2D/2G/2A/2|\"G\"B/2A/2G/2F/2E/2D/2|\"C\"G/2E/2C/2E/2\"D\"D/2E/2F/2A/2|\"G\"B/2A/2G/2F/2E/2D/2|\"C\"G/2E/2C/2E/2c/2e/2d/2|\"D\"c/2A/2B/2c/2A/2B/2A/2|\"G\"GB/2G/2d/2g/2f/2|\"C\"e/2f/2g/2e/2\"G\"d/2B/2G|\"C\"e/2f/2g/2e/2\"G\"d/2B/2G|\"C\"c/2A/2G/2F/2E/2C/2A/2|\n",
      "---------------\n",
      "M:4/4L:1/4K:G|\"G\"|\"C\"|\"D\"|\"D\"|\"G\"|\"C\"|\"D\"|\"G\"|]\n",
      "d|\"G\"g/2g/2g/2B/2|g/2g/2e/2d/2|\"C\"e/2A/2A/2A/2|\"D\"F/2A/2F/2G/2A/2|\"G\"G/4G/2G/2G/2|\"C\"e/2e/2e/2d/2|\"D\"B/2c/2d/2c/2|\"G\"G/2G/2G|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:G\n",
      "|\"G\"|\"C\"|\"G\"|\"Am\"\"D7\"|\"G\"|\"C\"|\"G\"\"D7\"|\"G\"|\"G\"|\"Em\"|\"Am\"\"D7\"|\"G\"|\"Em\"|\"Am\"\"D7\"|\"G\"|\"C\"|\"G\"\"D7\"|\"G\"|]\n",
      "D|\"G\"GG/2A/2BG/2A/2|\"C\"G/2E/2C/2D/2E/2G/2A/2|\"D\"DDD/2E/2F/2A/2|\"G\"G/2D/2G/2B/2d/2G/2A/2|\"Am\"B/2A/2G/2F/2\"D7\"E/2D/2C/2D/2|\"G\"GG/2A/2BG/2A/2|\"C\"G/2E/2C/2D/2E/2G/2A/2A/2|\"G\"G/2D/2G/2B/2\"D7\"A/2D/2C/2A/2|\"G\"G2G|G/2F/2|\"Em\"EE/2F/2G/2A/2|\"Am\"B/2A/2G/2F/2\"D7\"E/2D\n",
      "---------------\n",
      "M:4/4L:1/4K:G|\"G\"|\"C\"|\"D\"|\"D\"|\"G\"|\"C\"|\"D\"|\"G\"|]\n",
      "d|\"G\"dd2d|dd2d|\"C\"ee2e|ee2e|\"D\"dd2d|d2d2|\"D\"d2d2|\"G\"dd2d|d2d2|dd2e|\"C\"g2f|e2e2|\"D\"d2d2|\"G\"d2d2|d2d2|\"G\"d2d2|d2e2|\"C\"e2e2|\"A\"e2e2|\"D\"d2d2|\"G\"d2d2|\"G\"d2d2|\"G\"d2B2|\"G\"B2B2|\"D\"A2B2|\"E7\"A2B2|\"Am\"c2c2|\"D\"B2A2|\"G\"B2F2|\"E7\"A2B2|\"Am\"c2A2|\"D\"A2B2|\"G\"B2B2|B2B2|\"D\"B2B2|\"Am\"ce2|\"D\"d2d2|\"G\"d2B2|\"G\"B2B2|\"D\"A2B2|\"D\"A2B2|\"E7\"cB2|\"Am\"c2c2|\"D\"B4-|\"G\"B2|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:G\n",
      "|\"G\"|\"G\"|\"C\"|\"G\"|\"D7\"|\"G\"|\"C\"|\"G\"|\"D7\"|\"G\"|\"G\"|\"C\"|\"G\"|\"D7\"|\"G\"|\"G7\"|\"C\"|\"G\"|\"D7\"|\"G\"\"D7\"|\"G\"|]\n",
      "B/2c/2|\"G\"d2d2|\"G\"ed3|\"C\"e2e2|\"G\"d2d2|\"D7\"c2c2|\"\n",
      "---------------\n",
      "M:4/4L:1/4K:G|\"G\"|\"C\"|\"D\"|\"D\"|\"G\"|\"C\"|\"D\"|\"G\"|]\n",
      "B,/2C/2|\"G\"D/2G/2G/2G/2B/2A/2G/2|\"C\"c/2B/2A/2G/2E/2D/2E/2|\"D\"F/2D/2DB,/2C/2|\"G\"D/2G/2G/2G/2GB/2A/2|\"G\"d/2d/2d/2c/2B/2A/2G/2|\"C\"c/2G/2E/2D/2C/2E/2G/2A/2|\"D\"F/2D/2DB,/2C/2|\"G\"D/2GG/2GG/2A/2|\"G\"B/2A/2B/2c/2d/2e/2f/2|\"C\"g/2e/2f/2d/2e/2d/2c/2d/2|\"D\"d/2B/2A/2F/2DB,/2C/2|\"G\"DD/2G/2GG/2A/2|\"G\"B/2A/2B/2c/2d/2e/2f/2d/2|\"C\"g/2e/2f/2d/2e/2d/2c/2B/2|\"D\"A/2G/2F/2E/2Dc|\"G\"B/2A/2B/2c/2de/2f/2|\"C\"g/2e/2f/2d/2e/2d/2c/2B/2|\"D\"A/2G/2F/2E/2DB,/2C/2|\"G\"DD/2G/2GG/2A/2|\"G\"B/2A/2B/2c/2dg/2f/2|\"C\"g/2e/2f/2d/2e/2d/2c/2d/\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "!python3 sample.py --out_dir=out-abc-char --start='M:4/4L:1/4K:G|\"G\"|\"C\"|\"D\"|\"D\"|\"G\"|\"C\"|\"D\"|\"G\"|]'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test major key with low samples: C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding: out_dir = out-abc-char\n",
      "Overriding: start = M:4/4L:1/4K:C|\"C\"|\"F\"|\"G\"|\"G\"|\"C\"|\"F\"|\"G\"|\"C\"|]\n",
      "number of parameters: 21.26M\n",
      "abc_char\n",
      "Loading meta from data/abc_char/meta.pkl...\n",
      "M:4/4L:1/4K:C|\"C\"|\"F\"|\"G\"|\"G\"|\"C\"|\"F\"|\"G\"|\"C\"|]\n",
      "A/2B/2|\"C\"c/2ed/2c/2|B/2A/2G/2EA/2B/2|\"F\"c/2A/2F/2A/2B/2|c/2A/2F/2A/2B/2c/2|\"G\"d/2cB/2A/2|\"G\"G/2ed/2c/2|B/2A/2G/2A/2G/2E/2G/2|\"C\"c/2ed/2c/2|B/2A/2G/2EA/2B/2|\"F\"c/2A/2F/2A/2B/2c/2|\"G\"d/2cB/2A/2|G/2e/2d/2c/2B/2A/2G/2|\"C\"c/2ed/2c/2|B/2A/2G/2EA/2B/2|\"F\"c/2A/2F/2A/2c/2F/2A/2|B/2A/2G/2Fe/2d/2|\"G\"g/2fe/2d/2|\"C\"ced/2c/2|\"G\"Bd/2c/2B/2A/2G/2e/2|\"C\"ced/2c/2|B/2A/2G/2EA/2B/2|\"F\"c/2A/2F/2A/2c/2F/2A/2|B/2A/2G/2F/2A/2B/2c/2|d/2f/2e/2d/2c/2B/2A/2G/2|\"G\"B/2G/2e/2d/2c/2B/2A/2G/2|\"C\"cede/2d/2|\"F\"ced2|\"G\"Bd/2c/2B/\n",
      "---------------\n",
      "M:4/4L:1/4K:C|\"C\"|\"F\"|\"G\"|\"G\"|\"C\"|\"F\"|\"G\"|\"C\"|]\n",
      "G|\"C\"cc/2d/2|c3B|A3/2G/2EC|\"F\"FA\"C\"G2|\"F\"F3/2G/2AF|A3/2G/2FC|\"G\"=EG3/2G/2|\"G\"cB3|\"C\"E3/2C/2DC|E3C|\"F\"FA\"C\"G2|\"F\"A3/2G/2FC|\"G\"=EGA|\"C\"BGE|\"F\"F3/2G/2AF|A3/2G/2FC|\"G\"=EGA|\"G\"=BGD|GAB|\"C\"c3/2B/2cE|\"F\"fff|\"G\"edd/2e/2d/2c/2|\"C\"Bce|\"F\"f3/2e/2dc|\"G\"=BGBd|\"C\"c3|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:F\n",
      "|\"F\"|\"F\"|\"Gm\"\"C7\"|\"F\"|\"F\"|\"B\"|\"F\"|\"Gm\"\"C7\"|\"F\"|\"B\"|\"F\"|\"B\"|\"F\"|]\n",
      "\"F\"FA/2G/2F/2E/2C|FA/2G/2F/2E/2|\"F\"FA/2G/2F/2E/2|\"Gm\"D3/2E/2\"C7\"C2|\"F\"FA/2G/2F/2E/2C|FA/2G/2F/2E/2C|\"F\"FA/2G/2F/2E/2|\"B\"D3/2E/2F/2D/2|\"F\"CFA/2G/2F/2E/2|\"Gm\"D3/2E/\n",
      "---------------\n",
      "M:4/4L:1/4K:C|\"C\"|\"F\"|\"G\"|\"G\"|\"C\"|\"F\"|\"G\"|\"C\"|]\n",
      "c|\"C\"ccc/2d/2c/2A/2G/2|\"F\"FF/2G/2F/2E/2F/2|\"G\"GG/2A/2Gc/2d/2|\"G\"e/2d/2c/2B/2A/2G/2F/2|\"G\"GG/2A/2G|\"C\"cc/2d/2c/2B/2A/2G/2|\"F\"FF/2G/2F/2E/2F/2|\"G\"GG/2A/2Gc/2d/2|\"C\"e/2d/2e/2f/2g/2e/2f/2|\"F\"d/2c/2A/2FG/2A/2|\"G\"Gd/2e/2d/2c/2d/2e/2|\"C\"fec|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:C\n",
      "|\"C\"|\"F\"|\"C\"|\"G7\"|\"C\"\"G7\"|\"C\"|\"F\"|\"C\"|\"G\"|\"F\"|\"G\"|\"F\"|\"G7\"|\"C\"|]\n",
      "G|\"C\"cc/2d/2cG|\"F\"F/2G/2A/2B/2c/2A/2F|\"C\"C/2E/2G/2c/2eA|\"G7\"G/2A/2B/2c/2d/2e/2f/2d/2|\"C\"e/2c/2d/2e/2\"G7\"cG|\"C\"cc/2d/2cG|\"F\"F/2G/2A/2B/2c/2A/2F|\"C\"C/2E/2G/2c/2eA|\"G\"G/2A/2B/2c/2B/2G/\n",
      "---------------\n",
      "M:4/4L:1/4K:C|\"C\"|\"F\"|\"G\"|\"G\"|\"C\"|\"F\"|\"G\"|\"C\"|]\n",
      "\"C\"cee/2d/2c/2A/2G/2E/2|\"F\"A/2_E/2F/2A/2B/2c/2d/2e/2|f/2e/2d/2e/2d/2c/2B/2A/2|\"G\"Gee/2d/2c/2A/2G/2E/2|\"G\"Gecd/2c/2|\"G\"B/2G/2A/2B/2G/2A/2G/2=E/2|\"C\"Cece/2d/2c/2A/2|\"F\"A/2=E/2F/2A/2c/2f/2e/2d/2|\"G\"c/2G/2A/2B/2G/2_E/2|\"C\"Cece/2g/2|\"F\"f/2e/2d/2e/2d/2c/2B/2A/2|\"G\"G/2E/2A/2G/2=E/2G/2G/2E/2|\"C\"ccec/2f/2|\"F\"c/2A/2d/2e/2d/2c/2B/2A/2|\"G\"G/2E/2A/2G/2=E/2G/2E/2|\"C\"c/2G/2A/2B/2G/2_E/2G/2E/2|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:F\n",
      "|\"F\"|\"F\"|\"C\"|\"F\"|\"F\"|\"F\"|\"C\"|\"F\"|\"F\"|\"G\"|\"C\"|\"F\"|\"F\"|\"G\"|\"C\"|\"F\"|\"F\"|\"G\"|\"C\"|\"F\"|]\n",
      "\"F\"c3|\"F\"f3|\"C\"e\n",
      "---------------\n",
      "M:4/4L:1/4K:C|\"C\"|\"F\"|\"G\"|\"G\"|\"C\"|\"F\"|\"G\"|\"C\"|]\n",
      "e/2f/2|\"C\"g/2e/2c/2e/2|g/2e/2c/2e/2|g/2e/2c/2e/2|\"F\"f/2c/2A/2B/2|c/2A/2F/2A/2|\"G\"B/2A/2G/2B/2|\"G\"G/2E/2G/2B/2|\"C\"ce/2d/2|c/2G/2E/2c/2|\"C\"e3/2f/2|g/2e/2c/2e/2|\"F\"f/2e/2d/2c/2|A3/2e/2|\"G\"g/2f/2e/2d/2|\"C\"c3|]\n",
      "\n",
      "M:2/4\n",
      "L:1/4\n",
      "K:G\n",
      "|\"G\"|\"G\"|\"C\"|\"G\"|\"Am\"|\"D\"|\"G\"|\"E#m\"|\"Am\"\"D\"|\"G\"|\"E#m\"|\"Am\"|\"G\"|\"E#m\"|]\n",
      "\"G\"d/2B/2G/2|B/2G/2D3/4G/4|\"G\"d/2B/2G/2B/2|G/2D3/4G/4|\"C\"e/2d/2G/2|\"G\"B/2G/2D3/4G/4|\"Am\"e/2d/2G/2|\"D\"A/2G/2F/2A/2|\"G\"G3/2|e/4d/4|\"E#m\"e/2B/2d/2|e/2d/2|\"Am\"c3/2e/2|\"G\"d/2B/2D3/4G/4|A/2F/2D3/4G/4|\"E#m\"e/2d/2\n",
      "---------------\n",
      "M:4/4L:1/4K:C|\"C\"|\"F\"|\"G\"|\"G\"|\"C\"|\"F\"|\"G\"|\"C\"|]\n",
      "g/2|\"C\"a/2g/2e/2g/2|a/2g/2e/2g/2|\"F\"f/2e/2d/2e/2|f/2e/2d/2c/2|\"G\"B/2e/2d/2c/2|B/2G/2A/2G/2|\"C\"c/2c/2c/2c/2|\"C\"c/2c/2c/2c/2|\"F\"f/2f/2f/2a/2|f/2e/2d/2c/2|\"G\"B/2G/2G/2B/2|\"C\"c/2c/2c/2e/2|\"F\"f/2f/2f/2a/2|f/2e/2d/2c/2|\"G\"B/2G/2G/2A/2|B/2G/2A/2B/2G/2|\"C\"c/2c/2c/2c/2|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:G\n",
      "|\"G\"|\"C\"|\"G\"\"D\"|\"G\"|\"C\"|\"G\"|\"G\"|\"C\"|\"G\"\"D\"|\"G\"|\"C\"|\"G\"|\"G\"|\"A\"|\"D\"|\"G\"|\"C\"|\"G\"|\"C\"|\"G\"|\"G\"|]\n",
      "d|\"G\"d/2B/2G/2B/2|\"C\"e/2d/2B/2d/2|\"G\"d/2B/2G/2B/2|\"G\"d/2G/2B/2D/2G/2|\"C\"e/2d/2B/2d/2|\"G\"d/2G/2B/2D/2G/2|\"G\"d/2B/2G/2B/2|\"C\"e/2\n",
      "---------------\n",
      "M:4/4L:1/4K:C|\"C\"|\"F\"|\"G\"|\"G\"|\"C\"|\"F\"|\"G\"|\"C\"|]\n",
      "c/2d/2|\"C\"e3/4d/4cA/2c/2|e/2c/2G/2cc/2A/2|\"F\"F/2A/2A/2G/2F/2E/2|\"G\"D3/2C/2E/2D/2C/2|\"G\"B,/2D/2G/2D/2G/2B/2A/2|\"C\"G/2E/2G/2C/2A,/2C/2A,/2|\"F\"_EE|\"G\"D/2D/2D/2D/2F/2D/2B,/2A,/2|D/2D/2D/2D/2D/2F/2D/2|\"C\"_EE|G/2E/2E/2E/2E/2E|\"F\"F/2A/2A/2A/2A/2A/2A/2|c/2A/2A/2A/2A/2A/2A/2A/2A/2|\"C\"_B/2G/2_E/2E/2E/2G/2E/2|G/2E/2E/2E/2E/2E|\"F\"F/2A/2A/2A/2A/2A/2A/2A/2|c/2A/2A/2A/2A/2A/2A/2A/2A/2|\"C\"_B/2G/2A/2G/2C/2A,/2G,/2|\"F\"A,/2_D/2D/2D/2DD/2F/2|d/2A/2A/2A/2A/2A/2A/2A/2|\"G\"G/2=F/2G/2B/2B/2B/2B/2B/2G/2|\"C\"c/2=B/2c/2e/2\n",
      "---------------\n",
      "M:4/4L:1/4K:C|\"C\"|\"F\"|\"G\"|\"G\"|\"C\"|\"F\"|\"G\"|\"C\"|]\n",
      "|\"C\"CE/2C/2|\"F\"F/2A/2A/2A/2|F/2A/2A/2A/2|\"G\"DD/2^D/2|\"G\"D/2G/2B/2d/2|\"C\"c/2B/2A/2G/2|\"F\"F3/2|G/2|\"G\"G/2B/2B/2A/2|G/2B/2B/2A/2|\"C\"G/2E/2C/2E/2|\"F\"F/2A/2A/2A/2A/2|F/2A/2A/2A/2|\"G\"DD/2=D/2|\"C\"C/2E/2E/2C/2E/2|E/2E/2E/2C/2E/2|\"F\"F/2A/2A/2A/2|\"G\"DD/2^D/2^D/2|\"C\"C/2E/2E/2E/2C/2|E/2E/2C/2E/2|\"F\"F/2A/2A/2A/2|F/2A/2A/2A/2A/2|\"G\"G/2B/2B/2A/2|G/2B/2B/2A/2|\"C\"G/2E/2C/2E/2|\"F\"F3/2|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:F#\n",
      "|\"F\"|\"F\"|\"F\"|\"F\"\"C\"|\"F\"|\"F\"|\"C\"|\"F\"|\"F\"|\"F\"|\"F\"|\"F\"\"C\"|\"F\"|\"F\"|\"C\"|\"F\"|\"F\"|\"C\"|\"F\"|]\n",
      "F/2G/2|\"F\"AA/2G/2FC|\"F\"A\n",
      "---------------\n",
      "M:4/4L:1/4K:C|\"C\"|\"F\"|\"G\"|\"G\"|\"C\"|\"F\"|\"G\"|\"C\"|]\n",
      "\"C\"CE/2E/2C/2E/2D/2C/2|E/2C/2C/2C/2E/2G/2E/2G/2|\"F\"A/2c/2A/2F/2A/2c/2A/2|\"G\"G/2^F/2G/2B/2G/2F/2G/2A/2|B/2G/2B/2G/2F/2G/2A/2G/2|\"C\"E/2C/2C/2C/2E/2G/2E/2G/2|\"F\"A/2c/2A/2F/2A/2c/2A/2|\"G\"G/2^F/2G/2B/2G/2F/2G/2A/2|B/2G/2F/2G/2F/2G/2A/2G/2|\"C\"E/2C/2C/2C/2C|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:C\n",
      "|\"C\"|\"F\"\"G7\"|\"C\"|\"G7\"\"C\"|\"G7\"|\"C\"\"G7\"|\"C\"|\"C\"|\"G7\"|\"G\"|\"G7\"\"C\"|]\n",
      "c/2B/2|\"C\"cc/2c/2cc/2c/2|\"F\"ff/2f/2\"G7\"d/2d/2B/2d/2|\"C\"c/2B/2c/2d/2c/2B/2c/2|\"G7\"d/2G/2G/2B/2\"C\"c/2B/2c/2e/2|\"G7\"d/2c/2d/2B/2\"C\"c/2d/2e/2f/2|\"G7\"gg/2g/2f/2g/2f/2g/2\n",
      "---------------\n",
      "M:4/4L:1/4K:C|\"C\"|\"F\"|\"G\"|\"G\"|\"C\"|\"F\"|\"G\"|\"C\"|]\n",
      "A/2B/2|\"C\"c/2G/2c/2e/2|c/2G/2c/2|\"F\"f/2fe/2|dc|\"G\"B/2GA/2|\"G\"B/2GA/2|\"C\"G/2E/2C/2E/2|\"F\"f/2fe/2|d/2c/2B/2AB/2c/2|\"G\"e/2d/2c/2B/2A/2G/2|\"C\"ccc|\"F\"f2fe|\"G\"d/2c/2B/2A/2G/2A/2|\"C\"cec|\"F\"f2fe|\"G\"d/2c/2B/2A/2GG/2A/2|\"C\"cec|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:C\n",
      "|\"C\"|\"C\"|\"F\"|\"G\"\"C\"|\"G\"|\"C\"|\"F\"|\"F\"|\"G\"|\"C\"|\"F\"|\"F\"\"C\"|\"G\"|\"C\"|\"F\"\"C\"|\"F\"\"C\"|\"F\"\"C\"|\"F\"\"C\"|\"G\"|\"C\"|]\n",
      "G|\"C\"G/2A/2G/2E/2CE|\"C\"G/2A/2G/2E/2CE|\"F\"F/2G/2A/2B/2cd|\"G\"G/2A/2G/2E/2\"C\"C2|\"F\"A/2G/2A/2B/2c/2A/2G/2F/2|\"G\"A/2B/2c/2d/2B/2AG/2F/2|\"C\"E/2C/2E/2C/2E/2C/2E/2C/2|\"F\"\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "!python3 sample.py --out_dir=out-abc-char --start='M:4/4L:1/4K:C|\"C\"|\"F\"|\"G\"|\"G\"|\"C\"|\"F\"|\"G\"|\"C\"|]'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test minor key with low samples: Am"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding: out_dir = out-abc-char\n",
      "Overriding: start = M:4/4L:1/4K:Am|\"Am\"|\"Dm\"|\"E\"|\"E\"|\"Am\"|\"Dm\"|\"E\"|\"Am\"|]\n",
      "number of parameters: 21.26M\n",
      "abc_char\n",
      "Loading meta from data/abc_char/meta.pkl...\n",
      "M:4/4L:1/4K:Am|\"Am\"|\"Dm\"|\"E\"|\"E\"|\"Am\"|\"Dm\"|\"E\"|\"Am\"|]\n",
      "A/2|\"Am\"aae/2g/2|\"Dm\"a/2g/2f/2e/2f/2g/2|\"E\"e/2c/2B/2A/2B/2c/2|\"E\"B/2c/2B/2A/2G/2E/2B/2|\"Am\"c/2A/2E/2F/2G/2A/2E/2|\"Dm\"F/2G/2A/2G/2F/2G/2A/2|\"E\"B/2c/2B/2A/2\"E7\"G/2A/2B/2G/2|\"Am\"A2A|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:Am\n",
      "|\"Am\"|\"Am\"|\"G\"|\"G\"|\"Am\"|\"Am\"\"Em\"|\"Am\"|\"Am\"\"Am\"|\"G\"|\"G\"|\"Am\"|\"Am\"\"G\"|\"Am\"|]\n",
      "A/2d/2|\"Am\"e/2d/2e/2f/2e/2f/2g/2|\"Am\"a/2e/2g/2f/2e/2d/2c/2B/2|\"G\"d/2c/2d/2B/2GA/2B/2|\"G\"d/2e/2d/2e/2f/2g/2|\"Am\"a/2e/2g/2f/2e/2d/2c/2B/2|\"Am\"\"Am\"cAA|c/2d/2|\"Am\"e/2a/2b/2c'/2b/2a/2g/2f/2|\"Am\"e/2a/2b/2c'/2\"Em\"b/2a/2g/2b/2|\"Am\"a2\n",
      "---------------\n",
      "M:4/4L:1/4K:Am|\"Am\"|\"Dm\"|\"E\"|\"E\"|\"Am\"|\"Dm\"|\"E\"|\"Am\"|]\n",
      "E|\"Am\"A3/2B/2c/2B/2A/2|\"Dm\"EDE|\"E\"B3/2A/2G/2F/2|\"E\"EDE|\"E\"B3/2A/2G/2F/2|\"Am\"EDE|\"Dm\"F3/2E/2DC|\"E\"B,ED|\"E\"B3/2A/2GF|\"Am\"EDE|\"Am\"A2AB/2c/2|\"Dm\"d3|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:F#\n",
      "|\"F\"|\"F\"\"C7\"|\"F\"|\"F\"\"C7\"|\"F\"\"B\"|\"F\"\"C7\"|\"F\"|\"F\"|\"F\"\"C7\"|\"F\"|\"F\"\"C7\"|\"F\"|\"F\"\"C7\"|\"F\"\"C7\"|\"F\"|]\n",
      "c|\"F\"fc/2A/2FG/2|\"F\"Ac\"C7\"C/2C/2A,/2B,/2|\"F\"F,A,C|\"F\"FG/2F/2\"C7\"E/2G/2A/2B/2|\"F\"cc-c/2c/2|\"F\"fc/2A/2FG/2A/2|\"F\"\"C7\"GE/2C/2A,/2B,/2|\"F\"FA,C|\"F\"FG/2F/2\"C7\"E/2G/2A/2B/2|\"F\"ccB/2A/2G/2F/2|\"F\"CF\"C7\"GE|\"F\"F3|A/2G/2|\"F\"FF\"C7\"GE|\"F\"F3|A/2G/2|\"F\"F/2G\n",
      "---------------\n",
      "M:4/4L:1/4K:Am|\"Am\"|\"Dm\"|\"E\"|\"E\"|\"Am\"|\"Dm\"|\"E\"|\"Am\"|]\n",
      "e|\"Am\"ec/2d/2e/2f/2|e/2c/2A/2B/2|\"Dm\"A3/2e/2|\"Am\"c/2A/2B/2c/2|\"Dm\"d3|\"E\"e/2d/2e/2f/2|e/2d/2c/2B/2|\"Am\"A/2B/2A/2G/2Ae|\"Dm\"f/2g/2f/2e/2f/2g/2f/2|\"E\"e/2d/2e/2f/2e/2d/2c/2B/2|\"Am\"AA|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:G\n",
      "|\"G\"\"Am\"|\"C\"|\"G\"\"D\"|\"G\"|\"G\"\"C\"|\"G\"\"Em\"|\"G\"|\"D\"|\"G\"|\"G\"|\"G\"|\"G\"|\"Am\"\"D\"|\"G\"|\"D\"|\"G\"|\"G\"|]\n",
      "B|\"G\"BA\"Am\"A3/2B/2|\"C\"A/2B/2G/2E/2D/2E/2D/2E/2|\"G\"DD\"D\"E|\"G\"GA\"D\"A3/2B/2|\"G\"\"Em\"GB/2c/2d|\"D\"ed/2e/2f/2e/2d/2|\"G\"BAGe|\"D\"dd/2e/2d|\"G\"GGB/2c/2d|\"G\"g3/2a/2ge|\"G\"d3/2e/2dd|\"G\"BGA/2B/2c/2B/2|\"Am\"AA\"D\"A3/2B/2|\"G\"cde/2f/\n",
      "---------------\n",
      "M:4/4L:1/4K:Am|\"Am\"|\"Dm\"|\"E\"|\"E\"|\"Am\"|\"Dm\"|\"E\"|\"Am\"|]\n",
      "\"Am\"e3/4d/4B/2A/2|\"Dm\"F/2G/2A/2B/2|\"E\"G/2E/2F/2E/2|\"E\"B3/4d/4e/2c/2|\"E\"B/2A/2G/2F/2|\"Am\"E2|\"Dm\"F3/4G/4A/2B/2|\"E\"c/2B/2A/2G/2|\"Am\"F3/4G/4A/2B/2|\"Dm\"F3/4G/4A/2B/2|\"E\"c/2B/2A/2G/2|\"Am\"F3/4G/4A/2B/2|\"E\"c/2B/2A/2G/2|\"Am\"F3/4G/4A/2B/2|\"Dm\"c3/4B/4\"Bm\"A/2G/2|\"E\"F/2E/2\"Am\"E|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:A\n",
      "|\"A\"\"D\"|\"A\"\"E\"|\"A\"\"Fm\"|\"Bm\"|\"E\"\"A\"\"E\"|\"A\"\"Fm\"|\"Bm\"\"E\"|\"Bm\"\"E\"|\"B\"\"E\"|\"B\"\"E\"|\"B\"\"E\"|\"B\"\"E\"|\"A\"\"Fm\"|\"B\"\"E\"|]\n",
      "\"A\"AB/2A/2\"D\"F/2A/2|\"A\"AB/2\"E\"c/2d/2|\"A\"e/2f/2e/2c/2\"Fm\"ae/2c/2|\"Bm\"B/2c/2B/2A/2\"E\"G/2A/2B/2G/2|\"E\"B3/2c/2\"\n",
      "---------------\n",
      "M:4/4L:1/4K:Am|\"Am\"|\"Dm\"|\"E\"|\"E\"|\"Am\"|\"Dm\"|\"E\"|\"Am\"|]\n",
      "E|\"Am\"A3/2B/2c/2|\"Dm\"d3/2c/2B/2|\"E\"G/2A/2B/2e/2|\"E\"d3/2c/4d/2B/2|\"Am\"A3/2B/2c/2d/2|\"Dm\"c/4B/4A/2F/2E/2|\"E\"E3/2E/2|\"Am\"A3/2B/2c/2|\"Dm\"d3/2c/2B/2|\"E\"G/2A/2G/2F/2|\"Am\"A3/2B/2c/2|\"Dm\"d3/2c/2B/2|\"E\"G/2A/2B/2G/2|\"Am\"A3/2B/2c/2|\"Dm\"d3/2c/2B/2c/2|\"E\"B/2c/2B/2c/2|\"Am\"A3/2B/2c/2|\"E\"B/2c/2B/2c/2|\"E\"B/2c/2G/2E/2|\"Am\"A3/2B/2c/2|\"Dm\"d3/2c/2B/2|\"E\"G/2A/2G/2F/2|\"Am\"A3/2B/2c/2|\"Dm\"d3/2c/2B/2|\"E\"G/2A/2B/2G/2|\"Am\"A3/2B/2c/2|\"E\"B/2c/2B/2c/2|\"E\"B/2c/2B/2c/2|\"E\"B/2c/2B/2c/2|\"B\"d3/2c/2B/2|\"E\"G/2A/2G/2F/2|\"Am\"E3/2|]\n",
      "\n",
      "\n",
      "---------------\n",
      "M:4/4L:1/4K:Am|\"Am\"|\"Dm\"|\"E\"|\"E\"|\"Am\"|\"Dm\"|\"E\"|\"Am\"|]\n",
      "\"Am\"c/2e/2c/2B/2|\"Dm\"A/2B/2A/2G/2E/2|\"E\"B/2EF/2G3/2|\"E\"B/2d/2c/2B/2|\"Am\"A/2G/2F/2E/2|\"Dm\"F3/2G/2A/2|\"E\"B/2d/2c/2B/2|\"Am\"A/2G/2F/2E/2|\"Dm\"D3|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:G\n",
      "|\"G\"|\"Am\"|\"D7\"|\"G\"|\"G\"|\"Am\"|\"D\"|\"D7\"|\"G\"|\"G\"|\"G\"|\"Am\"|\"D\"|\"D7\"|\"G\"|\"G\"|\"Am\"|\"D\"|\"D\"|\"G\"|\"G\"|\"Am\"|\"D\"|\"D7\"|\"G\"|\"G\"|]\n",
      "\"G\"G2B2|\"Am\"AA2|\"D7\"d2cB|\"G\"B4|\"G\"G2A2|\"Am\"A2E2|\"D\"F2AG|\"D\"A2d2|\"D7\"d2cB|\"G\"G4-|\"G\"G2|G2|\"G\"d3|\"Am\"e3/2e/2|\"D\"e3/2e/2|\"D\"ef2e|\"D7\"d2Bc|\"G\"d3|\"G\"d3|\"Am\"e3/2e/2|\"D\"ef2e|\"D\"d2Bc|\"G\"d3|\"G\"Bcd|\"Am\"e3|\"D7\"d2A2|\"G\"G4-|\"G\"G2|]\n",
      "\n",
      "M:3/\n",
      "---------------\n",
      "M:4/4L:1/4K:Am|\"Am\"|\"Dm\"|\"E\"|\"E\"|\"Am\"|\"Dm\"|\"E\"|\"Am\"|]\n",
      "E|\"Am\"A/2A/2A/2B/2|\"Dm\"A/2A/2B/2c/2|\"E\"e3/4d/4c/2B/2|\"E\"B/2c/2d/2c/2|\"E\"B/2c/2d/2c/2|\"Am\"A3/2|G/2|\"Am\"A3/2(3c/2d/2e/2|\"Dm\"d3/2e/2d/2|\"E\"B/2c/2d/2c/2|\"Am\"A3/2c/2e/2|\"Dm\"d3/2e/2f/2e/2|\"E\"d/2B/2c/2d/2c/2|\"Am\"A3/2g/2e/2|\"Dm\"d3/2e/2f/2e/2|\"E\"d/2B/2c/2d/2c/2|\"Am\"A3/2G/2A/2|\"Am\"A3/2G/2A/2B/2|\"E\"c/2B/2A/2G/2|\"Am\"A3/2g/2e/2|\"Dm\"d3/2e/2f/2e/2|\"E\"d/2B/2c/2d/2c/2|\"Am\"A3/2c/2e/2|\"Dm\"d3/2e/2f/2e/2|\"E\"d/2B/2c/2d/2c/2|\"Am\"A3|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:G\n",
      "|\"G\"\"D7\"|\"G\"\"C\"|\"G\"\"D7\"|\"G\"|\"G\"\"C\"|\"G\"|\"D7\"\"G\"|\"G\"|\"C\"\"D7\"|\"G\"\"C\"|\"\n",
      "---------------\n",
      "M:4/4L:1/4K:Am|\"Am\"|\"Dm\"|\"E\"|\"E\"|\"Am\"|\"Dm\"|\"E\"|\"Am\"|]\n",
      "E|\"Am\"A2Bc|\"Dm\"dde3/2d/2|\"E\"cBBc|\"E\"B4|\"Am\"A2AB|\"Dm\"dde3/2d/2|\"E\"cBBc|\"Am\"A2AB|\"Dm\"d4-|\"Dm\"d3/2d/2|\"E\"cB2A|\"Am\"BAA2|\"Dm\"d4-|\"E\"d3/2d/2BA|\"Am\"B2AB|\"Dm\"d4|\"E\"cB2A|\"Am\"BAA2|\"Dm\"d4-|\"Dm\"d4|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:F#\n",
      "|\"F\"|\"F\"|\"F\"|\"C\"\"Dm\"|\"F\"|\"F\"|\"C\"\"Dm\"|\"F\"|\"F\"|\"C\"\"Dm\"|\"F\"\"C\"|\"F\"|\"F\"|\"C\"\"Dm\"|\"F\"|\"F\"|\"C\"\"Dm\"|]\n",
      "(3c/2d/2e/2|\"F\"fdcB|\"F\"A/2G/2F/2G/2A/2G/2F|\"C\"G/2F/2E/2D/2CD/2E/2|\"F\"cAAG|\"C\"G/2F/2E/2D/2CD/2E/2|\"F\"cAAG|\"C\"A/2c/2^B/2d/2\"Dm\"cA|\"F\"A/2G/2F/2A/2G/2F|\"C\"G/2F/2E/2D/2CD/2E/2|\"Dm\"FFD2|\"F\"A/2cA/2cA|\"C\"G/2F\n",
      "---------------\n",
      "M:4/4L:1/4K:Am|\"Am\"|\"Dm\"|\"E\"|\"E\"|\"Am\"|\"Dm\"|\"E\"|\"Am\"|]\n",
      "\"Am\"A3/2G/2F/2E/2|\"Dm\"F/2E/2D/2E/2F/2G/2|\"E\"B3/2d/2c/2B/2|\"E\"^cd3/2e/2|\"E\"B/2c/2B/2A/2G/2F/2|\"E\"^F/2G/2E/2F/2G/2|\"Am\"A3/2A/2G/2F/2E/2|\"Dm\"F/2E/2D/2E/2F/2G/2A/2|\"E\"=c3/2B/2A/2B3/2c/2|\"Am\"A3/2G/2F/2E3/2c/2|\"Dm\"F3/2E/2DE|\"E\"=cd3/2e/2|\"Am\"f/2e/2f/2g/2a/2e/2f/2g/2|\"Dm\"f/2e/2f/2g/2f3/2e/2|\"E\"^d3/2e/2BB/2c/2|\"E\"^dBeB|\"Am\"A3/2A/2GA|\"Am\"A3/2G/2FA|\"Dm\"F3/2E/2DE|\"E\"^cd3/2e/2|\"Am\"f/2e/2f/2g/2a/2e/2f/2g/2|\"Dm\"f/2e/2f/2g/2f3/2e/2|\"G\"^d3/2e/2f^d|\"Am\"e3/2A/2GA|\"Am\"A3|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:F\n",
      "|\"F\"|\"F\"|\"F\"|\"B\"|\"F\"|\"C\"\n",
      "---------------\n",
      "M:4/4L:1/4K:Am|\"Am\"|\"Dm\"|\"E\"|\"E\"|\"Am\"|\"Dm\"|\"E\"|\"Am\"|]\n",
      "A,|\"Am\"A,CE|\"Dm\"DDE/2F/2|\"E\"G/2E/2B,/2E/2G/2E/2|\"E\"B,/2ED/2E/2F/2|\"E\"A,/2A,/2A,/2B,/2C/2E,/2|\"Am\"A,CE|\"Dm\"D3|A|\"Am\"ee/2d/2e/2f/2e/2|\"Dm\"f/2e/2d/2c/2d3/2c/2|\"E\"d/2e/2d/2c/2d3/2c/2|\"Am\"A/2B/2A/2G/2FA|\"Dm\"dd/2A/2ee/2d/2|\"E\"ee/2d/2c/2d/2c/2B/2|\"Am\"AAA|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:F\n",
      "|\"F\"|\"F\"|\"F\"|\"F\"|\"F\"\"C7\"|\"F\"|\"F\"|\"B\"|\"F\"|\"C7\"|\"F\"\"B\"|\"F\"\"C7\"|\"F\"|]\n",
      "F/2G/2|\"F\"AAB/2A/2G/2F/2|\"F\"AAB/2A/2G/2F/2|\"F\"AAB/2A/2G/2F/2|\"F\"AAB/2A/2G/2F/2|\"F\"A/2G/2A/2F/2\"C7\"F/2G/2A/2B/2|\"F\"c/2d/2c/2B/2A/2G/2F/2|\"F\"AAB/2A/2G/2F/2|\"B\"E/2F/2G/\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "!python3 sample.py --out_dir=out-abc-char --start='M:4/4L:1/4K:Am|\"Am\"|\"Dm\"|\"E\"|\"E\"|\"Am\"|\"Dm\"|\"E\"|\"Am\"|]'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test older checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " !python3 sample.py --out_dir=older_ckpt/m_voices --path_meta=older_ckpt/m_voices --start='M:4/4L:1/4K:G|\"G\"|\"C\"|\"D\"|\"D\"|\"G\"|\"C\"|\"D\"|\"G\"|]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat older_ckpt/m_voices/ckpt.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l older_ckpt/m_voices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l out-abc-char/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!id"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
