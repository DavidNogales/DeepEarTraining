{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def load_dataframe(relative_path,dataframe_name):\n",
    "    df = pd.read_pickle(f'{relative_path}/{dataframe_name}.pkl')    \n",
    "    return df\n",
    "\n",
    "def read_file(relative_path,file_name):\n",
    "    text= \"\"\n",
    "    with open(f'{relative_path}/{file_name}.abc','r') as f:\n",
    "        text = f.read()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_path =\"notebooks/data/final_dataset\"\n",
    "filename_name = 'clean_augmented_data'\n",
    "#filename_name = 'clean_original_training_data'\n",
    "#relative_path =\"notebooks/data/original_dataset\"\n",
    "training_data_df = load_dataframe(relative_path,filename_name)\n",
    "training_data_df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_df[\"clean_header\"].str.len().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_df[\"clean_body\"].str.len().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bodies = \"\"\n",
    "silences = 0\n",
    "for body in training_data_df[\"clean_body\"]:\n",
    "    if 'z' in body:\n",
    "        silences +=1 \n",
    "    bodies += body+\"\\n\"\n",
    "chars = sorted(list(set(bodies)))\n",
    "vocab_size = len(chars)\n",
    "print('vocab: ',''.join(chars))\n",
    "print('vocab_size',vocab_size)\n",
    "print(\"silences \",silences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_text = read_file(relative_path,filename_name)\n",
    "\n",
    "print(\"number of chars:\",len(training_data_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(training_data_text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import tiktoken\n",
    "\n",
    "print(wandb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dockerfile  docker-compose.yaml  overrides.json\n",
      "README.md   notebooks\t\t requirements.txt\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "nano_path = 'notebooks/nanoGPT'\n",
    "os.chdir(nano_path)\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LICENSE      assets\t      data\t  out-abc-char\twandb\n",
      "README.md    config\t      model.py\t  sample.py\n",
      "__pycache__  configurator.py  older_ckpt  train.py\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset with multiple voices present\n",
    "#length of dataset in characters: 4,149,703\n",
    "#all the unique characters: \n",
    "#\"#'()+,-/123456789:=ABCDEFGKLM[]^_abcdefgmz|~\n",
    "#vocab size: 46\n",
    "#train has 3,734,732 tokens\n",
    "#val has 414,971 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters: 4,062,773\n",
      "all the unique characters: \n",
      "\"#'(),-/123456789:=ABCDEFGKLM[]^_abcdefgmz|~\n",
      "vocab size: 45\n",
      "train has 3,656,495 tokens\n",
      "val has 406,278 tokens\n"
     ]
    }
   ],
   "source": [
    "!python3 data/abc_char/prepare.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_abc_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-abc-char'\n",
      "eval_interval = 10 # keep frequent because we'll overfit\n",
      "eval_iters = 500\n",
      "log_interval = 5 # don't print too too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = True # override via command line if you like\n",
      "wandb_project = 'abc-char'\n",
      "wandb_run_name = 'mini-char-gpt-hd-8-ly-12-bt-4-ctx-256'\n",
      "\n",
      "dataset = 'abc_char'\n",
      "batch_size = 4\n",
      "block_size = 256 # context of up to 512 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 12\n",
      "n_head = 8\n",
      "n_embd = 384\n",
      "dropout = 0.2\n",
      "\n",
      "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
      "max_iters = 5000\n",
      "lr_decay_iters = 5000 # make equal to max_iters usually\n",
      "min_lr = 1e-4 # learning_rate / 10 usually\n",
      "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 5 # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "found vocab_size = 45 (inside data/abc_char/meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 21.26M\n",
      "using fused AdamW: True\n",
      "compiling the model... (takes a ~minute)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdavidnogales\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.0 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.14.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/pt-env/notebooks/nanoGPT/wandb/run-20230427_202703-n5i6m19z\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmini-char-gpt-hd-8-ly-12-bt-4-ctx-256\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/davidnogales/abc-char\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/davidnogales/abc-char/runs/n5i6m19z\u001b[0m\n",
      "step 0: train loss 3.9243, val loss 3.9241\n",
      "[2023-04-27 20:27:18,077] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-27 20:27:18,588] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-27 20:27:19,183] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-27 20:27:19,369] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-27 20:27:19,626] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-27 20:27:19,816] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-27 20:27:20,066] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-27 20:27:20,248] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-27 20:27:20,499] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-27 20:27:20,679] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-27 20:27:20,930] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-27 20:27:21,112] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-27 20:27:21,450] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-27 20:27:21,633] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-27 20:27:21,880] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-27 20:27:22,060] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-27 20:27:22,319] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-27 20:27:22,509] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-27 20:27:22,763] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-27 20:27:22,947] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-27 20:27:23,214] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-27 20:27:23,399] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-27 20:27:23,651] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-27 20:27:23,840] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "iter 0: loss 3.9031, time 23893.15ms, mfu -100.00%\n",
      "iter 5: loss 3.1619, time 758.54ms, mfu 2.45%\n",
      "step 10: train loss 2.8181, val loss 2.8023\n",
      "saving checkpoint to out-abc-char\n",
      "iter 10: loss 2.6862, time 7361.31ms, mfu 2.23%\n",
      "iter 15: loss 2.4379, time 756.88ms, mfu 2.26%\n",
      "step 20: train loss 2.3521, val loss 2.3798\n",
      "saving checkpoint to out-abc-char\n",
      "iter 20: loss 2.0251, time 7121.01ms, mfu 2.06%\n",
      "iter 25: loss 2.2773, time 869.04ms, mfu 2.06%\n",
      "step 30: train loss 2.1542, val loss 2.1680\n",
      "saving checkpoint to out-abc-char\n",
      "iter 30: loss 1.8552, time 7986.04ms, mfu 1.88%\n",
      "iter 35: loss 2.4103, time 860.06ms, mfu 1.91%\n",
      "step 40: train loss 2.0656, val loss 2.1081\n",
      "saving checkpoint to out-abc-char\n",
      "iter 40: loss 1.7526, time 8703.65ms, mfu 1.74%\n",
      "iter 45: loss 2.0127, time 835.71ms, mfu 1.79%\n",
      "step 50: train loss 1.9763, val loss 2.0079\n",
      "saving checkpoint to out-abc-char\n",
      "iter 50: loss 2.2000, time 8183.55ms, mfu 1.63%\n",
      "iter 55: loss 2.1385, time 874.95ms, mfu 1.68%\n",
      "step 60: train loss 1.8957, val loss 1.9266\n",
      "saving checkpoint to out-abc-char\n",
      "iter 60: loss 2.2617, time 8129.35ms, mfu 1.54%\n",
      "iter 65: loss 1.6259, time 931.12ms, mfu 1.58%\n",
      "step 70: train loss 1.8381, val loss 1.8588\n",
      "saving checkpoint to out-abc-char\n",
      "iter 70: loss 1.9270, time 7849.00ms, mfu 1.45%\n",
      "iter 75: loss 1.8883, time 870.76ms, mfu 1.52%\n",
      "step 80: train loss 1.7510, val loss 1.7794\n",
      "saving checkpoint to out-abc-char\n",
      "iter 80: loss 1.9147, time 7721.04ms, mfu 1.39%\n",
      "iter 85: loss 1.7294, time 840.42ms, mfu 1.47%\n",
      "step 90: train loss 1.7100, val loss 1.7474\n",
      "saving checkpoint to out-abc-char\n",
      "iter 90: loss 1.6418, time 7660.46ms, mfu 1.35%\n",
      "iter 95: loss 1.6556, time 796.63ms, mfu 1.45%\n",
      "step 100: train loss 1.6672, val loss 1.6796\n",
      "saving checkpoint to out-abc-char\n",
      "iter 100: loss 1.6766, time 7884.40ms, mfu 1.33%\n",
      "iter 105: loss 1.5766, time 955.57ms, mfu 1.39%\n",
      "step 110: train loss 1.6644, val loss 1.6704\n",
      "saving checkpoint to out-abc-char\n",
      "iter 110: loss 1.4824, time 7888.79ms, mfu 1.27%\n",
      "iter 115: loss 1.5342, time 877.43ms, mfu 1.36%\n",
      "step 120: train loss 1.6474, val loss 1.6357\n",
      "saving checkpoint to out-abc-char\n",
      "iter 120: loss 1.8258, time 8011.14ms, mfu 1.25%\n",
      "iter 125: loss 1.7135, time 907.34ms, mfu 1.33%\n",
      "step 130: train loss 1.5903, val loss 1.6163\n",
      "saving checkpoint to out-abc-char\n",
      "iter 130: loss 1.8322, time 7822.63ms, mfu 1.22%\n",
      "iter 135: loss 1.6249, time 890.16ms, mfu 1.30%\n",
      "step 140: train loss 1.5787, val loss 1.5889\n",
      "saving checkpoint to out-abc-char\n",
      "iter 140: loss 1.7521, time 8094.42ms, mfu 1.20%\n",
      "iter 145: loss 1.5287, time 857.30ms, mfu 1.29%\n",
      "step 150: train loss 1.5640, val loss 1.5701\n",
      "saving checkpoint to out-abc-char\n",
      "iter 150: loss 1.7300, time 8114.31ms, mfu 1.19%\n",
      "iter 155: loss 1.6136, time 914.87ms, mfu 1.27%\n",
      "step 160: train loss 1.5375, val loss 1.5473\n",
      "saving checkpoint to out-abc-char\n",
      "iter 160: loss 1.8302, time 7800.58ms, mfu 1.17%\n",
      "iter 165: loss 1.5648, time 869.91ms, mfu 1.27%\n",
      "step 170: train loss 1.5261, val loss 1.5317\n",
      "saving checkpoint to out-abc-char\n",
      "iter 170: loss 1.6899, time 7865.35ms, mfu 1.16%\n",
      "iter 175: loss 1.7371, time 901.54ms, mfu 1.25%\n",
      "step 180: train loss 1.5097, val loss 1.5266\n",
      "saving checkpoint to out-abc-char\n",
      "iter 180: loss 1.4528, time 7806.98ms, mfu 1.15%\n",
      "iter 185: loss 1.6772, time 814.89ms, mfu 1.26%\n",
      "step 190: train loss 1.4936, val loss 1.5216\n",
      "saving checkpoint to out-abc-char\n",
      "iter 190: loss 1.5908, time 8308.58ms, mfu 1.16%\n",
      "iter 195: loss 1.3471, time 922.98ms, mfu 1.25%\n",
      "step 200: train loss 1.4766, val loss 1.5150\n",
      "saving checkpoint to out-abc-char\n",
      "iter 200: loss 1.3097, time 7868.39ms, mfu 1.15%\n",
      "iter 205: loss 1.4703, time 852.29ms, mfu 1.25%\n",
      "step 210: train loss 1.4589, val loss 1.4684\n",
      "saving checkpoint to out-abc-char\n",
      "iter 210: loss 1.5299, time 8301.73ms, mfu 1.15%\n",
      "iter 215: loss 1.4732, time 969.21ms, mfu 1.22%\n",
      "step 220: train loss 1.4354, val loss 1.4571\n",
      "saving checkpoint to out-abc-char\n",
      "iter 220: loss 1.3885, time 8356.83ms, mfu 1.12%\n",
      "iter 225: loss 1.5876, time 937.45ms, mfu 1.21%\n",
      "step 230: train loss 1.4467, val loss 1.4570\n",
      "saving checkpoint to out-abc-char\n",
      "iter 230: loss 1.5331, time 7773.90ms, mfu 1.11%\n",
      "iter 235: loss 1.3153, time 769.40ms, mfu 1.24%\n",
      "step 240: train loss 1.4144, val loss 1.4118\n",
      "saving checkpoint to out-abc-char\n",
      "iter 240: loss 1.5553, time 7420.04ms, mfu 1.14%\n",
      "iter 245: loss 1.4999, time 912.58ms, mfu 1.23%\n",
      "step 250: train loss 1.3880, val loss 1.3898\n",
      "saving checkpoint to out-abc-char\n",
      "iter 250: loss 1.1206, time 7919.36ms, mfu 1.13%\n",
      "iter 255: loss 1.6083, time 835.32ms, mfu 1.24%\n",
      "step 260: train loss 1.3481, val loss 1.3694\n",
      "saving checkpoint to out-abc-char\n",
      "iter 260: loss 1.4488, time 8015.87ms, mfu 1.14%\n",
      "iter 265: loss 1.3939, time 955.13ms, mfu 1.22%\n",
      "step 270: train loss 1.3301, val loss 1.3440\n",
      "saving checkpoint to out-abc-char\n",
      "iter 270: loss 1.3241, time 7690.09ms, mfu 1.12%\n",
      "iter 275: loss 1.3055, time 919.70ms, mfu 1.21%\n",
      "step 280: train loss 1.3031, val loss 1.3229\n",
      "saving checkpoint to out-abc-char\n",
      "iter 280: loss 1.4789, time 7783.65ms, mfu 1.12%\n",
      "iter 285: loss 1.3801, time 863.92ms, mfu 1.22%\n",
      "step 290: train loss 1.2818, val loss 1.3151\n",
      "saving checkpoint to out-abc-char\n",
      "iter 290: loss 1.3897, time 7629.25ms, mfu 1.12%\n",
      "iter 295: loss 1.1841, time 778.83ms, mfu 1.25%\n",
      "step 300: train loss 1.2521, val loss 1.2734\n",
      "saving checkpoint to out-abc-char\n",
      "iter 300: loss 1.5193, time 8137.88ms, mfu 1.15%\n",
      "iter 305: loss 1.1718, time 880.18ms, mfu 1.24%\n",
      "step 310: train loss 1.2537, val loss 1.2644\n",
      "saving checkpoint to out-abc-char\n",
      "iter 310: loss 1.3250, time 7706.04ms, mfu 1.14%\n",
      "iter 315: loss 1.2238, time 782.23ms, mfu 1.27%\n",
      "step 320: train loss 1.2193, val loss 1.2456\n",
      "saving checkpoint to out-abc-char\n",
      "iter 320: loss 1.0173, time 7582.43ms, mfu 1.17%\n",
      "iter 325: loss 1.0600, time 890.77ms, mfu 1.26%\n",
      "step 330: train loss 1.2232, val loss 1.2449\n",
      "saving checkpoint to out-abc-char\n",
      "iter 330: loss 1.2698, time 8188.90ms, mfu 1.15%\n",
      "iter 335: loss 1.1623, time 823.31ms, mfu 1.26%\n",
      "step 340: train loss 1.1797, val loss 1.2084\n",
      "saving checkpoint to out-abc-char\n",
      "iter 340: loss 1.1522, time 7769.19ms, mfu 1.16%\n",
      "iter 345: loss 1.1197, time 816.30ms, mfu 1.27%\n",
      "step 350: train loss 1.1733, val loss 1.1816\n",
      "saving checkpoint to out-abc-char\n",
      "iter 350: loss 1.2163, time 7698.72ms, mfu 1.17%\n",
      "iter 355: loss 1.2479, time 1020.03ms, mfu 1.24%\n",
      "step 360: train loss 1.1488, val loss 1.1850\n",
      "iter 360: loss 1.2414, time 7306.63ms, mfu 1.14%\n",
      "iter 365: loss 1.3847, time 1069.90ms, mfu 1.20%\n",
      "step 370: train loss 1.1456, val loss 1.1720\n",
      "saving checkpoint to out-abc-char\n",
      "iter 370: loss 0.9029, time 7842.90ms, mfu 1.10%\n",
      "iter 375: loss 1.1377, time 985.00ms, mfu 1.18%\n",
      "step 380: train loss 1.1342, val loss 1.1753\n",
      "iter 380: loss 1.0739, time 7481.88ms, mfu 1.09%\n",
      "iter 385: loss 1.1771, time 869.39ms, mfu 1.19%\n",
      "step 390: train loss 1.1150, val loss 1.1407\n",
      "saving checkpoint to out-abc-char\n",
      "iter 390: loss 1.0292, time 7568.83ms, mfu 1.10%\n",
      "iter 395: loss 1.1670, time 871.22ms, mfu 1.20%\n",
      "step 400: train loss 1.0999, val loss 1.1440\n",
      "iter 400: loss 1.2243, time 7291.87ms, mfu 1.11%\n",
      "iter 405: loss 1.0560, time 882.57ms, mfu 1.21%\n",
      "step 410: train loss 1.0886, val loss 1.1250\n",
      "saving checkpoint to out-abc-char\n",
      "iter 410: loss 1.0979, time 7915.61ms, mfu 1.11%\n",
      "iter 415: loss 0.9013, time 871.09ms, mfu 1.21%\n",
      "step 420: train loss 1.0891, val loss 1.1119\n",
      "saving checkpoint to out-abc-char\n",
      "iter 420: loss 1.0279, time 7917.97ms, mfu 1.11%\n",
      "iter 425: loss 1.0409, time 963.72ms, mfu 1.20%\n",
      "step 430: train loss 1.0638, val loss 1.0986\n",
      "saving checkpoint to out-abc-char\n",
      "iter 430: loss 1.1308, time 8210.96ms, mfu 1.10%\n",
      "iter 435: loss 1.0507, time 839.44ms, mfu 1.21%\n",
      "step 440: train loss 1.0563, val loss 1.0955\n",
      "saving checkpoint to out-abc-char\n",
      "iter 440: loss 1.0491, time 7955.86ms, mfu 1.11%\n",
      "iter 445: loss 1.0127, time 971.91ms, mfu 1.19%\n",
      "step 450: train loss 1.0431, val loss 1.0712\n",
      "saving checkpoint to out-abc-char\n",
      "iter 450: loss 1.0141, time 7897.40ms, mfu 1.10%\n",
      "iter 455: loss 1.2203, time 888.03ms, mfu 1.20%\n",
      "step 460: train loss 1.0244, val loss 1.0587\n",
      "saving checkpoint to out-abc-char\n",
      "iter 460: loss 1.0362, time 7521.01ms, mfu 1.10%\n",
      "iter 465: loss 0.9804, time 883.20ms, mfu 1.20%\n",
      "step 470: train loss 1.0162, val loss 1.0334\n",
      "saving checkpoint to out-abc-char\n",
      "iter 470: loss 0.9313, time 7182.19ms, mfu 1.11%\n",
      "iter 475: loss 0.9518, time 844.25ms, mfu 1.22%\n",
      "step 480: train loss 1.0034, val loss 1.0225\n",
      "saving checkpoint to out-abc-char\n",
      "iter 480: loss 0.9886, time 7098.33ms, mfu 1.12%\n",
      "iter 485: loss 1.0090, time 873.44ms, mfu 1.22%\n",
      "step 490: train loss 0.9871, val loss 1.0194\n",
      "saving checkpoint to out-abc-char\n",
      "iter 490: loss 1.0880, time 6882.58ms, mfu 1.13%\n",
      "iter 495: loss 1.0308, time 761.08ms, mfu 1.26%\n",
      "step 500: train loss 0.9733, val loss 0.9910\n",
      "saving checkpoint to out-abc-char\n",
      "iter 500: loss 1.2744, time 7853.18ms, mfu 1.16%\n",
      "iter 505: loss 1.0257, time 867.27ms, mfu 1.26%\n",
      "step 510: train loss 0.9540, val loss 0.9996\n",
      "iter 510: loss 1.2146, time 7774.97ms, mfu 1.15%\n",
      "iter 515: loss 0.9674, time 867.00ms, mfu 1.25%\n",
      "step 520: train loss 0.9492, val loss 0.9733\n",
      "saving checkpoint to out-abc-char\n",
      "iter 520: loss 0.9090, time 7675.67ms, mfu 1.15%\n",
      "iter 525: loss 0.9793, time 853.90ms, mfu 1.26%\n",
      "step 530: train loss 0.9297, val loss 0.9611\n",
      "saving checkpoint to out-abc-char\n",
      "iter 530: loss 1.0936, time 8234.80ms, mfu 1.15%\n",
      "iter 535: loss 0.8946, time 938.34ms, mfu 1.24%\n",
      "step 540: train loss 0.9311, val loss 0.9535\n",
      "saving checkpoint to out-abc-char\n",
      "iter 540: loss 0.9087, time 7568.88ms, mfu 1.14%\n",
      "iter 545: loss 0.9607, time 1030.63ms, mfu 1.20%\n",
      "step 550: train loss 0.9258, val loss 0.9576\n",
      "iter 550: loss 0.9023, time 7320.60ms, mfu 1.11%\n",
      "iter 555: loss 1.1510, time 757.66ms, mfu 1.24%\n",
      "step 560: train loss 0.8972, val loss 0.9396\n",
      "saving checkpoint to out-abc-char\n",
      "iter 560: loss 0.9325, time 7796.99ms, mfu 1.14%\n",
      "iter 565: loss 1.0625, time 794.95ms, mfu 1.26%\n",
      "step 570: train loss 0.8984, val loss 0.9369\n",
      "saving checkpoint to out-abc-char\n",
      "iter 570: loss 0.9621, time 7148.02ms, mfu 1.16%\n",
      "iter 575: loss 1.0021, time 887.25ms, mfu 1.26%\n",
      "step 580: train loss 0.8931, val loss 0.9283\n",
      "saving checkpoint to out-abc-char\n",
      "iter 580: loss 0.8344, time 7343.10ms, mfu 1.16%\n",
      "iter 585: loss 0.7546, time 795.69ms, mfu 1.27%\n",
      "step 590: train loss 0.8775, val loss 0.9366\n",
      "iter 590: loss 1.0059, time 7313.21ms, mfu 1.17%\n",
      "iter 595: loss 0.9706, time 894.43ms, mfu 1.26%\n",
      "step 600: train loss 0.8675, val loss 0.9113\n",
      "saving checkpoint to out-abc-char\n",
      "iter 600: loss 0.9446, time 7656.54ms, mfu 1.16%\n",
      "iter 605: loss 0.8643, time 847.02ms, mfu 1.26%\n",
      "step 610: train loss 0.8600, val loss 0.8966\n",
      "saving checkpoint to out-abc-char\n",
      "iter 610: loss 1.0078, time 7762.47ms, mfu 1.16%\n",
      "iter 615: loss 0.8118, time 871.67ms, mfu 1.26%\n",
      "step 620: train loss 0.8584, val loss 0.8962\n",
      "saving checkpoint to out-abc-char\n",
      "iter 620: loss 0.9316, time 7117.83ms, mfu 1.16%\n",
      "iter 625: loss 0.9208, time 869.82ms, mfu 1.26%\n",
      "step 630: train loss 0.8432, val loss 0.8852\n",
      "saving checkpoint to out-abc-char\n",
      "iter 630: loss 0.8511, time 7168.28ms, mfu 1.16%\n",
      "iter 635: loss 1.0807, time 811.67ms, mfu 1.27%\n",
      "step 640: train loss 0.8390, val loss 0.8720\n",
      "saving checkpoint to out-abc-char\n",
      "iter 640: loss 0.7998, time 8284.92ms, mfu 1.17%\n",
      "iter 645: loss 0.8271, time 883.32ms, mfu 1.26%\n",
      "step 650: train loss 0.8284, val loss 0.8665\n",
      "saving checkpoint to out-abc-char\n",
      "iter 650: loss 1.0207, time 8241.79ms, mfu 1.16%\n",
      "iter 655: loss 0.7872, time 953.49ms, mfu 1.24%\n",
      "step 660: train loss 0.8113, val loss 0.8456\n",
      "saving checkpoint to out-abc-char\n",
      "iter 660: loss 0.8892, time 8551.59ms, mfu 1.13%\n",
      "iter 665: loss 0.8717, time 913.66ms, mfu 1.22%\n",
      "step 670: train loss 0.8178, val loss 0.8610\n",
      "iter 670: loss 0.8353, time 8155.23ms, mfu 1.12%\n",
      "iter 675: loss 0.9083, time 865.76ms, mfu 1.23%\n",
      "step 680: train loss 0.8130, val loss 0.8425\n",
      "saving checkpoint to out-abc-char\n",
      "iter 680: loss 0.9479, time 7939.58ms, mfu 1.13%\n",
      "iter 685: loss 0.7716, time 861.08ms, mfu 1.23%\n",
      "step 690: train loss 0.7968, val loss 0.8292\n",
      "saving checkpoint to out-abc-char\n",
      "iter 690: loss 0.6614, time 7839.51ms, mfu 1.13%\n",
      "iter 695: loss 0.8084, time 877.35ms, mfu 1.23%\n",
      "step 700: train loss 0.7894, val loss 0.8402\n",
      "iter 700: loss 0.7783, time 6933.04ms, mfu 1.13%\n",
      "iter 705: loss 0.9732, time 993.24ms, mfu 1.21%\n",
      "step 710: train loss 0.7775, val loss 0.8125\n",
      "saving checkpoint to out-abc-char\n",
      "iter 710: loss 0.9073, time 6989.53ms, mfu 1.11%\n",
      "iter 715: loss 0.7076, time 799.04ms, mfu 1.24%\n",
      "step 720: train loss 0.7717, val loss 0.8153\n",
      "iter 720: loss 0.6022, time 6722.99ms, mfu 1.14%\n",
      "iter 725: loss 0.9254, time 777.28ms, mfu 1.27%\n",
      "step 730: train loss 0.7539, val loss 0.7930\n",
      "saving checkpoint to out-abc-char\n",
      "iter 730: loss 0.9023, time 7039.62ms, mfu 1.17%\n",
      "iter 735: loss 0.6913, time 829.02ms, mfu 1.27%\n",
      "step 740: train loss 0.7536, val loss 0.8066\n",
      "iter 740: loss 0.7677, time 7735.57ms, mfu 1.17%\n",
      "iter 745: loss 0.9004, time 829.85ms, mfu 1.28%\n",
      "step 750: train loss 0.7548, val loss 0.7965\n",
      "iter 750: loss 0.8779, time 7487.95ms, mfu 1.17%\n",
      "iter 755: loss 0.7136, time 1006.50ms, mfu 1.24%\n",
      "step 760: train loss 0.7424, val loss 0.7855\n",
      "saving checkpoint to out-abc-char\n",
      "iter 760: loss 0.7464, time 7534.17ms, mfu 1.14%\n",
      "iter 765: loss 0.8452, time 881.28ms, mfu 1.24%\n",
      "step 770: train loss 0.7355, val loss 0.7708\n",
      "saving checkpoint to out-abc-char\n",
      "iter 770: loss 0.7879, time 7739.36ms, mfu 1.14%\n",
      "iter 775: loss 0.6951, time 874.10ms, mfu 1.24%\n",
      "step 780: train loss 0.7158, val loss 0.7727\n",
      "iter 780: loss 0.6895, time 7031.44ms, mfu 1.14%\n",
      "iter 785: loss 0.7595, time 944.58ms, mfu 1.22%\n",
      "step 790: train loss 0.7165, val loss 0.7704\n",
      "saving checkpoint to out-abc-char\n",
      "iter 790: loss 0.8268, time 7765.80ms, mfu 1.13%\n",
      "iter 795: loss 0.7136, time 864.37ms, mfu 1.23%\n",
      "step 800: train loss 0.7087, val loss 0.7572\n",
      "saving checkpoint to out-abc-char\n",
      "iter 800: loss 0.7538, time 7520.00ms, mfu 1.13%\n",
      "iter 805: loss 0.7436, time 957.20ms, mfu 1.21%\n",
      "step 810: train loss 0.7042, val loss 0.7551\n",
      "saving checkpoint to out-abc-char\n",
      "iter 810: loss 0.7051, time 7727.49ms, mfu 1.11%\n",
      "iter 815: loss 0.6764, time 782.64ms, mfu 1.24%\n",
      "step 820: train loss 0.7002, val loss 0.7454\n",
      "saving checkpoint to out-abc-char\n",
      "iter 820: loss 0.6864, time 7646.65ms, mfu 1.14%\n",
      "iter 825: loss 0.6688, time 938.75ms, mfu 1.22%\n",
      "step 830: train loss 0.6926, val loss 0.7381\n",
      "saving checkpoint to out-abc-char\n",
      "iter 830: loss 0.7980, time 7491.45ms, mfu 1.13%\n",
      "iter 835: loss 0.7530, time 865.84ms, mfu 1.23%\n",
      "step 840: train loss 0.6883, val loss 0.7386\n",
      "iter 840: loss 0.8214, time 7614.92ms, mfu 1.13%\n",
      "iter 845: loss 0.8600, time 858.15ms, mfu 1.23%\n",
      "step 850: train loss 0.6829, val loss 0.7308\n",
      "saving checkpoint to out-abc-char\n",
      "iter 850: loss 0.7095, time 7522.09ms, mfu 1.14%\n",
      "iter 855: loss 0.6872, time 831.08ms, mfu 1.25%\n",
      "step 860: train loss 0.6867, val loss 0.7395\n",
      "iter 860: loss 0.6583, time 7164.82ms, mfu 1.15%\n",
      "iter 865: loss 0.6348, time 813.72ms, mfu 1.26%\n",
      "step 870: train loss 0.6761, val loss 0.7399\n",
      "iter 870: loss 0.8270, time 8063.44ms, mfu 1.16%\n",
      "iter 875: loss 0.7299, time 879.15ms, mfu 1.25%\n",
      "step 880: train loss 0.6728, val loss 0.7198\n",
      "saving checkpoint to out-abc-char\n",
      "iter 880: loss 0.6955, time 8015.68ms, mfu 1.15%\n",
      "iter 885: loss 0.6089, time 816.81ms, mfu 1.26%\n",
      "step 890: train loss 0.6622, val loss 0.7111\n",
      "saving checkpoint to out-abc-char\n",
      "iter 890: loss 0.6203, time 8018.38ms, mfu 1.16%\n",
      "iter 895: loss 0.6271, time 970.46ms, mfu 1.24%\n",
      "step 900: train loss 0.6579, val loss 0.7091\n",
      "saving checkpoint to out-abc-char\n",
      "iter 900: loss 0.7382, time 7859.12ms, mfu 1.14%\n",
      "iter 905: loss 0.6387, time 828.90ms, mfu 1.25%\n",
      "step 910: train loss 0.6653, val loss 0.7213\n",
      "iter 910: loss 0.7048, time 7641.46ms, mfu 1.15%\n",
      "iter 915: loss 0.6154, time 890.23ms, mfu 1.24%\n",
      "step 920: train loss 0.6447, val loss 0.7031\n",
      "saving checkpoint to out-abc-char\n",
      "iter 920: loss 0.6724, time 8142.46ms, mfu 1.14%\n",
      "iter 925: loss 0.7457, time 912.94ms, mfu 1.23%\n",
      "step 930: train loss 0.6390, val loss 0.6983\n",
      "saving checkpoint to out-abc-char\n",
      "iter 930: loss 0.7038, time 6941.42ms, mfu 1.13%\n",
      "iter 935: loss 0.6574, time 911.50ms, mfu 1.22%\n",
      "step 940: train loss 0.6456, val loss 0.6948\n",
      "saving checkpoint to out-abc-char\n",
      "iter 940: loss 0.6186, time 6829.42ms, mfu 1.13%\n",
      "iter 945: loss 0.6613, time 803.84ms, mfu 1.25%\n",
      "step 950: train loss 0.6373, val loss 0.6926\n",
      "saving checkpoint to out-abc-char\n",
      "iter 950: loss 0.6541, time 6950.91ms, mfu 1.15%\n",
      "iter 955: loss 0.6469, time 822.79ms, mfu 1.26%\n",
      "step 960: train loss 0.6403, val loss 0.6852\n",
      "saving checkpoint to out-abc-char\n",
      "iter 960: loss 0.6504, time 6868.93ms, mfu 1.16%\n",
      "iter 965: loss 0.7106, time 787.74ms, mfu 1.28%\n",
      "step 970: train loss 0.6243, val loss 0.6866\n",
      "iter 970: loss 0.5402, time 8469.81ms, mfu 1.18%\n",
      "iter 975: loss 0.7552, time 1026.23ms, mfu 1.24%\n",
      "step 980: train loss 0.6320, val loss 0.6905\n",
      "iter 980: loss 0.6376, time 8342.65ms, mfu 1.14%\n",
      "iter 985: loss 0.7176, time 948.27ms, mfu 1.22%\n",
      "step 990: train loss 0.6233, val loss 0.6923\n",
      "iter 990: loss 0.5946, time 7666.73ms, mfu 1.12%\n",
      "iter 995: loss 0.6576, time 919.33ms, mfu 1.21%\n",
      "step 1000: train loss 0.6168, val loss 0.6809\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1000: loss 0.5503, time 7922.30ms, mfu 1.11%\n",
      "iter 1005: loss 0.7058, time 934.63ms, mfu 1.20%\n",
      "step 1010: train loss 0.6144, val loss 0.6706\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1010: loss 0.5904, time 8070.55ms, mfu 1.11%\n",
      "iter 1015: loss 0.7131, time 831.08ms, mfu 1.22%\n",
      "step 1020: train loss 0.6072, val loss 0.6765\n",
      "iter 1020: loss 0.6176, time 7122.72ms, mfu 1.12%\n",
      "iter 1025: loss 0.6627, time 892.96ms, mfu 1.22%\n",
      "step 1030: train loss 0.6108, val loss 0.6701\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1030: loss 0.5591, time 7872.04ms, mfu 1.12%\n",
      "iter 1035: loss 0.5602, time 833.99ms, mfu 1.23%\n",
      "step 1040: train loss 0.5981, val loss 0.6686\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1040: loss 0.7150, time 7720.67ms, mfu 1.13%\n",
      "iter 1045: loss 0.6190, time 827.91ms, mfu 1.24%\n",
      "step 1050: train loss 0.6002, val loss 0.6620\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1050: loss 0.5577, time 7758.19ms, mfu 1.14%\n",
      "iter 1055: loss 0.6298, time 888.96ms, mfu 1.24%\n",
      "step 1060: train loss 0.5957, val loss 0.6576\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1060: loss 0.6484, time 7674.75ms, mfu 1.14%\n",
      "iter 1065: loss 0.5557, time 924.32ms, mfu 1.23%\n",
      "step 1070: train loss 0.5910, val loss 0.6570\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1070: loss 0.5010, time 8065.38ms, mfu 1.13%\n",
      "iter 1075: loss 0.6321, time 847.91ms, mfu 1.23%\n",
      "step 1080: train loss 0.5870, val loss 0.6530\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1080: loss 0.6927, time 7391.19ms, mfu 1.14%\n",
      "iter 1085: loss 0.7037, time 858.80ms, mfu 1.24%\n",
      "step 1090: train loss 0.5859, val loss 0.6545\n",
      "iter 1090: loss 0.6175, time 7244.38ms, mfu 1.14%\n",
      "iter 1095: loss 0.5325, time 897.01ms, mfu 1.23%\n",
      "step 1100: train loss 0.5783, val loss 0.6478\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1100: loss 0.5314, time 7764.62ms, mfu 1.13%\n",
      "iter 1105: loss 0.5157, time 903.87ms, mfu 1.23%\n",
      "step 1110: train loss 0.5800, val loss 0.6512\n",
      "iter 1110: loss 0.6073, time 8066.34ms, mfu 1.13%\n",
      "iter 1115: loss 0.5875, time 950.21ms, mfu 1.21%\n",
      "step 1120: train loss 0.5758, val loss 0.6442\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1120: loss 0.6205, time 8057.79ms, mfu 1.11%\n",
      "iter 1125: loss 0.5055, time 954.56ms, mfu 1.20%\n",
      "step 1130: train loss 0.5699, val loss 0.6418\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1130: loss 0.6442, time 8666.52ms, mfu 1.10%\n",
      "iter 1135: loss 0.6263, time 909.43ms, mfu 1.19%\n",
      "step 1140: train loss 0.5723, val loss 0.6476\n",
      "iter 1140: loss 0.5744, time 7663.46ms, mfu 1.10%\n",
      "iter 1145: loss 0.4839, time 850.58ms, mfu 1.21%\n",
      "step 1150: train loss 0.5677, val loss 0.6358\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1150: loss 0.4980, time 7775.21ms, mfu 1.11%\n",
      "iter 1155: loss 0.5183, time 770.11ms, mfu 1.24%\n",
      "step 1160: train loss 0.5605, val loss 0.6349\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1160: loss 0.6942, time 6877.36ms, mfu 1.14%\n",
      "iter 1165: loss 0.5750, time 823.58ms, mfu 1.26%\n",
      "step 1170: train loss 0.5638, val loss 0.6344\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1170: loss 0.5517, time 7079.13ms, mfu 1.16%\n",
      "iter 1175: loss 0.6494, time 798.56ms, mfu 1.27%\n",
      "step 1180: train loss 0.5608, val loss 0.6298\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1180: loss 0.5393, time 7061.45ms, mfu 1.17%\n",
      "iter 1185: loss 0.6799, time 843.25ms, mfu 1.28%\n",
      "step 1190: train loss 0.5612, val loss 0.6319\n",
      "iter 1190: loss 0.5306, time 6847.41ms, mfu 1.18%\n",
      "iter 1195: loss 0.4981, time 879.22ms, mfu 1.27%\n",
      "step 1200: train loss 0.5510, val loss 0.6201\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1200: loss 0.5675, time 8061.02ms, mfu 1.17%\n",
      "iter 1205: loss 0.5770, time 832.32ms, mfu 1.27%\n",
      "step 1210: train loss 0.5452, val loss 0.6281\n",
      "iter 1210: loss 0.6350, time 7762.00ms, mfu 1.17%\n",
      "iter 1215: loss 0.4867, time 927.69ms, mfu 1.25%\n",
      "step 1220: train loss 0.5396, val loss 0.6267\n",
      "iter 1220: loss 0.5014, time 7872.18ms, mfu 1.15%\n",
      "iter 1225: loss 0.6633, time 926.12ms, mfu 1.24%\n",
      "step 1230: train loss 0.5415, val loss 0.6205\n",
      "iter 1230: loss 0.5436, time 7545.42ms, mfu 1.14%\n",
      "iter 1235: loss 0.6588, time 871.84ms, mfu 1.24%\n",
      "step 1240: train loss 0.5421, val loss 0.6198\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1240: loss 0.5034, time 7356.00ms, mfu 1.14%\n",
      "iter 1245: loss 0.5592, time 865.16ms, mfu 1.24%\n",
      "step 1250: train loss 0.5399, val loss 0.6227\n",
      "iter 1250: loss 0.5958, time 7476.09ms, mfu 1.14%\n",
      "iter 1255: loss 0.5439, time 864.52ms, mfu 1.24%\n",
      "step 1260: train loss 0.5313, val loss 0.6130\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1260: loss 0.4878, time 7478.36ms, mfu 1.14%\n",
      "iter 1265: loss 0.5776, time 812.80ms, mfu 1.26%\n",
      "step 1270: train loss 0.5271, val loss 0.6156\n",
      "iter 1270: loss 0.5550, time 7053.00ms, mfu 1.16%\n",
      "iter 1275: loss 0.4468, time 991.22ms, mfu 1.23%\n",
      "step 1280: train loss 0.5288, val loss 0.6146\n",
      "iter 1280: loss 0.6313, time 7263.51ms, mfu 1.13%\n",
      "iter 1285: loss 0.6339, time 844.07ms, mfu 1.24%\n",
      "step 1290: train loss 0.5272, val loss 0.6094\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1290: loss 0.6460, time 7294.19ms, mfu 1.14%\n",
      "iter 1295: loss 0.5315, time 906.20ms, mfu 1.23%\n",
      "step 1300: train loss 0.5285, val loss 0.6125\n",
      "iter 1300: loss 0.4734, time 7433.09ms, mfu 1.13%\n",
      "iter 1305: loss 0.5578, time 908.70ms, mfu 1.23%\n",
      "step 1310: train loss 0.5151, val loss 0.6092\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1310: loss 0.5366, time 7824.92ms, mfu 1.13%\n",
      "iter 1315: loss 0.5497, time 868.59ms, mfu 1.23%\n",
      "step 1320: train loss 0.5158, val loss 0.6067\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1320: loss 0.5276, time 7635.94ms, mfu 1.13%\n",
      "iter 1325: loss 0.5692, time 903.38ms, mfu 1.22%\n",
      "step 1330: train loss 0.5126, val loss 0.6017\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1330: loss 0.4961, time 6345.52ms, mfu 1.13%\n",
      "iter 1335: loss 0.4431, time 957.93ms, mfu 1.21%\n",
      "step 1340: train loss 0.5076, val loss 0.5946\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1340: loss 0.5163, time 7747.71ms, mfu 1.11%\n",
      "iter 1345: loss 0.5866, time 865.38ms, mfu 1.22%\n",
      "step 1350: train loss 0.5062, val loss 0.6042\n",
      "iter 1350: loss 0.5026, time 7767.80ms, mfu 1.12%\n",
      "iter 1355: loss 0.5971, time 908.82ms, mfu 1.21%\n",
      "step 1360: train loss 0.5055, val loss 0.5944\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1360: loss 0.7081, time 7776.22ms, mfu 1.12%\n",
      "iter 1365: loss 0.5003, time 929.25ms, mfu 1.20%\n",
      "step 1370: train loss 0.5042, val loss 0.5906\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1370: loss 0.5364, time 7542.19ms, mfu 1.11%\n",
      "iter 1375: loss 0.5282, time 813.27ms, mfu 1.23%\n",
      "step 1380: train loss 0.4959, val loss 0.5911\n",
      "iter 1380: loss 0.4813, time 7498.61ms, mfu 1.13%\n",
      "iter 1385: loss 0.6088, time 864.66ms, mfu 1.23%\n",
      "step 1390: train loss 0.4925, val loss 0.5904\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1390: loss 0.6005, time 6908.99ms, mfu 1.13%\n",
      "iter 1395: loss 0.4959, time 765.23ms, mfu 1.26%\n",
      "step 1400: train loss 0.4920, val loss 0.5951\n",
      "iter 1400: loss 0.5278, time 6824.84ms, mfu 1.17%\n",
      "iter 1405: loss 0.5094, time 769.86ms, mfu 1.29%\n",
      "step 1410: train loss 0.4856, val loss 0.5903\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1410: loss 0.5170, time 7135.70ms, mfu 1.19%\n",
      "iter 1415: loss 0.5678, time 827.99ms, mfu 1.29%\n",
      "step 1420: train loss 0.4809, val loss 0.5776\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1420: loss 0.4952, time 6795.41ms, mfu 1.19%\n",
      "iter 1425: loss 0.4955, time 811.05ms, mfu 1.30%\n",
      "step 1430: train loss 0.4787, val loss 0.5847\n",
      "iter 1430: loss 0.4920, time 6938.11ms, mfu 1.20%\n",
      "iter 1435: loss 0.4257, time 809.90ms, mfu 1.31%\n",
      "step 1440: train loss 0.4836, val loss 0.5838\n",
      "iter 1440: loss 0.5503, time 7257.63ms, mfu 1.20%\n",
      "iter 1445: loss 0.4973, time 850.23ms, mfu 1.30%\n",
      "step 1450: train loss 0.4693, val loss 0.5733\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1450: loss 0.4678, time 7699.05ms, mfu 1.20%\n",
      "iter 1455: loss 0.4859, time 938.44ms, mfu 1.27%\n",
      "step 1460: train loss 0.4789, val loss 0.5896\n",
      "iter 1460: loss 0.5003, time 7146.65ms, mfu 1.17%\n",
      "iter 1465: loss 0.4572, time 878.09ms, mfu 1.27%\n",
      "step 1470: train loss 0.4660, val loss 0.5706\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1470: loss 0.5228, time 7381.19ms, mfu 1.17%\n",
      "iter 1475: loss 0.4716, time 848.26ms, mfu 1.27%\n",
      "step 1480: train loss 0.4719, val loss 0.5813\n",
      "iter 1480: loss 0.5100, time 7189.12ms, mfu 1.17%\n",
      "iter 1485: loss 0.5119, time 858.36ms, mfu 1.27%\n",
      "step 1490: train loss 0.4650, val loss 0.5692\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1490: loss 0.6496, time 7483.86ms, mfu 1.17%\n",
      "iter 1495: loss 0.5667, time 835.93ms, mfu 1.27%\n",
      "step 1500: train loss 0.4567, val loss 0.5649\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1500: loss 0.5313, time 7491.61ms, mfu 1.17%\n",
      "iter 1505: loss 0.4690, time 831.83ms, mfu 1.28%\n",
      "step 1510: train loss 0.4509, val loss 0.5662\n",
      "iter 1510: loss 0.5738, time 7211.36ms, mfu 1.17%\n",
      "iter 1515: loss 0.5682, time 881.62ms, mfu 1.27%\n",
      "step 1520: train loss 0.4485, val loss 0.5685\n",
      "iter 1520: loss 0.4241, time 7304.00ms, mfu 1.17%\n",
      "iter 1525: loss 0.5278, time 836.57ms, mfu 1.27%\n",
      "step 1530: train loss 0.4526, val loss 0.5611\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1530: loss 0.4995, time 7643.14ms, mfu 1.17%\n",
      "iter 1535: loss 0.4786, time 878.54ms, mfu 1.26%\n",
      "step 1540: train loss 0.4439, val loss 0.5623\n",
      "iter 1540: loss 0.4351, time 7378.71ms, mfu 1.16%\n",
      "iter 1545: loss 0.4913, time 843.94ms, mfu 1.27%\n",
      "step 1550: train loss 0.4489, val loss 0.5648\n",
      "iter 1550: loss 0.4614, time 7465.28ms, mfu 1.17%\n",
      "iter 1555: loss 0.4654, time 874.07ms, mfu 1.26%\n",
      "step 1560: train loss 0.4412, val loss 0.5614\n",
      "iter 1560: loss 0.4670, time 7359.22ms, mfu 1.16%\n",
      "iter 1565: loss 0.4047, time 897.27ms, mfu 1.25%\n",
      "step 1570: train loss 0.4365, val loss 0.5646\n",
      "iter 1570: loss 0.4425, time 7366.74ms, mfu 1.15%\n",
      "iter 1575: loss 0.4506, time 839.85ms, mfu 1.26%\n",
      "step 1580: train loss 0.4359, val loss 0.5524\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1580: loss 0.4783, time 7652.87ms, mfu 1.16%\n",
      "iter 1585: loss 0.3897, time 898.01ms, mfu 1.25%\n",
      "step 1590: train loss 0.4303, val loss 0.5535\n",
      "iter 1590: loss 0.4700, time 7232.24ms, mfu 1.15%\n",
      "iter 1595: loss 0.4908, time 906.53ms, mfu 1.24%\n",
      "step 1600: train loss 0.4253, val loss 0.5528\n",
      "iter 1600: loss 0.3883, time 7363.56ms, mfu 1.14%\n",
      "iter 1605: loss 0.5610, time 872.16ms, mfu 1.24%\n",
      "step 1610: train loss 0.4248, val loss 0.5408\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1610: loss 0.3548, time 7519.67ms, mfu 1.14%\n",
      "iter 1615: loss 0.4321, time 824.84ms, mfu 1.25%\n",
      "step 1620: train loss 0.4149, val loss 0.5532\n",
      "iter 1620: loss 0.4091, time 7063.93ms, mfu 1.15%\n",
      "iter 1625: loss 0.3925, time 957.33ms, mfu 1.23%\n",
      "step 1630: train loss 0.4185, val loss 0.5564\n",
      "iter 1630: loss 0.3551, time 6792.81ms, mfu 1.14%\n",
      "iter 1635: loss 0.4738, time 811.65ms, mfu 1.25%\n",
      "step 1640: train loss 0.4114, val loss 0.5326\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1640: loss 0.5500, time 7145.28ms, mfu 1.15%\n",
      "iter 1645: loss 0.4519, time 765.94ms, mfu 1.28%\n",
      "step 1650: train loss 0.4105, val loss 0.5357\n",
      "iter 1650: loss 0.3826, time 6923.40ms, mfu 1.18%\n",
      "iter 1655: loss 0.5445, time 775.23ms, mfu 1.30%\n",
      "step 1660: train loss 0.4099, val loss 0.5404\n",
      "iter 1660: loss 0.4287, time 6919.96ms, mfu 1.20%\n",
      "iter 1665: loss 0.4386, time 813.28ms, mfu 1.31%\n",
      "step 1670: train loss 0.4081, val loss 0.5411\n",
      "iter 1670: loss 0.4162, time 6683.69ms, mfu 1.20%\n",
      "iter 1675: loss 0.3834, time 844.87ms, mfu 1.30%\n",
      "step 1680: train loss 0.3995, val loss 0.5408\n",
      "iter 1680: loss 0.3144, time 6861.92ms, mfu 1.20%\n",
      "iter 1685: loss 0.4425, time 830.40ms, mfu 1.30%\n",
      "step 1690: train loss 0.4013, val loss 0.5383\n",
      "iter 1690: loss 0.4225, time 7012.94ms, mfu 1.20%\n",
      "iter 1695: loss 0.3883, time 798.79ms, mfu 1.31%\n",
      "step 1700: train loss 0.3935, val loss 0.5337\n",
      "iter 1700: loss 0.4678, time 6848.64ms, mfu 1.21%\n",
      "iter 1705: loss 0.3913, time 829.02ms, mfu 1.31%\n",
      "step 1710: train loss 0.3922, val loss 0.5299\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1710: loss 0.4564, time 7098.45ms, mfu 1.21%\n",
      "iter 1715: loss 0.4431, time 803.47ms, mfu 1.32%\n",
      "step 1720: train loss 0.3884, val loss 0.5306\n",
      "iter 1720: loss 0.4241, time 6885.63ms, mfu 1.21%\n",
      "iter 1725: loss 0.4607, time 807.89ms, mfu 1.32%\n",
      "step 1730: train loss 0.3844, val loss 0.5195\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1730: loss 0.4145, time 7269.35ms, mfu 1.22%\n",
      "iter 1735: loss 0.3964, time 855.49ms, mfu 1.31%\n",
      "step 1740: train loss 0.3851, val loss 0.5245\n",
      "iter 1740: loss 0.3964, time 6931.65ms, mfu 1.21%\n",
      "iter 1745: loss 0.4042, time 844.99ms, mfu 1.31%\n",
      "step 1750: train loss 0.3829, val loss 0.5264\n",
      "iter 1750: loss 0.3623, time 7053.65ms, mfu 1.20%\n",
      "iter 1755: loss 0.4534, time 883.64ms, mfu 1.29%\n",
      "step 1760: train loss 0.3798, val loss 0.5239\n",
      "iter 1760: loss 0.4073, time 7067.81ms, mfu 1.19%\n",
      "iter 1765: loss 0.3774, time 867.30ms, mfu 1.29%\n",
      "step 1770: train loss 0.3759, val loss 0.5214\n",
      "iter 1770: loss 0.4263, time 7177.72ms, mfu 1.18%\n",
      "iter 1775: loss 0.4060, time 840.26ms, mfu 1.29%\n",
      "step 1780: train loss 0.3762, val loss 0.5180\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1780: loss 0.3749, time 7267.18ms, mfu 1.18%\n",
      "iter 1785: loss 0.4581, time 870.58ms, mfu 1.28%\n",
      "step 1790: train loss 0.3653, val loss 0.5131\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1790: loss 0.3943, time 7667.37ms, mfu 1.17%\n",
      "iter 1795: loss 0.4116, time 844.13ms, mfu 1.28%\n",
      "step 1800: train loss 0.3649, val loss 0.5184\n",
      "iter 1800: loss 0.4022, time 7036.71ms, mfu 1.18%\n",
      "iter 1805: loss 0.4135, time 811.97ms, mfu 1.29%\n",
      "step 1810: train loss 0.3607, val loss 0.5192\n",
      "iter 1810: loss 0.4134, time 6942.03ms, mfu 1.19%\n",
      "iter 1815: loss 0.4392, time 891.11ms, mfu 1.28%\n",
      "step 1820: train loss 0.3642, val loss 0.5180\n",
      "iter 1820: loss 0.3269, time 7374.76ms, mfu 1.17%\n",
      "iter 1825: loss 0.4250, time 878.78ms, mfu 1.27%\n",
      "step 1830: train loss 0.3565, val loss 0.5007\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1830: loss 0.4981, time 7259.23ms, mfu 1.17%\n",
      "iter 1835: loss 0.3754, time 919.75ms, mfu 1.25%\n",
      "step 1840: train loss 0.3547, val loss 0.5037\n",
      "iter 1840: loss 0.3818, time 7387.04ms, mfu 1.15%\n",
      "iter 1845: loss 0.3788, time 848.35ms, mfu 1.26%\n",
      "step 1850: train loss 0.3481, val loss 0.5071\n",
      "iter 1850: loss 0.3748, time 7278.73ms, mfu 1.16%\n",
      "iter 1855: loss 0.4635, time 930.52ms, mfu 1.24%\n",
      "step 1860: train loss 0.3444, val loss 0.4932\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1860: loss 0.3562, time 7659.15ms, mfu 1.14%\n",
      "iter 1865: loss 0.3945, time 854.88ms, mfu 1.24%\n",
      "step 1870: train loss 0.3499, val loss 0.5020\n",
      "iter 1870: loss 0.3580, time 7507.82ms, mfu 1.14%\n",
      "iter 1875: loss 0.4160, time 856.20ms, mfu 1.25%\n",
      "step 1880: train loss 0.3392, val loss 0.4946\n",
      "iter 1880: loss 0.4066, time 7144.98ms, mfu 1.15%\n",
      "iter 1885: loss 0.4269, time 818.13ms, mfu 1.26%\n",
      "step 1890: train loss 0.3403, val loss 0.4954\n",
      "iter 1890: loss 0.4841, time 6918.77ms, mfu 1.16%\n",
      "iter 1895: loss 0.3904, time 868.60ms, mfu 1.26%\n",
      "step 1900: train loss 0.3408, val loss 0.5001\n",
      "iter 1900: loss 0.3879, time 7098.26ms, mfu 1.16%\n",
      "iter 1905: loss 0.3116, time 870.29ms, mfu 1.26%\n",
      "step 1910: train loss 0.3298, val loss 0.4972\n",
      "iter 1910: loss 0.3339, time 7077.50ms, mfu 1.16%\n",
      "iter 1915: loss 0.3564, time 849.65ms, mfu 1.26%\n",
      "step 1920: train loss 0.3296, val loss 0.4889\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1920: loss 0.3555, time 7262.01ms, mfu 1.16%\n",
      "iter 1925: loss 0.3891, time 866.59ms, mfu 1.26%\n",
      "step 1930: train loss 0.3300, val loss 0.4893\n",
      "iter 1930: loss 0.3925, time 7149.10ms, mfu 1.16%\n",
      "iter 1935: loss 0.3608, time 893.57ms, mfu 1.25%\n",
      "step 1940: train loss 0.3286, val loss 0.4989\n",
      "iter 1940: loss 0.3906, time 7274.82ms, mfu 1.15%\n",
      "iter 1945: loss 0.3857, time 832.74ms, mfu 1.26%\n",
      "step 1950: train loss 0.3246, val loss 0.4818\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1950: loss 0.3910, time 7231.56ms, mfu 1.16%\n",
      "iter 1955: loss 0.3613, time 863.64ms, mfu 1.26%\n",
      "step 1960: train loss 0.3221, val loss 0.4827\n",
      "iter 1960: loss 0.3653, time 7205.86ms, mfu 1.16%\n",
      "iter 1965: loss 0.3147, time 897.66ms, mfu 1.25%\n",
      "step 1970: train loss 0.3213, val loss 0.4788\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1970: loss 0.4169, time 7531.92ms, mfu 1.15%\n",
      "iter 1975: loss 0.4159, time 904.59ms, mfu 1.24%\n",
      "step 1980: train loss 0.3174, val loss 0.4769\n",
      "saving checkpoint to out-abc-char\n",
      "iter 1980: loss 0.3890, time 7192.43ms, mfu 1.14%\n",
      "iter 1985: loss 0.4412, time 824.23ms, mfu 1.25%\n",
      "step 1990: train loss 0.3147, val loss 0.4853\n",
      "iter 1990: loss 0.4533, time 7093.91ms, mfu 1.16%\n",
      "iter 1995: loss 0.3227, time 849.85ms, mfu 1.26%\n",
      "step 2000: train loss 0.3153, val loss 0.4887\n",
      "iter 2000: loss 0.3835, time 7239.49ms, mfu 1.16%\n",
      "iter 2005: loss 0.3470, time 967.34ms, mfu 1.23%\n",
      "step 2010: train loss 0.3058, val loss 0.4795\n",
      "iter 2010: loss 0.3481, time 6969.02ms, mfu 1.14%\n",
      "iter 2015: loss 0.3730, time 862.99ms, mfu 1.24%\n",
      "step 2020: train loss 0.3052, val loss 0.4741\n",
      "saving checkpoint to out-abc-char\n",
      "iter 2020: loss 0.4204, time 7288.56ms, mfu 1.14%\n",
      "iter 2025: loss 0.3549, time 783.84ms, mfu 1.26%\n",
      "step 2030: train loss 0.3009, val loss 0.4762\n",
      "iter 2030: loss 0.4249, time 6834.66ms, mfu 1.17%\n",
      "iter 2035: loss 0.3403, time 826.33ms, mfu 1.27%\n",
      "step 2040: train loss 0.3024, val loss 0.4642\n",
      "saving checkpoint to out-abc-char\n",
      "iter 2040: loss 0.2752, time 7204.77ms, mfu 1.17%\n",
      "iter 2045: loss 0.3397, time 838.96ms, mfu 1.28%\n",
      "step 2050: train loss 0.2966, val loss 0.4720\n",
      "iter 2050: loss 0.3169, time 6859.92ms, mfu 1.18%\n",
      "iter 2055: loss 0.3477, time 883.55ms, mfu 1.27%\n",
      "step 2060: train loss 0.2973, val loss 0.4736\n",
      "iter 2060: loss 0.2807, time 7029.42ms, mfu 1.17%\n",
      "iter 2065: loss 0.3358, time 835.77ms, mfu 1.27%\n",
      "step 2070: train loss 0.2925, val loss 0.4731\n",
      "iter 2070: loss 0.3507, time 6823.79ms, mfu 1.17%\n",
      "iter 2075: loss 0.3616, time 827.07ms, mfu 1.28%\n",
      "step 2080: train loss 0.2875, val loss 0.4511\n",
      "saving checkpoint to out-abc-char\n",
      "iter 2080: loss 0.3312, time 7159.76ms, mfu 1.18%\n",
      "iter 2085: loss 0.3404, time 829.96ms, mfu 1.29%\n",
      "step 2090: train loss 0.2877, val loss 0.4625\n",
      "iter 2090: loss 0.3833, time 6965.66ms, mfu 1.18%\n",
      "iter 2095: loss 0.3695, time 825.74ms, mfu 1.29%\n",
      "step 2100: train loss 0.2868, val loss 0.4639\n",
      "iter 2100: loss 0.3793, time 6910.68ms, mfu 1.19%\n",
      "iter 2105: loss 0.2988, time 828.35ms, mfu 1.29%\n",
      "step 2110: train loss 0.2820, val loss 0.4707\n",
      "iter 2110: loss 0.2954, time 7019.35ms, mfu 1.19%\n",
      "iter 2115: loss 0.3571, time 829.77ms, mfu 1.30%\n",
      "step 2120: train loss 0.2821, val loss 0.4447\n",
      "saving checkpoint to out-abc-char\n",
      "iter 2120: loss 0.3155, time 7099.08ms, mfu 1.19%\n",
      "iter 2125: loss 0.2727, time 826.77ms, mfu 1.30%\n",
      "step 2130: train loss 0.2812, val loss 0.4574\n",
      "iter 2130: loss 0.3617, time 7069.89ms, mfu 1.20%\n",
      "iter 2135: loss 0.2970, time 834.73ms, mfu 1.30%\n",
      "step 2140: train loss 0.2779, val loss 0.4685\n",
      "iter 2140: loss 0.3588, time 7014.01ms, mfu 1.20%\n",
      "iter 2145: loss 0.3454, time 819.45ms, mfu 1.30%\n",
      "step 2150: train loss 0.2758, val loss 0.4535\n",
      "iter 2150: loss 0.3245, time 6851.08ms, mfu 1.20%\n",
      "iter 2155: loss 0.2714, time 828.68ms, mfu 1.30%\n",
      "step 2160: train loss 0.2683, val loss 0.4490\n",
      "iter 2160: loss 0.3267, time 7039.95ms, mfu 1.20%\n",
      "iter 2165: loss 0.3425, time 821.35ms, mfu 1.31%\n",
      "step 2170: train loss 0.2747, val loss 0.4513\n",
      "iter 2170: loss 0.2828, time 6950.40ms, mfu 1.20%\n",
      "iter 2175: loss 0.2904, time 866.47ms, mfu 1.30%\n",
      "step 2180: train loss 0.2640, val loss 0.4368\n",
      "saving checkpoint to out-abc-char\n",
      "iter 2180: loss 0.2796, time 7203.10ms, mfu 1.19%\n",
      "iter 2185: loss 0.3275, time 871.58ms, mfu 1.29%\n",
      "step 2190: train loss 0.2683, val loss 0.4526\n",
      "iter 2190: loss 0.2921, time 7151.04ms, mfu 1.18%\n",
      "iter 2195: loss 0.3287, time 961.64ms, mfu 1.26%\n",
      "step 2200: train loss 0.2606, val loss 0.4456\n",
      "iter 2200: loss 0.3459, time 6932.83ms, mfu 1.16%\n",
      "iter 2205: loss 0.3071, time 837.89ms, mfu 1.27%\n",
      "step 2210: train loss 0.2607, val loss 0.4500\n",
      "iter 2210: loss 0.3418, time 7084.48ms, mfu 1.17%\n",
      "iter 2215: loss 0.3578, time 845.98ms, mfu 1.27%\n",
      "step 2220: train loss 0.2609, val loss 0.4442\n",
      "iter 2220: loss 0.3156, time 6973.34ms, mfu 1.17%\n",
      "iter 2225: loss 0.3757, time 842.05ms, mfu 1.27%\n",
      "step 2230: train loss 0.2586, val loss 0.4396\n",
      "iter 2230: loss 0.3088, time 7060.19ms, mfu 1.17%\n",
      "iter 2235: loss 0.2744, time 834.77ms, mfu 1.28%\n",
      "step 2240: train loss 0.2587, val loss 0.4436\n",
      "iter 2240: loss 0.2884, time 7005.33ms, mfu 1.18%\n",
      "iter 2245: loss 0.2648, time 916.28ms, mfu 1.26%\n",
      "step 2250: train loss 0.2559, val loss 0.4438\n",
      "iter 2250: loss 0.2414, time 7072.52ms, mfu 1.16%\n",
      "iter 2255: loss 0.3050, time 927.95ms, mfu 1.25%\n",
      "step 2260: train loss 0.2499, val loss 0.4335\n",
      "saving checkpoint to out-abc-char\n",
      "iter 2260: loss 0.2179, time 7307.81ms, mfu 1.15%\n",
      "iter 2265: loss 0.3422, time 819.82ms, mfu 1.26%\n",
      "step 2270: train loss 0.2483, val loss 0.4383\n",
      "iter 2270: loss 0.3962, time 7017.48ms, mfu 1.16%\n",
      "iter 2275: loss 0.3137, time 844.34ms, mfu 1.26%\n",
      "step 2280: train loss 0.2478, val loss 0.4349\n",
      "iter 2280: loss 0.3359, time 6881.54ms, mfu 1.16%\n",
      "iter 2285: loss 0.3069, time 860.96ms, mfu 1.26%\n",
      "step 2290: train loss 0.2447, val loss 0.4318\n",
      "saving checkpoint to out-abc-char\n",
      "iter 2290: loss 0.2164, time 7438.42ms, mfu 1.16%\n",
      "iter 2295: loss 0.3007, time 822.89ms, mfu 1.27%\n",
      "step 2300: train loss 0.2455, val loss 0.4346\n",
      "iter 2300: loss 0.3041, time 7245.70ms, mfu 1.17%\n",
      "iter 2305: loss 0.2904, time 862.24ms, mfu 1.27%\n",
      "step 2310: train loss 0.2411, val loss 0.4299\n",
      "saving checkpoint to out-abc-char\n",
      "iter 2310: loss 0.3032, time 7347.82ms, mfu 1.17%\n",
      "iter 2315: loss 0.3446, time 938.44ms, mfu 1.25%\n",
      "step 2320: train loss 0.2392, val loss 0.4248\n",
      "saving checkpoint to out-abc-char\n",
      "iter 2320: loss 0.2381, time 7738.79ms, mfu 1.15%\n",
      "iter 2325: loss 0.2734, time 932.98ms, mfu 1.23%\n",
      "step 2330: train loss 0.2360, val loss 0.4170\n",
      "saving checkpoint to out-abc-char\n",
      "iter 2330: loss 0.2659, time 7662.25ms, mfu 1.13%\n",
      "iter 2335: loss 0.2162, time 879.03ms, mfu 1.23%\n",
      "step 2340: train loss 0.2357, val loss 0.4315\n",
      "iter 2340: loss 0.2662, time 7099.16ms, mfu 1.14%\n",
      "iter 2345: loss 0.3241, time 829.10ms, mfu 1.25%\n",
      "^C\n",
      "Process ForkProcess-2:\n",
      "Process ForkProcess-7:\n",
      "Process ForkProcess-1:\n",
      "Process ForkProcess-12:\n",
      "Process ForkProcess-8:\n",
      "Process ForkProcess-16:\n",
      "Process ForkProcess-10:\n",
      "Process ForkProcess-18:\n",
      "Process ForkProcess-13:\n",
      "Process ForkProcess-6:\n",
      "Process ForkProcess-19:\n",
      "Process ForkProcess-3:\n",
      "Process ForkProcess-9:\n",
      "Process ForkProcess-14:\n",
      "Process ForkProcess-15:\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 253, in <module>\n",
      "    losses = estimate_loss()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"train.py\", line 213, in estimate_loss\n",
      "    logits, loss = model(X, Y)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/_dynamo/eval_frame.py\", line 82, in forward\n",
      "    return self.dynamo_ctx(self._orig_mod.forward)(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/_dynamo/eval_frame.py\", line 209, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/pt-env/notebooks/nanoGPT/model.py\", line 188, in forward\n",
      "    x = block(x)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/pt-env/notebooks/nanoGPT/model.py\", line 111, in forward\n",
      "    x = x + self.attn(self.ln_1(x))\n",
      "  File \"/pt-env/notebooks/nanoGPT/model.py\", line 111, in <graph break in forward>\n",
      "    x = x + self.attn(self.ln_1(x))\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/_dynamo/eval_frame.py\", line 209, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py\", line 2819, in forward\n",
      "    return compiled_fn(full_args)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py\", line 1222, in g\n",
      "    return f(*args)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py\", line 1898, in runtime_wrapper\n",
      "    all_outs = call_func_with_args(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/_functorch/aot_autograd.py\", line 1247, in call_func_with_args\n",
      "    out = normalize_as_list(f(args))\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/_inductor/compile_fx.py\", line 248, in run\n",
      "    return model(new_inputs)\n",
      "  File \"/tmp/torchinductor_root/pa/cpa3vr2yxnbofrjwenft4huyqq6b2bhdtieg6max2ovevco4z3de.py\", line 207, in call\n",
      "    triton__3.run(arg3_1, arg4_1, buf8, buf9, 393216, grid=grid(393216), stream=stream0)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/_inductor/triton_ops/autotune.py\", line 190, in run\n",
      "    result = launcher(\n",
      "  File \"<string>\", line 6, in launcher\n",
      "KeyboardInterrupt\n",
      "Process ForkProcess-17:\n",
      "Process ForkProcess-4:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/concurrent/futures/process.py\", line 233, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 96, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/concurrent/futures/process.py\", line 233, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.8/concurrent/futures/process.py\", line 233, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 96, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 96, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/concurrent/futures/process.py\", line 233, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 96, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/concurrent/futures/process.py\", line 233, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 96, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/concurrent/futures/process.py\", line 233, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 96, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/concurrent/futures/process.py\", line 233, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 96, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/concurrent/futures/process.py\", line 233, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 96, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/concurrent/futures/process.py\", line 233, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 96, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Process ForkProcess-11:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/concurrent/futures/process.py\", line 233, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 96, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/concurrent/futures/process.py\", line 233, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 96, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/concurrent/futures/process.py\", line 233, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 96, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/concurrent/futures/process.py\", line 233, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 96, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/concurrent/futures/process.py\", line 233, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 97, in get\n",
      "    res = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Process ForkProcess-20:\n",
      "Process ForkProcess-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/concurrent/futures/process.py\", line 233, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 96, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/concurrent/futures/process.py\", line 233, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 96, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/concurrent/futures/process.py\", line 233, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.8/concurrent/futures/process.py\", line 233, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 96, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 96, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/concurrent/futures/process.py\", line 233, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 96, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/concurrent/futures/process.py\", line 233, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 96, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[31m(failed 255).\u001b[0m Press Control-C to abort syncing.\n"
     ]
    }
   ],
   "source": [
    "!python3 train.py config/train_abc_char.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test key with most occurrences: G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding: out_dir = out-abc-char\n",
      "Overriding: start = M:4/4L:1/4K:G|\"G\"|\"C\"|\"D\"|\"D\"|\"G\"|\"C\"|\"D\"|\"G\"|]\n",
      "number of parameters: 21.26M\n",
      "abc_char\n",
      "Loading meta from data/abc_char/meta.pkl...\n",
      "M:4/4L:1/4K:G|\"G\"|\"C\"|\"D\"|\"D\"|\"G\"|\"C\"|\"D\"|\"G\"|]\n",
      "B/2c/2|\"G\"d3/2e/2d/2c/2|B/2G/2D/2G/2|\"C\"cde|\"D\"d3/2e/2f/2e/2|\"D\"d/2B/2A/2G/2|\"G\"G/2B/2d3/2e/2|\"D\"d3/2e/2f/2e/2|\"G\"B/2G/2D3/2G/2|\"C\"cde|\"D\"dc\"G\"c|B/2c/2|\"G\"d3/2g/2g3/2f/2|\"C\"e/2c/2e/2g3/2e/2|\"D\"d/2B/2A/2G/2F/2G/2A/2|\"D\"c/2A/2d/2f/2a3/2b/2|\"C\"a/2g/2f/2e/2\"G\"d/2B/2G/2B/2|\"C\"c/2e/2d/2e/2f/2g3/2B/2|\"D\"A/2G/2A/2B/2\"G\"G2|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:Gm\n",
      "|\"Gm\"|\"Dm\"|\"Dm\"|\"Gm\"|\"Gm\"\"Dm\"|\"Gm\"|\"Gm\"|\"Dm\"|\"Dm\"|\"Gm\"|\"Dm\"|\"Gm\"|\"Gm\"\"C7\"|\"F\"|\"F\"|\"B\"\"F\"|\"F\"\"Dm\"|\"Gm\"|]\n",
      "D|\"Gm\"GABBAG|\"Dm\"Adddef|\"Dm\"dcAAdd|\"Gm\"GABBAG|\"Gm\"dcd\"Dm\"d2\n",
      "---------------\n",
      "M:4/4L:1/4K:G|\"G\"|\"C\"|\"D\"|\"D\"|\"G\"|\"C\"|\"D\"|\"G\"|]\n",
      "B/2c/2|\"G\"d3/2d/2e/2d/2|\"C\"e/2d/2c/2B/2\"D\"A/2B/2|\"G\"B/2c/2d/2e/2d/2|\"C\"e/2d/2c/2B/2c/2G/2|\"D\"AD/2D/2F/2A/2|\"D\"A/2G/2F/2E/2D/2E/2F/2|\"G\"GGG|d/2c/2|\"G\"B/2d/2g/2d/2B/2d/2g/2d/2|\"C\"e/2g/2c/2a/2e/2g/2c/2g/2|\"D\"e/2g/2f/2e/2\"G\"d/2B/2G/2B/2|\"Am\"E/2A/2A/2B/2c/2B/2A/2c/2|\"D7\"d/2c/2d/2B/2\"G\"GB/2c/2|\"G\"d/2B/2c/2A/2B/2d/2g/2d/2|\"C\"e/2g/2c/2e/2\"G\"d/2B/2G/2B/2|\"D7\"A/2G/2F/2E/2D/2A/2G/2F/2|\"G\"GGG|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:A\n",
      "|\"A\"|\"A\"\"D\"|\"A\"|\"A\"|\"A\"\"D\"|\"A\"\"D\"|\"A\"|\"A\"|\"A\"\"E7\"|\"A\"|\"A\"\"E7\"|\"A\"|\"A\"|\"A\"\"D\"|\"A\"|\"A\"\"E7\"|\"A\"|]\n",
      "e\n",
      "---------------\n",
      "M:4/4L:1/4K:G|\"G\"|\"C\"|\"D\"|\"D\"|\"G\"|\"C\"|\"D\"|\"G\"|]\n",
      "B/2c/2|\"G\"d3/2B/2G/2G/2|B/2c/2d/2B/2G/2|\"C\"c3/2A/2G/2E/2|\"D\"F/2G/2A/2B/2c/2A/2F/2|\"D\"A/2B/2c/2d/2A/2F/2A/2|\"G\"B/2c/2d/2B/2G/2|\"C\"c/2B/2c/2d/2e/2d/2c/2|\"D\"B/2c/2d/2A/2B/2c/2A/2|\"G\"BGG|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:G\n",
      "|\"Am\"|\"C\"\"G\"|\"Am\"|\"Am\"|\"C\"\"G\"|\"Am\"|\"C\"|\"G\"|\"C\"\"G\"|\"Am\"|\"C\"\"G\"|\"Am\"|]\n",
      "e/2d/2|\"Am\"c/2AG/2c3/2A/2|\"C\"G/2c/2e/2g/2e/2d/2c/2|\"G\"B/2GF/2Gd/2e/2|\"Am\"c/2AG/2AA/2B/2|\"Am\"c/2A/2c/2e/2a/2e/2d/2c/2|\"C\"e/2g/2e/2g/2e/2d/2c/2|\"G\"B/2GF/2Gd/2e/2|\"G\"d/2g/2f/2a/2\"G\"gG/2|\"C\"cc/2cB/2c/2|\"G\"B3/2B/2GG/2B/2|\"Am\"A/2B/2c/\n",
      "---------------\n",
      "M:4/4L:1/4K:G|\"G\"|\"C\"|\"D\"|\"D\"|\"G\"|\"C\"|\"D\"|\"G\"|]\n",
      "\"G\"g3/2f/2e/2d/2|\"C\"e/2f/2g/2e/2c/2|\"G\"B/2d/2G/2B/2D/2d/2|\"C\"e/2f/2g/2e/2|\"D\"d3/4c/4\"D\"B/2d/2|\"G\"g3/2f/2|\"C\"e/2f/2g/2e/2|\"D\"d/2c/2B/2A/2|\"G\"G/2B/2D/2G/2F/2|\"C\"E/2c/2e/2c/2|\"D\"d/2c/2B/2A/2|\"G\"G/2B/2D/2G/2B/2|\"C\"c/2e/2g/2e/2|\"D\"d/2c/2B/2A/2|\"G\"G/2B/2D/2G/2B/2|\"C\"c/2e/2g/2e/2|\"D\"d/2c/2B/2A/2|\"G\"G/2B/2D/2G/2B/2|\"G\"G/2B/2d/2G/2B/2|\"C\"ce/2c/2e/2c/2e/2|\"D\"d/2c/2B/2A/2\"G\"GB/2c/2|\"G\"d/2c/2B/2d/2g/2f/2e/2|\"D\"d/2c/2B/2A/2\"G\"G|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:G\n",
      "|\"G\"|\"C\"\"G\"|\"G\"|\"G\"|\"Am\"\"D7\"|\"G\"|\"C\"\"G\"|\"G\"\"A7\"|\"D7\"|\"G\"|\"G\"|\n",
      "---------------\n",
      "M:4/4L:1/4K:G|\"G\"|\"C\"|\"D\"|\"D\"|\"G\"|\"C\"|\"D\"|\"G\"|]\n",
      "\"G\"d3/4G/4G/2G/2|\"C\"E/2G/2G/2A/2|\"D\"F3/4G/4A/2G/2|\"D\"F/2A/2D/2A/2|\"D\"F/2F/2B/2A/2|\"G\"G3/2G/2|[2GF/2G/2|\"G\"B3/2G/2|\"C\"e/2cd/2c/2|\"G\"B/2A/2G/2F/2|\"Am\"E3/4F/4G/2E/2|\"D\"F/2D/2\"A\"E/2G/2|\"D\"F/2D/2\"A\"E/2G/2|\"D\"F/2D/2\"A\"E/2G/2|\"D\"F/2D/2\"A\"E/2G/2|\"D\"F/2D/2\"A\"E/2G/2|\"D\"F/2D/2\"A\"E/2G/2|\"D\"F/2D/2\"A\"E/2G/2|\"D\"F/2D/2\"A\"E/2G/2|\"D\"F/2D/2F/2A/2|\"D\"F/2A/2\"Bm\"B/2A/2|\"A\"A3/4G/4A/2c/2|\"D\"F/2D/2\"A\"E/2G/2|\"D\"F/2D/2\"A\"E/2G/2|\"D\"F/2D/2F/2A/2d/2|\"A\"c/2e/2A/2c/2e/2A/2c/2|\"D\"d3/4c/4\"A\"A/2c/2|\"D\"d/2c/2d/2f/2d/2f/2d/2f/2|\"A\n",
      "---------------\n",
      "M:4/4L:1/4K:G|\"G\"|\"C\"|\"D\"|\"D\"|\"G\"|\"C\"|\"D\"|\"G\"|]\n",
      "B/4c/4|\"G\"d/2d/4c/4B/4A/4|G/2B/2G/2e/4e/4|\"C\"g/2g/2f/2e/2|\"G\"d/2d/4c/4B/2B/2|\"D\"A/2A/2\"G\"G/2B/2|\"G\"B/2B/4B/4B/2B/2|\"C\"c/2c/2c|e/4g/4g/4f/4e/4d/4c/4|\"G\"B/2B/2B/2A/2|G/2B/2B/2e/4e/4|\"G\"d/4d/4d/4B/4d/4e/4d/4|\"D\"A/2A/2\"G\"G/2|\"G\"B/4d/4B/4d/4d/4e/4d/4B/4|B/4d/4B/4d/4e/2e/4|\"G\"B/4d/4A/4B/4d/2e/4|\"D\"A/4F/4A/4c/4e/4d/4A/4B/4|\"A7\"A/4B/4c/4d/4e/4g/4|\"D\"f/4e/4d/4e/4f/4a/4g/4|\"A\"f/4e/4d/4e/4f/4e/4d/4|\"G\"B/2B/2B/2|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:G\n",
      "|\"Em\"|\"Em\"|\"D\"|\"Em\"|\"Em\"|\"G\"|\"Am\"|\"Em\"|\"D\"\"G\"|\"G\"|\"Em\"|\"Am\"|\"Em\"|\"G\"|\"Em\"|\"G\n",
      "---------------\n",
      "M:4/4L:1/4K:G|\"G\"|\"C\"|\"D\"|\"D\"|\"G\"|\"C\"|\"D\"|\"G\"|]\n",
      "B/2c/2|\"G\"d3/2B/2G/2E/2D/2|\"C\"C/2E/2G/2c/2ed|\"C\"c3/2B/2G/2E/2D/2|\"D\"C/2E/2F/2G/2AA/2B/2|\"D\"c/2B/2A/2c/2f/2e/2d/2|\"G\"B/2G/2B/2d/2gg|\"C\"f/2e/2d/2c/2B/2A/2B/2G/2|\"Am\"A/2G/2F/2E/2\"D\"D2|]\n",
      "\n",
      "M:2/4\n",
      "L:1/4\n",
      "K:G\n",
      "|\"G\"|\"D\"|\"D\"|\"G\"|\"G\"|\"G\"|\"D\"|\"G\"|\"G\"|\"D\"|\"A7\"|\"D\"|\"D\"|\"G\"|\"D\"|\"G\"|]\n",
      "B/2c/2|\"G\"d3/2e/2d3/4e/4|\"D\"d3/4e/4d3/4e/4|\"D\"f3/4e/4d3/4e/4|\"G\"d3/4e/4d3/4e/4|\"G\"B3/4d/4e3/4d/4|\"Am\"c3/4B/4\"D\"A3/4c/4|\"G\"d3/4e/4d3/4e/4|\"G\"B3/4d/4g3/4f/4|\"C\"e3/4d/4c3/4B/4|\"Am\"A3/4B/4\"D7\"A3/4c/4B/2|\"G\"G3/2d/2d/2|\"G\"g3/4f/4g3/4f/4|\n",
      "---------------\n",
      "M:4/4L:1/4K:G|\"G\"|\"C\"|\"D\"|\"D\"|\"G\"|\"C\"|\"D\"|\"G\"|]\n",
      "d|\"G\"G/2B/2d3/4d/4|\"C\"e/2e/2c/2e/2|\"D\"d/2d/2B/2A/2|\"G\"G/4G/4G/2G/2d/2|\"G\"d/2B/2d3/4d/4|\"C\"e/2e/2c/2e/2|\"D\"d/2d/2B/2A/2|\"G\"G/2B/2d3/4d/4|\"C\"e/2e/2c/2e/2|\"G\"d/2d/2B/2G/2|\"C\"e/2e/2\"D\"d|\"G\"G/2B/4B/4B/2B/2|\"C\"c/2e/2\"D\"d/2c/4B/4|\"G\"B/2B/2B/2|\"G\"B/2B/2B3/4B/4|\"D\"A/2A/2E/2F/2|\"G\"G/2G/2G/2D/2|\"C\"E/2A/2G/2E/2|\"G\"G/2G/2G|\"C\"c/2c/2c/2B/2|\"A\"A3/4B/4A/2G/2|A/2c/2c/2B/2|\"A\"A/2G/2F/2E/2|\"D\"D/2D/2D/2F/2|\"G\"G/2G/2G|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:G\n",
      "|\"G\"|\"G\"|\"Am\"|\"Am\"\"D7\"|\"G\"|\"G\"|\"G\"|\"Am\"|\"D\"|\"D\"|\"G\"|\"Am\"|\"D\"|\"G\"|\"G\"|\"Am\"\"D7\"|\"\n",
      "---------------\n",
      "M:4/4L:1/4K:G|\"G\"|\"C\"|\"D\"|\"D\"|\"G\"|\"C\"|\"D\"|\"G\"|]\n",
      "d|\"G\"d/2B/2B/2B/2B/2B|\"C\"e/2B/2B/2B/2B/2B/2B/2B/2|\"D\"A/2A/2A/2A/2A/2A/2A/2A/2|\"D\"d/2d/2d/2d/2e/2f/2d/2B/2|\"C\"e/2c/2B/2B/2c3/2d/2|\"D\"d/2A/2A/2A/2B/2c/2d/2|\"D\"a/2d/2d/2d/2e/2f/2d/2B/2|\"C\"e/2f/2e/2d/2cB/2c/2|\"D\"d/2A/2A/2A/2B/2d/2A/2|\"G\"B/2G/2G/2G|(3D/2E/2F/2|\"G\"Gg/2f/2g/2d/2B/2G/2|\"C\"c/2e/2d/2B/2\"D\"A/2B/2A/2G/2|\"F\"A/2B/2c/2A/2\"G\"B3/2d/2|\"G\"d/2e/2d/2B/2\"C\"c3/2B/2|\"Am\"A/2B/2c/2A/2\"G\"B/2c/2B/2G/2|\"F\"A/2B/2c/2A/2\"G\"B/2c/2B/2G/2|\"D\"A/2B/2c/2A/2\"G\"B/2c/2d/2B/2|\"D\"A/2F/2A/2F/2\"G\"G|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:C\n",
      "|\"C\"\n",
      "---------------\n",
      "M:4/4L:1/4K:G|\"G\"|\"C\"|\"D\"|\"D\"|\"G\"|\"C\"|\"D\"|\"G\"|]\n",
      "B,A,|\"G\"G,B,DG|B/2d/2c/2B/2AB,|\"C\"CE\"D\"A,2|\"G\"B,DB,DG|B/2c/2B/2A/2GD|\"C\"c/2B/2A/2G/2Ee|\"D\"d/2c/2B/2A/2\"D\"GB/2c/2|\"G\"d/2c/2B/2A/2GD|\"C\"E/2F/2G/2E/2\"D\"FD|\"G\"B/2c/2B/2A/2GB,|\"C\"c/2d/2c/2B/2A/2G/2E/2F/2|\"D\"D/2F/2A/2F/2D/2F/2A/2F/2|\"G\"GB/2G/2\"D\"FA/2F/2|\"Em\"E/2F/2G/2A/2\"A7\"BA|\"D\"d/2c/2d/2e/2\"D7\"dB/2c/2|\"G\"d/2c/2B/2A/2GB/2c/2|\"G\"d/2e/2d/2B/2G|\"C\"c/2d/2c/2B/2A/2G/2A/2|\"G\"B/2c/2B/2A/2GB/2c/2|\"D7\"d/2e/2d/2c/2B/2A/2G/2|\"G\"BB/2A/2GB/2c/2|\"G\"d/2e/2d/2B/2GB/2c/2|\"D7\"d/2e/2d/2c/2B/2A/2G/2|\"G\"DGBA/2G/2|\"D7\"F/2\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "!python3 sample.py --out_dir=out-abc-char --start='M:4/4L:1/4K:G|\"G\"|\"C\"|\"D\"|\"D\"|\"G\"|\"C\"|\"D\"|\"G\"|]'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test major key with low samples: C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding: out_dir = out-abc-char\n",
      "Overriding: start = M:4/4L:1/4K:C|\"C\"|\"F\"|\"G\"|\"G\"|\"C\"|\"F\"|\"G\"|\"C\"|]\n",
      "number of parameters: 21.26M\n",
      "abc_char\n",
      "Loading meta from data/abc_char/meta.pkl...\n",
      "M:4/4L:1/4K:C|\"C\"|\"F\"|\"G\"|\"G\"|\"C\"|\"F\"|\"G\"|\"C\"|]\n",
      "A/2B/2|\"C\"c/2ee/2c/2e|c/2e/2g/2e/2ce|\"F\"f/2e/2d/2f/2d/2e/2d/2c/2|\"G\"B/2c/2d/2e/2f/2e/2d/2|\"G\"B/2e/2d/2c/2B/2G/2A/2B/2|\"C\"c/2ee/2c/2e|c/2e/2g/2e/2ce|\"F\"d/2e/2f/2g/2aa|c/2d/2e/2f/2d/2e/2d/2c/2|\"G\"B/2c/2d/2e/2f/2e/2d/2f/2|\"G\"e/2d/2c/2B/2\"C\"ce|\"G\"B/2c/2d/2e/2f/2e/2d/2f/2|\"C\"g/2e/2d/2e/2f/2e/2d/2f/2|\"C\"g/2e/2d/2c/2B/2c/2d/2|\"D7\"B/2G/2A/2B/2\"G\"G|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:D\n",
      "|\"D\"|\"D\"|\"G\"|\"A7\"|\"D\"|\"D\"|\"G\"|\"A7\"|\"D\"|\"D\"|\"G\"|\"A7\"|\"D\"|]\n",
      "AG|\"D\"FDFD|F/2A/2d/2c/2B/2A/2B/2A/2|\"D\"FDFD|\"G\"GBE3/2G/2|\"A7\"c/2B/2A/2B/2c/2B/2A\n",
      "---------------\n",
      "M:4/4L:1/4K:C|\"C\"|\"F\"|\"G\"|\"G\"|\"C\"|\"F\"|\"G\"|\"C\"|]\n",
      "G|\"C\"e/2e/2e/2d/2e3/2c/2d/2|e/2e/2e/2d/2e/2d/2c/2d/2|\"F\"c/2c/2c/2B/2A/2G/2F/2|\"G\"e/2e/2d/2e3/2G/2|\"G\"d/2d/2d/2e/2f/2e/2d/2c/2|\"G\"B/2G/2A/2B/2G/2A/2B/2c/2|\"G\"d/2d/2c/2B/2G/2A/2B/2c/2|\"G\"d/2d/2c/2B/2\"D7\"A/2B/2c/2A/2|\"G\"GBG|d|\"G\"g/2f/2g/2e/2\"D7\"f/2e/2f/2d/2|\"G\"g/2f/2g/2e/2\"D7\"de/2f/2|\"G\"g/2f/2g/2e/2\"D7\"f/2e/2f/2d/2|\"E#m\"e/2f/2g/2e/2\"D7\"f/2e/2d/2c/2|\"G\"dBG|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:G\n",
      "|\"G\"|\"G\"|\"C\"\"G\"|\"Am\"\"D7\"|\"G\"|\"G\"|\"C\"\"G\"|\"Am\"\"D7\"|\"G\"|\"G\"|\"C\"|\"Am\"\"D7\"|\"G\"|\"G\"|\"C\"|\"Am\"\"D7\"|\"G\"|]\n",
      "B/2A/2|\"G\"GBGG|\"G\"GBd2|\"C\"ce\n",
      "---------------\n",
      "M:4/4L:1/4K:C|\"C\"|\"F\"|\"G\"|\"G\"|\"C\"|\"F\"|\"G\"|\"C\"|]\n",
      "e/2^d/2|\"C\"ee/2c/2d/2|cc/2d/2|\"F\"c2A|\"G\"G3/2A/2B|\"G\"d2e/2^d/2|\"G\"g3/2f/2e/2d/2|\"G\"d2c|\"C\"c3|E/2F/2|\"C\"Gcc|\"F\"A/2c/2A/2G/2|\"G\"A3/2G/2GF|E/2GA/2B/2G|\"C\"g2e/2d/2|\"F\"c/2A/2G/2F/2A/2|\"G\"G3|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:F\n",
      "|\"F\"|\"C\"|\"Dm\"\"C7\"|\"F\"|\"C\"|\"C\"|\"Dm\"\"C7\"|\"F\"\"C7\"|\"F\"|\"C\"|\"Dm\"\"G7\"|\"F\"|\"C\"|\"C7\"|\"F\"|\"C\"|\"G\"|\"G\"\"F\"|\"C\"|\"F\"|\"G7\"|\"C\"|]\n",
      "F/2G/2|\"F\"AFF|\"C\"E2C2|\"Dm\"FA\"C7\"FG|\"F\"AFFA|\"C\"E2C|\"Dm\"FA\"C7\"GE|\"F\"F3|\"F\"F3|AB|\"F\"c2c3/2c/2|\"C\"cdcB|\"Dm\"A3\"C7\"AB|\"F\"c2c3/2c/2|\"C\"cdcB|\"Dm\"A3\"G7\"AB|\"C\"c2c3/2c/2|\"C\"cdcB|\"F\"c2c3/2c/2|\"\n",
      "---------------\n",
      "M:4/4L:1/4K:C|\"C\"|\"F\"|\"G\"|\"G\"|\"C\"|\"F\"|\"G\"|\"C\"|]\n",
      "E/2F/2|\"C\"GEE|G/2A/2GE|\"F\"FAF|\"C\"E2D/2C/2|\"F\"A,CF|\"G\"G/2A/2G/2F/2|\"C\"C2\"G\"BB|\"C\"c/2B/2c/2d/2ee|\"F\"c/2d/2c/2d/2ef|\"G\"G2G|\"C\"g/2a/2g/2f/2ee|\"F\"f/2e/2d/2c/2B/2A/2G/2=F/2|\"G\"G/2A/2G/2A/2G/2E/2F/2D/2|\"C\"C/2E/2G/2A/2cB|\"F\"A/2B/2c/2A/2\"G\"GB/2c/2|\"G\"d/2e/2d/2B/2G/2B/2d/2e/2|\"G\"f/2e/2d/2B/2\"C\"e/2d/2c/2e/2|\"F\"c/2d/2c/2d/2e/2f/2d/2|\"C\"c/2d/2c/2d/2e/2f/2d/2c/2|\"G\"B/2G/2A/2B/2\"C\"cc\"C\"g/2e/2c/2e/2\"F\"f/2e/2f/2a/2|\"C\"g/2e/2c/2e/2gg|\"F\"a/2g/2f/2e/2\"G7\"d/2f/2e/2d/2|\"C\"g/2e/2c/2e/2gg|\"F\"a/2g/2f/2e/2\"G7\"d/2f/2e/2d\n",
      "---------------\n",
      "M:4/4L:1/4K:C|\"C\"|\"F\"|\"G\"|\"G\"|\"C\"|\"F\"|\"G\"|\"C\"|]\n",
      "e/2^d/2|\"C\"e/2g/2a/2g/2e/2g/2a/2|g/2e/2g/2-g/2e/2g|\"F\"a/2a/2a/2-a/2a/2g/2a/2|\"G\"g/2e/2d/2B/2G/2B/2c/2|\"G\"d/2f/2e/2e/2dd/2e/2|\"C\"f/2e/2d/2f/2e/2d/2c/2|\"F\"A/2B/2c/2A/2\"G\"GA/2B/2|\"C\"c\"F\"f/2e/2d/2c/2|\"G\"B/2G/2A/2B/2\"C\"ce|\"C\"cee|\"C\"cee|\"C\"e/2g/2e/2g/2c/2e/2g/2|\"Dm\"a/2g/2f/2e/2\"G\"d2|\"C\"cee/2c/2e/2g/2|\"C\"e/2g/2e/2g/2c/2e/2g/2|\"Dm\"a/2g/2f/2\"G\"d/2c/2B/2c/2|\"G\"dBG|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:C\n",
      "|\"C\"|\"C\"\"G7\"|\"C\"|\"C\"|\"G7\"|\"C\"|\"C\"|\"G7\"|\"C\"|\"F\"|\"C\"|\"G7\"|\"C\"|\"F\"|\"C\"|\"G7\"|\"C\"|]\n",
      "E/2|\"C\"CE/2G/2ce/2d/2|ce/2g/2^f/2g/2e/2c/2|\"\n",
      "---------------\n",
      "M:4/4L:1/4K:C|\"C\"|\"F\"|\"G\"|\"G\"|\"C\"|\"F\"|\"G\"|\"C\"|]\n",
      "e/4f/4|\"C\"g/2g/4a/4g/4e/4c/4e/4|g/2g/4a/4g/4e/4c/4|\"F\"f/4e/4f/4g/4a/2a/4f/4|\"C\"g/4e/4c/4e/4g/4e/4c/4e/4|\"G\"d/4B/4G/4d/4B/4G/4B/4|\"G\"G/4B/4A/4B/4\"C\"c/2e/2|\"G\"e/4d/4c/4B/4\"C\"c|\"C\"E/2G/4A/4G/2G/2|]\n",
      "\n",
      "M:6/8\n",
      "L:1/8\n",
      "K:G\n",
      "|\"G\"|\"G\"|\"G\"|\"G\"|\"G\"\"D7\"|\"G\"|\"G\"|\"G\"|\"G\"\"D7\"|\"G\"|\"G\"|\"G\"|\"G\"|\"C\"\"D7\"|\"G\"|\"G\"|\"G\"|\"G\"|\"G\"|\"D7\"|\"G\"|]\n",
      "B/2A/2|\"G\"GBDG/2A/2|\"G\"Bdg2|\"G\"GB/2c/2d/2e/2d/2B/2G/2|\"G\"Bdg2|\"G\"GB/2c/2d/2e/2d/2B/2G/2|\"G\"Bdg2|\"G\"GB/2c/2d/2e/2d/2B/2G/2|\"D7\"A3B/2c/2|\"G\"d3e/2d/2|\"G\"dgb3/2a/2|\"G\"bagf|\"A7\"e3a/2g/2|\"D7\"fd\n",
      "---------------\n",
      "M:4/4L:1/4K:C|\"C\"|\"F\"|\"G\"|\"G\"|\"C\"|\"F\"|\"G\"|\"C\"|]\n",
      "c/2d/2|\"C\"e/2G/2e/2c/2|e/2G/2e/2c/4c/4|\"F\"f/2F/2f/2d/2|\"Dm\"d/2d/2d/2c/4d/4|\"G\"e/2d/2d/2c/4d/4|\"G\"e/2G/2e/4c/4|\"G\"d/2d/4B/4d/2A/2|\"G\"G/2B/2c/4d/4|\"C\"e/2G/2e/4c/4c/4c/4|\"F\"f/2d/2d/4e/4f/4d/4|\"G\"g/2d/2d/4e/4f/4d/4|\"C\"g/2d/2c/4d/4e/4c/4|\"F\"f/2d/2c/4d/4f/4d/4|\"C\"e/2d/2\"G7\"c/2d/2|\"C\"e/2c/2c/4d/4e/4c/4|\"Dm\"f/2d/2d/2c/4d/4|\"G7\"e/2e/2B/4c/4d/4B/4|\"C\"c3/2|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:F\n",
      "|\"F\"|\"F\"|\"Gm\"|\"Gm\"\"C7\"|\"F\"\"C7\"|\"F\"|\"F\"|\"F\"|\"Gm\"|\"C\"|\"C\"|\"C\"|\"Gm\"\"C7\"|\"F\"\"C7\"|\"F\"|\"F\"|\"B\"|\"Gm\"|\"C\"|\"C\"|\"F\"|]\n",
      "A/2B/2|\"F\"cc/2c/2cd|\"F\"c\n",
      "---------------\n",
      "M:4/4L:1/4K:C|\"C\"|\"F\"|\"G\"|\"G\"|\"C\"|\"F\"|\"G\"|\"C\"|]\n",
      "EF|\"C\"G2E2|c2B2|\"F\"dcBA|F2A,2|\"G\"B,2D2|F2G2|\"G\"F4-|F4|\"G\"G2G2|B2Bd|\"C\"e2e2|e2e2|\"F\"dcde|f2e2|\"G\"d4-|d2Bd|\"C\"e2e2|e2e2|\"F\"d2d2|\"G\"B2A2|\"G\"G2G2|\"F\"A2A2|\"G\"G2G2|\"C\"B2B2|\"F\"c2e2|e2c2|\"G\"B4-|B2G2|\"G\"G2G2|B2Bd|\"C\"e2e2|e2e2|\"F\"d2d2|\"G\"cB2c|\"D\"d2de|\"G\"f2ed|\"C\"g2fe|\"G\"d2BA|\"G\"G2G2|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:D\n",
      "|\"D\"|\"G\"\"A7\"|\"D\"|\"D\"|\"G\"\"A7\"|\"D\"|\"D\"|\"G\"\"A7\"|\"D\"|\"D\"|\"G\"|\"A7\"|\"D\"|\"G\"|\"A7\"|\"D\"|]\n",
      "F/2E/2|\"D\"D3/2E/2FD/2E/2|F/2AF/2A3/2A/2|\"G\"B/2G/2B/2d/2e/2d/2B/2A/2|\"D\"F/2A/2A/2AF/2A/2|\"G\"B/2G/2B/2d3/2B/2|\"A7\"A/2c/2e/2g/2f/\n",
      "---------------\n",
      "M:4/4L:1/4K:C|\"C\"|\"F\"|\"G\"|\"G\"|\"C\"|\"F\"|\"G\"|\"C\"|]\n",
      "E/2F/2|\"C\"GEE|\"F\"FDC|\"G\"B,2D|\"G\"B,DG|\"G\"G/2F/2E/2D/2-|\"G\"D2E/2F/2|\"C\"EGc|\"F\"c2A|\"G\"B2G/2F/2|\"G\"EDG|\"G\"B,DG|\"G\"B/2B/2-B2|\"G\"B/2G/2B/2d/2|\"D\"c/2A/2F/2E/2|\"G\"D2|\"G\"B,DG|\"G\"B/2B/2-B2|\"G\"B2G/2F/2|\"E7\"EeB|\"A\"c/2A/2F/2E/2|\"E7\"c/2A/2F/2E/2|\"A\"A2|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:A\n",
      "|\"A\"|\"A\"|\"A\"|\"A\"|\"A\"|\"A\"|\"A\"\"D\"|\"A\"|\"A\"|\"A\"\"F7\"|\"B7\"|\"B7\"|\"E7\"|\"A\"|\"A\"|\"A\"|\"A\"|\"A\"|\"A\"\"D\"|\"A\"|\"A\"|]\n",
      "cB|\"A\"A3/2E/2CE|AEcA|\"A\"A3/2E/2CE|AEcA|\"A\"c3/2A/2EA|\"A\"ce\"D\"dc|\"A\"c2ec|\"A\"A3/2E/2CE|AEcA|\"A\"c3/2A/2EA|\"A\"c3/2A/2EA|\"A\"ce\"D\"dc|\"A\"c2ec|\"A\"e3/2c\n",
      "---------------\n",
      "M:4/4L:1/4K:C|\"C\"|\"F\"|\"G\"|\"G\"|\"C\"|\"F\"|\"G\"|\"C\"|]\n",
      "e/2f/2|\"C\"g/2e/2c/2e/2G/2e/2c/2e/2|g/2e/2c/2e/2G/2e/2f/2|\"C\"g/2e/2c/2e/2G/2e/2c/2e/2|\"F\"a/2f/2d/2f/2a/2f/2d/2f/2|\"G\"g/2e/2c/2e/2g/2e/2c/2e/2|\"G\"d/2B/2G/2B/2d/2f/2d/2B/2|\"G\"d/2B/2G/2B/2de/2f/2|\"C\"g/2e/2c/2e/2gg/2e/2|\"F\"a/2f/2d/2f/2aa/2g/2|\"F\"f/2d/2e/2c/2\"G\"d/2c/2B/2d/2|\"C\"ccc|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:G\n",
      "|\"G\"|\"G\"|\"D7\"|\"G\"|\"G\"|\"D7\"|\"G\"|\"G\"|\"G\"|\"Am\"|\"D7\"|\"G\"|\"G\"|\"G\"|\"Am\"|\"D7\"|\"G\"|]\n",
      "d|\"G\"dd/2c/2BA|\"G\"GGG/2A/2B/2c/2|\"D7\"d/2e/2d/2c/2BG|\"G\"ddd/2c/2BA|\"G\"GGG/2A/2B/2c/2|\"D7\"d/2e/2d/2c/2BA|\"G\"G3|D/2C/2|\"G\"B,/2D/2G\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "!python3 sample.py --out_dir=out-abc-char --start='M:4/4L:1/4K:C|\"C\"|\"F\"|\"G\"|\"G\"|\"C\"|\"F\"|\"G\"|\"C\"|]'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test minor key with low samples: Am"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding: out_dir = out-abc-char\n",
      "Overriding: start = M:4/4L:1/4K:Am|\"Am\"|\"Dm\"|\"E\"|\"E\"|\"Am\"|\"Dm\"|\"E\"|\"Am\"|]\n",
      "number of parameters: 21.26M\n",
      "abc_char\n",
      "Loading meta from data/abc_char/meta.pkl...\n",
      "M:4/4L:1/4K:Am|\"Am\"|\"Dm\"|\"E\"|\"E\"|\"Am\"|\"Dm\"|\"E\"|\"Am\"|]\n",
      "A/2B/2|\"F\"c/2F/2F/2c/2F/2c/2F/2|\"F\"A/2G/2F/2d/2F/2c/2F/2|\"G\"=B/2G/2d/2G/2B/2G/2d/2G/2|\"G\"=B/2G/2d/2G/2e/2G/2d/2G/2|\"Am\"A/2B/2c/2d/2e/2f/2g/2e/2|\"Dm\"f/2e/2d/2c/2\"A\"AB/2c/2|\"Dm\"d/2^c/2d/2e/2f/2e/2d/2c/2|\"E\"=B/2G/2A^B|\"Am\"c/2A/2^B/2c/2\"Dm\"d/2c/2d|\"C\"=e/2g/2e/2cg/2e/2|\"Dm\"f/2e/2d/2c/2\"G7\"=B/2c/2d|\"C\"c/2d/2e/2c/2\"G7\"d/2e/2f/2d/2|\"C\"c/2d/2e/2c/2\"G\"=B/2c/2d|\"G\"d/2e/2f/2d/2\"C\"e/2f/2g/2e/2|\"Dm\"f/2e/2d/2c/2\"A\"Ad/2e/2|\"Dm\"f/2e/2d/2c/2\"G7\"=B/2c/2d/2B/2|\"C\"c/2d/2e/2c/2\"G\"=B/2c/2d|\"F\"A/2G/2A/2=B/2\"C\"c/2G/2E/\n",
      "---------------\n",
      "M:4/4L:1/4K:Am|\"Am\"|\"Dm\"|\"E\"|\"E\"|\"Am\"|\"Dm\"|\"E\"|\"Am\"|]\n",
      "E|\"A\"AA/2B/2c/2B/2c/2d/2|\"Am\"e/2f/2e/2c/2e/2c/2d/2|\"Dm\"a/2d/2c/2d/2B/2\"E\"e2|\"E\"^G/2B/2e/2G/2B/2e/2G/2B/2|\"E\"^G/2B/2e/2c/2B/2c/2d/2|\"E\"^g/2e/2c/2B/2c/2d/2e/2c/2|\"Am\"A/2B/2c/2d/2e/2d/2c/2|\"Dm\"B/2c/2d/2e/2\"E\"d/2c/2B/2A/2|\"Gm\"B/2G/2A/2F/2\"Am\"EF/2E/2|\"Dm\"D/2A/2c/2d/2A/2c/2A/2d/2|\"E\"^G/2A/2B/2c/2B/2c/2d/2|\"Am\"e/2^d/2e/2c/2\"Gm\"B/2c/2d|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:D\n",
      "|\"D\"|\"D\"|\"Em\"|\"Em\"\"A7\"|\"D\"|\"D\"|\"D\"|\"Em\"|\"A7\"|\"D\"|\"D\"|\"G\"|\"D\"|\"Em\"\"A7\"|\"G\"|\"D\"|\"Em\"\"A7\"|\"D\"|]\n",
      "A|\"D\"ddd/2e/2d/2B/2|\"D\"AAAA|\"Em\"GG\"A7\"GG|\"D\"FA\"A7\"A3/2G/2\n",
      "---------------\n",
      "M:4/4L:1/4K:Am|\"Am\"|\"Dm\"|\"E\"|\"E\"|\"Am\"|\"Dm\"|\"E\"|\"Am\"|]\n",
      "AB|\"A\"cdcc|c/2d/2c/2A/2c/2d/2e/2f/2|\"Dm\"dAAB|d/2e/2d/2c/2Bd|\"Am\"c/2d/2c/2B/2AA|\"Dm\"d/2e/2d/2c/2Bd|\"E\"e/2f/2e/2d/2c/2B/2c/2|\"Am\"AAAe/2f/2|\"Am\"gg/2a/2g/2f/2e/2d/2|\"Am\"c/2d/2c/2B/2Ad/2e/2|\"Dm\"ff/2g/2f/2e/2d/2c/2|\"Am\"c/2d/2c/2B/2\"Am\"AB/2c/2|\"Dm\"d/2g/2g/2f/2g/2e/2d/2c/2|\"Am\"A/2B/2c/2A/2\"Dm\"A|]\n",
      "\n",
      "M:2/4\n",
      "L:1/4\n",
      "K:F\n",
      "|\"F\"|\"Gm\"|\"C\"|\"F\"|\"Dm\"\"Gm\"|\"F\"\"C7\"\"F\"|\"F\"|\"F\"\"C\"|\"F\"\"B\"|\"F\"|\"Gm\"\"C\"|\"F\"\"B\"|\"F\"\"C7\"\"F\"|]\n",
      "(F/2E/2)|\"F\"C/2F/2A/2c/2f/2c/2A/2F/2|\"Gm\"G/2^F/2G/2A/2B/2c/2d/2e/2|\"C\"(3g/2a/2g/2f/2e/2\"F\"f/2e/2f/2c/2|\"\n",
      "---------------\n",
      "M:4/4L:1/4K:Am|\"Am\"|\"Dm\"|\"E\"|\"E\"|\"Am\"|\"Dm\"|\"E\"|\"Am\"|]\n",
      "\"Am\"e/2A/4A/4c/4e/4|\"Dm\"f3/4e/4d/2c/2|\"Am\"A/2c/2c/4d/4|\"E\"e/2E/2e/4c/4B/4|\"Am\"A/2A/2A\"Am\"e/4=e/4f/4e/2A/2|\"Dm\"A/2d/2d/4c/4d/2|\"E\"B/2G/2c/4B/4|\"Am\"A/2A/2A|\"Am\"e/4e/4e/4e/2d/4e/4|\"Dm\"f/4d/4f/4e/4d/4c/4|\"E\"B/2B/2B/2c/4B/4|\"E\"B/4G/4F/4G/4B/4c/4B/4|\"Am\"A/4=G/4A/4B/4c/2B/2|\"Dm\"d/4^e/4f/4e/4d/2c/2|\"E\"B/2G/2c/4B/4|\"E\"G/2F/2\"Am\"A|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:Bbm\n",
      "|\"Bm\"|\"A\"|\"Bm\"|\"F7\"\"Bm\"|\"D\"\"A\"|\"Bm\"|\"D\"|\"A\"\"F7\"\"Bm\"|\"D\"\"A\"|\"Bm\"|\"F7\"\"Bm\"|]\n",
      "B,/2C/2|\"Bm\"D/2E/2D/2E/2F/2E/2D/2|\"A\"E/2F/2E/2F/2C/2E/2F/2G/2|\"Bm\"D/2E/2D/2E/2FF\n",
      "---------------\n",
      "M:4/4L:1/4K:Am|\"Am\"|\"Dm\"|\"E\"|\"E\"|\"Am\"|\"Dm\"|\"E\"|\"Am\"|]\n",
      "E|\"A\"A3/2B/2c3/2A/2E|\"A\"ABcd|\"D\"f3/2e/2d3/2c/2|\"E\"B3/2c/2B3/2G/2\"A\"A2E2|\"A\"A3/2B/2c3/2e/2a/2g/2f3/2e/2|\"D\"d3/2c/2d3/2f/2\"A\"e3/2c/2A3/2e/2|\"D\"d3/2c/2d3/2f/2\"A\"e2e2|\"D\"f3/2e/2d3/2c/2\"A\"d3/2c/2e3/2c/2|\"D\"d3/2c/2d3/2f/2\"A\"e2e2|\"D\"f3/2e/2d3/2f/2\"A\"g3/2e/2c3/2e/2|\"D\"d3/2c/2d3/2f/2\"A\"e2e2|\"D\"f3/2e/2d3/2f/2\"A\"e3/2c/2A3/2c/2|\"D\"d3/2f/2\"A\"e3/2d/2c3/2e/2|\"D\"d3/2c/2d3/2f/2\"A\"e2e2|\"D\"f3/2e/2d3/2f/2\"A\"g3/2e/2c3/2e/2|\"D\"d3/2f/2a3/2f/2\"A\"g3/2e/2c3/2e/2|\"D\"d3/2f/2\"A\"e3/2g/2\"D\"f3/2e/2d3/2f/2|\"G\"g3/2f/2g3/2a/2\"A\"\n",
      "---------------\n",
      "M:4/4L:1/4K:Am|\"Am\"|\"Dm\"|\"E\"|\"E\"|\"Am\"|\"Dm\"|\"E\"|\"Am\"|]\n",
      "\"Am\"A3/2B/2c3/2d/2|\"Dm\"A3/2B/2c3/2d/2|\"E\"e3/2d/2c3/2B/2|\"Am\"A3/2B/2c3/2d/2|\"Dm\"A3/2B/2c3/2d/2|\"E\"e3/2d/2c3/2B/2|\"Am\"A3/2B/2c3/2d/2|\"Dm\"A3/2B/2c3/2d/2|\"E\"e3/2d/2c3/2B/2|\"Am\"A3/2B/2c3/2d/2|\"Dm\"A3/2B/2c3/2d/2|\"E\"e3/2d/2c3/2e/2|\"Am\"A3/2B/2c3/2e/2|\"Dm\"d3/2c/2d3/2e/2|\"G7\"f3/2e/2d3/2|\"C\"c3/2B/2\"G7\"c3/2d/2|\"C\"e3/2d/2c3/2d/2|\"F\"c3/2A/2G3/2e/2|\"D7\"d3/2c/2B3/2A/2|\"G7\"G3|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:G\n",
      "|\"G\"|\"G\"|\"F\"|\"G\"|\"D7\"|\"G\"|\"G\"|\"F\"|\"G\"|\"D7\"|\"G\"|\"G\"|\"F\"|\"G\"|]\n",
      "(3def|\"G\"g2b2d2g2|\"G\"Bdg2g2g2|\"F\"a2a2a4|\"G\"g2g2g2|\"D7\"f2f\n",
      "---------------\n",
      "M:4/4L:1/4K:Am|\"Am\"|\"Dm\"|\"E\"|\"E\"|\"Am\"|\"Dm\"|\"E\"|\"Am\"|]\n",
      "E|\"Am\"A/2^G/2A/2B/2c/2d/2e/2f/2|\"Am\"e/2d/2c/2B/2cE|\"Dm\"FDD/2E/2F/2G/2|\"E\"E/2G/2B/2c/2B/2A/2B/2c/2|\"Am\"A/2^G/2A/2B/2c/2d/2e/2f/2|\"E\"e/2=d/2e/2B/2c/2B/2A/2B/2|\"Am\"c/2A/2B/2c/2\"Dm\"d|e|\"Am\"a/2=g/2a/2e/2g/2a/2g/2|\"Am\"e/2a/2=g/2a/2\"Dm\"e/2d/2f/2a/2|\"F\"=g/2f/2a/2c'/2\"C\"=b/2g/2e/2c/2|\"dm\"d/2^c/2d/2e/2\"Dm\"d/2e/2b/2c'/2|\"E#7\"a/2g/2f/2e/2\"Am\"c/2a/2a|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:A\n",
      "|\"A\"|\"A\"|\"A\"|\"A\"\"E7\"|\"A\"\"E7\"|\"A\"|\"A\"|\"A\"|\"A\"|\"E7\"\"A\"|\"A\"|\"A\"|\"A\"|\"A\"\"E7\"|\"A\"|\"A\"|\"A\"|\"A\"|\"E7\"\"A\"|]\n",
      "G/2|\"A\"A/2B/2c/2d/2ea|\"A\"A/2G/2A/2B/2\"A\"c\n",
      "---------------\n",
      "M:4/4L:1/4K:Am|\"Am\"|\"Dm\"|\"E\"|\"E\"|\"Am\"|\"Dm\"|\"E\"|\"Am\"|]\n",
      "E|\"Am\"AB/2c/2AA/2B/2|c/2B/2A/2c/2B/2c/2d/2|\"Dm\"a3d/2e/2|\"Am\"edd/2e/2f/2g/2|a/2b/2a/2g/2e/2g/2e/2d/2|\"Am\"e/2d/2c/2B/2c/2A/2B/2c/2|\"Dm\"d/2e/2f/2g/2ag/2e/2|\"E\"d/2e/2e/2f/2g/2e/2f/2|\"Am\"de/2d/2e/2f/2g/2|a/2g/2e/2g/2e/2d/2g/2e/2|\"Dm\"a/2f/2a/2f/2ag/2e/2|\"Am\"d/2e/2f/2g/2\"Dm\"ag/2e/2|\"D\"f/2e/2f/2g/2e/2f/2d/2e/2|\"A\"^c/2A/2c/2e/2c/2e/2c/2|a/2g/2e/2c/2g/2e/2c/2g/2|\"Dm\"a/2g/2e/2g/2ag/2e/2|\"G\"^f/2d/2g/2e/2g/2e/2c/2d/2|\"cc'\"eccd|\"Cc'\"eccd|\"cc'\"e^d/2e/2\"G7\"ff|\"C\"e/2^d/2e/2f/2\"G7\"e/2d/2c/2d/2|\"cc'\"e/2^d/2e/2f/2\n",
      "---------------\n",
      "M:4/4L:1/4K:Am|\"Am\"|\"Dm\"|\"E\"|\"E\"|\"Am\"|\"Dm\"|\"E\"|\"Am\"|]\n",
      "E|\"A\"AA/2B/2c/2B/2c/2d/2|\"Am\"e/2f/2e/2c/2\"Dm\"BB/2c/2|\"E\"B/2c/2B/2A/2G/2E/2F/2|\"Am\"AAA/2B/2c/2d/2|\"Am\"e/2f/2e/2c/2\"Dm\"BB/2c/2|\"E\"B/2c/2B/2A/2GE|\"Am\"AA/2B/2c/2B/2A/2F/2|\"Dm\"A/2B/2c/2B/2\"E\"AB/2c/2|\"Am\"d/2e/2d/2c/2\"Dm\"Be/2f/2|\"E\"e/2f/2e/2c/2\"Am\"d/2e/2a/2e/2|\"Am\"e/2f/2e/2c/2\"Dm\"d|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:G\n",
      "|\"G\"|\"G\"\"C\"|\"G\"\"C\"|\"G\"|\"C\"|\"G\"\"C\"|\"G\"\"C\"|\"G\"\"C\"|\"G\"|\"G\"\"C\"|\"G\"\"C\"|\"G\"\"C\"|\"G\"\"C\"|\"G\"|\"Am\"\"D7\"|\"G\"\"C\"|\"G\"\"C\"|\"G\"\"C\"|\"G\"\"C\"|\"G\"\"C\"|\"G\"\"C\"|\"G\"|\"G\"\"C\"|]\n",
      "G/2|\"G\"G/4A/4B/4c/4\"C\"G/2c/2|\"G\"G/2B/4c/4\"C\"G/2c/2|\"G\"G/\n",
      "---------------\n",
      "M:4/4L:1/4K:Am|\"Am\"|\"Dm\"|\"E\"|\"E\"|\"Am\"|\"Dm\"|\"E\"|\"Am\"|]\n",
      "E|\"Am\"ABccB/2c/2|\"E\"d/2=e/2f/2d/2e/2c/2B/2|\"Am\"cAAc/2d/2|\"Am\"edcc/2d/2|\"E\"e/2f/2e/2d/2c/2B/2c/2|\"Am\"A/2B/2c/2A/2E/2A/2B/2|\"E\"=G/2A/2B/2G/2EG/2B/2|\"Am\"c/2d/2c/2B/2A/2B/2c/2|\"E\"=G/2A/2B/2G/2\"Am\"AB/2c/2|\"Dm\"d/2=e/2f/2d/2\"E\"e/2d/2c/2B/2|\"Am\"c/2A/2B/2G/2A/2B/2c/2d/2|\"Am\"e/2f/2e/2d/2\"E\"ed/2e/2|\"Am\"c/2A/2B/2G/2\"Am\"A|]\n",
      "\n",
      "M:4/4\n",
      "L:1/4\n",
      "K:C\n",
      "|\"G\"|\"D\"|\"G\"|\"C\"\"G\"|\"Am\"\"D7\"|\"G\"|\"D\"|\"G\"|\"C\"\"G\"|\"C\"\"D7\"|\"G\"|\"G\"|\"C\"\"G\"|\"Am\"\"D7\"|\"G\"|\"G\"|\"C\"\"G\"|\"C\"\"D7\"|\"G\"|]\n",
      "G/2A/2|\"G\"BBd3/2d/2|\"D\"cBA3/2c/2|\"G\"BBd3/2d/2|\"C\"cB\"G\"G3/2B/\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "!python3 sample.py --out_dir=out-abc-char --start='M:4/4L:1/4K:Am|\"Am\"|\"Dm\"|\"E\"|\"E\"|\"Am\"|\"Dm\"|\"E\"|\"Am\"|]'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test older checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " !python3 sample.py --out_dir=older_ckpt/m_voices --path_meta=older_ckpt/m_voices --start='M:4/4L:1/4K:G|\"G\"|\"C\"|\"D\"|\"D\"|\"G\"|\"C\"|\"D\"|\"G\"|]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat older_ckpt/m_voices/ckpt.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l older_ckpt/m_voices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l out-abc-char/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!id"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
