{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA for Nottingham Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for preprocessing Nottigham dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "sns.set_style(\"dark\")\n",
    "global control_var\n",
    "control_var = True\n",
    "def get_tuneBooks_file_names(path):\n",
    "    tuneBook_filenames = []\n",
    "    \n",
    "    for tuneBook_name in os.listdir(path):\n",
    "        if tuneBook_name.endswith('.abc'):\n",
    "            tuneBook_filenames.append(tuneBook_name)\n",
    "    \n",
    "    #return [\"jigs.abc\"]\n",
    "    return tuneBook_filenames\n",
    "def split_abc_song(abc_song):\n",
    "    k_index = abc_song.find('K:')\n",
    "\n",
    "    split_index = abc_song.find('\\n', k_index)\n",
    "\n",
    "    header = abc_song[:split_index]\n",
    "    body = abc_song[split_index+1:]\n",
    "    return header, body\n",
    "\n",
    "def tuneBook_to_dataframe(tuneBook):\n",
    "    song_list = tuneBook.split(\"\\n\\n\")\n",
    "    song_list = [song.strip() for song in song_list]\n",
    "    songs_header_body_format = [split_abc_song(song) for song in song_list if song != '']\n",
    "    \"\"\"\n",
    "    for (h,b) in songs_header_body_format:\n",
    "        print(\"header\")\n",
    "        print(h)\n",
    "        print(\"*\"*10)\n",
    "        print(\"body\")\n",
    "        print(b)\n",
    "        print(\"\\n\")\n",
    "        print(\"-\"*50)\n",
    "        print(\"\\n\")\n",
    "    \"\"\"\n",
    "    \n",
    "    return pd.DataFrame(songs_header_body_format, columns=['header', 'body'])\n",
    "\n",
    "def get_attributes_from_song_header(tuneBook_df,tuneBook_name):\n",
    "    attributes_column = tuneBook_df[\"header\"]\n",
    "    attributes_list = list(attributes_column)\n",
    "    \n",
    "    data = []\n",
    "    for string in attributes_list:\n",
    "        song = {}\n",
    "        for line in string.split(\"\\n\"):\n",
    "            if not line.startswith(\"%\"):\n",
    "                if ':' in line:\n",
    "                    key, value = line.split(\":\", maxsplit=1)\n",
    "                    song[key.strip()] = value.strip()\n",
    "        data.append(song)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    descriptive_names = {\n",
    "                        'X': 'reference_number',\n",
    "                        'T': 'title',\n",
    "                        'S': 'source',\n",
    "                        'M': 'meter',\n",
    "                        'L': 'unit_note_length',\n",
    "                        'R': 'rhythm',\n",
    "                        'P': 'parts',\n",
    "                        'K': 'key',\n",
    "                        'F': 'file_name',\n",
    "                        'N': 'notes'\n",
    "                        }\n",
    "    df.rename(columns=descriptive_names, inplace=True)\n",
    "    df[\"tuneBook\"] = tuneBook_name\n",
    "    df[\"original_header\"] =  tuneBook_df[\"header\"]\n",
    "    df[\"original_body\"] =  tuneBook_df[\"body\"]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_chord_progression(song_body):\n",
    "    bars = song_body.split(\"|\")\n",
    "    chord_progression = \"|\"\n",
    "    for bar_content in bars:\n",
    "        match_chords = r'\"[^\"]*\"'\n",
    "        chords = re.findall(match_chords, bar_content)\n",
    "        bar_chords = ''.join(chords)\n",
    "        if bar_chords != \"\":\n",
    "            chord_progression += bar_chords +'|'\n",
    "    return chord_progression\n",
    "\n",
    "def get_chord_occurrences(song_body):\n",
    "    bars = song_body.split(\"|\")\n",
    "    chord_progression = \"|\"\n",
    "    match_chords = r'\"[^\"]*\"'\n",
    "    chords = [re.findall(match_chords, bar_content)  for bar_content in bars if bar_content]\n",
    "    chords = [chord for bar_chords in chords for chord in bar_chords]\n",
    "    chord_occurrences = dict(Counter(chords))\n",
    "    return chord_occurrences\n",
    "\n",
    "def erase_double_bars(song):\n",
    "    pattern = r'\\|{2,}'\n",
    "    return re.sub(pattern,'|',song)\n",
    "\n",
    "def add_ending_char(song):\n",
    "    song_with_end = song \n",
    "    if not song.endswith(']'):\n",
    "        if(not song.endswith('|')):\n",
    "            song_with_end = song_with_end+'|]'\n",
    "        else:\n",
    "            pattern = r'\\|$'\n",
    "            song_with_end = re.sub(pattern,'|]',song)\n",
    "    return song_with_end\n",
    "\n",
    "def get_chords_data(song_body=\"\"):\n",
    "    #song_body = 'P:A\\nD|\"Gm\"GB2G|\"D7\"^Fd2D|\"Gm\"GB2G|\"D7\"^FA2D|\"Gm\"GB2G|\"D\"^Fg -\"Cm\"gc|\\\\\\n\"Gm/d\"B2 \"D7\"A2|\"Gm\"G3:|\\nP:B\\nd|\"Gm\"dg2d|\"Gm\"gb2d|\"F\"cf2c|\"F\"fa2c|\"Eb\"Be2d/2c/2|\"Gm\"dg -\"Cm\"gc|\\\\\\n\"Gm/d\"B2 \"D7\"A2|\"Gm\"G3:|'\n",
    "    clean_body_song = \"\"\n",
    "    match_pattern = \"\"\n",
    "    for line in song_body.splitlines():\n",
    "        #if not line.startswith(\"%\"):\n",
    "            #if not \"P:\" in line:\n",
    "        if not line.startswith(('P:', 'M:', '%', \"L:\", 'K:')):\n",
    "            #print(\"Header detected!!!\",line)\n",
    "            new_line = line.strip(\"\\\\\")\n",
    "            match_pattern = r\"[: ]\"\n",
    "            new_line = re.sub(match_pattern, \"\", new_line)\n",
    "            clean_body_song = clean_body_song+new_line\n",
    "            if('||||' in clean_body_song):\n",
    "                match_pattern = r'\\|\\|\\|\\|'\n",
    "                clean_body_song = re.sub(match_pattern, \"|\", clean_body_song)\n",
    "                print(\"new line\",clean_body_song)\n",
    "                print(\"old line\",line)\n",
    "                print(\"-\"*10)\n",
    "\n",
    "    clean_body_song = erase_double_bars(clean_body_song)\n",
    "    clean_body_song = add_ending_char(clean_body_song)\n",
    "\n",
    "    if 'K' in clean_body_song:\n",
    "        print(\"Found invalid character\",clean_body_song)\n",
    "        raise Exception         \n",
    "    chord_progression = get_chord_progression(clean_body_song)\n",
    "    chords_data = get_chord_occurrences(clean_body_song)\n",
    "    chords_data[\"chord_progression\"] = chord_progression\n",
    "\n",
    "    chords_data[\"clean_body\"] = clean_body_song\n",
    "    return chords_data\n",
    "\n",
    "### treat strings line by line to ignore comments and parts!!!!!\n",
    "def get_song_characteristics_from_body(tuneBook_df, header_col_name=\"original_body\"):\n",
    "    songs_bodies=tuneBook_df[header_col_name]\n",
    "    #global control_var\n",
    "    #print(songs_bodies)\n",
    "    #if(control_var):\n",
    "        #control_var = False\n",
    "    #df = tuneBook_df.join(songs_bodies.apply(get_chords_data))\n",
    "    new_data = songs_bodies.apply(get_chords_data)\n",
    "    chords_df = new_data.apply(pd.Series)\n",
    "    chords = chords_df['chord_progression']\n",
    "    chords_df = chords_df.drop('chord_progression', axis=1)\n",
    "    chords_df[\"chord_progression\"] = chords\n",
    "    tuneBook_df = tuneBook_df.join(chords_df)\n",
    "    #print(tuneBook_df.columns)\n",
    "    #print(\"*\"*20)   \n",
    "    # get anacrusis bool to see that the dataset is balanced\n",
    "    #tuneBook[\"num_notes_per_bar\"] # count num -> useless? to measure intensity?\n",
    "    #tuneBook[\"num_bars\"] # count number of || ins song -> to detect possible outliers?\n",
    "    #tuneBook[\"num_notes_in_song\"] # count a,b,c,d...in string, match regex \"a\"   \n",
    "    #tuneBook[\"chord_progression\"] #string \"|\"C\"|\"Dm\"|\" -> to count chords and to have a \"Tree view of common chord progressions\"\n",
    "    #tuneBook[\"multiple_parts\"] #bool -> to treat songs with multiple parts\n",
    "    ##  Count types of notes in each song to see the proportion of notes for a given key\n",
    "    ## Maybe by chord also?\n",
    "    return tuneBook_df\n",
    "def get_songs_metadata(songs_dataframe, tuneBook_name):\n",
    "    #songs_dataframe[\"number_in_tunebook\"] = get_attributes_from_song_header(songs_dataframe)\n",
    "    df = get_attributes_from_song_header(songs_dataframe,tuneBook_name)\n",
    "    df = get_song_characteristics_from_body(df)\n",
    "    return df\n",
    "\n",
    "def get_chord_columns_as_occurrences(songs_df):\n",
    "    chords_names = songs_df.columns[songs_df.columns.str.contains('\"[^\"]*\"')]\n",
    "    chords_occurrences = songs_df[chords_names].sum()\n",
    "    return chords_occurrences\n",
    "\n",
    "\n",
    "\n",
    "def drop_rows_by_chord_occurrence(songs_df,drop_threshold=15):\n",
    "    \n",
    "    chords_occurrences = get_chord_columns_as_occurrences(songs_df)\n",
    "    ## Gets the names of the chords that appear less times than the threshold.\n",
    "    chords_names_least_occurrences = chords_occurrences[chords_occurrences<drop_threshold].index.to_list()\n",
    "    print(\"number chords least occurrences: \",len(chords_names_least_occurrences))\n",
    "    print(\"chords least occurrences\",chords_names_least_occurrences)\n",
    "\n",
    "    ## Applies a logical OR to the selected columns in order to drop the rows that have a value in the corresponding column\n",
    "    mask_drop_songs_with_low_chords_sample = songs_df[chords_names_least_occurrences].apply(lambda row: row>0).any(axis=1)\n",
    "    drop_num = mask_drop_songs_with_low_chords_sample.value_counts()\n",
    "    print(\"number of values to be dropped:\\n\",drop_num)\n",
    "\n",
    "    ##Apply the negative mask to preserve the rows that don't have occurrences of the selected chords\n",
    "    clean_songs_df = songs_df[~mask_drop_songs_with_low_chords_sample]\n",
    "\n",
    "    ##Drop columns of useless chords\n",
    "    clean_songs_df = clean_songs_df.drop(chords_names_least_occurrences, axis=1)\n",
    "    clean_songs_df = clean_songs_df.reset_index(drop=True)\n",
    "\n",
    "    return clean_songs_df\n",
    "\n",
    "def drop_rows_by_extended_chords(songs_df,chords_to_drop=[]):\n",
    "    \n",
    "    chords_occurrences = get_chord_columns_as_occurrences(songs_df)\n",
    "\n",
    "    ## Gets the names of the chords that have '/' \n",
    "    extended_chords_names = songs_df.columns[songs_df.columns.str.contains(\"/\")].to_list()\n",
    "    extended_chords_names = extended_chords_names + chords_to_drop\n",
    "\n",
    "    ## Applies a logical OR to the selected columns in order to drop the rows that have a value in the corresponding column\n",
    "    mask_drop_songs_with_extended_chords = songs_df[extended_chords_names].apply(lambda row: row>0).any(axis=1)\n",
    "    drop_num = mask_drop_songs_with_extended_chords.value_counts()\n",
    "    print(\"number of values to be dropped:\\n\",drop_num)\n",
    "\n",
    "    ##Apply the negative mask to preserve the rows that don't have occurrences of the selected chords\n",
    "    clean_songs_df = songs_df[~mask_drop_songs_with_extended_chords]\n",
    "\n",
    "    ##Drop columns of useless chords\n",
    "    clean_songs_df = clean_songs_df.drop(extended_chords_names, axis=1)\n",
    "    clean_songs_df = clean_songs_df.reset_index(drop=True)\n",
    "\n",
    "    return clean_songs_df\n",
    "\n",
    "\n",
    "def count_bars_in_songs(songs_df):\n",
    "    return 0\n",
    "\n",
    "def calculate_note_length(songs_df_row,col_name=\"unit_note_length\"):\n",
    "    new_note_length = \"\"\n",
    "    ## See: https://abcnotation.com/wiki/abc:standard:v2.1#lunit_note_length \n",
    "    map_meter_to_note_length = {\n",
    "        \"4/4\":\"1/8\",\n",
    "        \"6/8\":\"1/8\",\n",
    "        \"3/4\":\"1/8\",\n",
    "        \"2/4\":\"1/16\",\n",
    "        \"9/8\":\"1/8\",\n",
    "        \"2/2\":\"1/8\",\n",
    "        \"3/2\":\"1/8\",\n",
    "        \"6/4\":\"1/8\"\n",
    "    }\n",
    "    if(pd.isna(songs_df_row[col_name])):\n",
    "        print(\"NA detected!\")\n",
    "        meter = songs_df_row[\"meter\"]\n",
    "        new_note_length = map_meter_to_note_length[meter]\n",
    "    else:\n",
    "        new_note_length = songs_df_row[col_name]\n",
    "\n",
    "    return new_note_length\n",
    "\n",
    "def fill_missing_note_length(songs_df):\n",
    "    col_with_na = \"unit_note_length\"\n",
    "    songs_df[col_with_na] = songs_df.apply(calculate_note_length,axis=1)\n",
    "    return songs_df\n",
    "\n",
    "def drop_useless_columns(df,drop_columns=[\"Y\",\"notes\",\"parts\",\"rhythm\",\"source\"]):\n",
    "    clean_df = df\n",
    "    df_columns = clean_df.columns\n",
    "    for useless_column in drop_columns:\n",
    "        if(useless_column in df_columns):\n",
    "            clean_df = clean_df.drop(useless_column, axis=1)\n",
    "    return clean_df\n",
    "\n",
    "\n",
    "def generate_clean_header(songs_df,for_training=False):\n",
    "    descriptive_name_to_code = {\n",
    "                        'reference_number':'X',\n",
    "                        'title':'T',\n",
    "                        'source':'S',\n",
    "                        'meter':'M',\n",
    "                        'unit_note_length':'L',\n",
    "                        'rhythm':'R',\n",
    "                        'parts':'P',\n",
    "                        'key':'K',\n",
    "                        'file_name':'F',\n",
    "                        'notes':'N',\n",
    "                        }\n",
    "    header_cols = []\n",
    "    if for_training:\n",
    "        header_cols = [\"meter\",\"unit_note_length\",\"key\"]\n",
    "    else:\n",
    "        header_cols = [\"reference_number\",\"title\",\"meter\",\"unit_note_length\",\"key\"]\n",
    "\n",
    "    compose_header_codes_and_values = lambda row: [f'{descriptive_name_to_code[col_name]}:{row[col_name]}' for col_name in header_cols]\n",
    "    songs_df[\"clean_header\"] = songs_df[header_cols].apply(lambda row: ('\\n'.join(compose_header_codes_and_values(row))), axis=1)\n",
    "    if for_training:\n",
    "        append_chord_prog = lambda row: f'{row[\"clean_header\"]}\\n{row[\"chord_progression\"]}\\n'\n",
    "        songs_df[\"clean_header\"] = songs_df[[\"clean_header\",\"chord_progression\"]].apply(append_chord_prog, axis=1)\n",
    "\n",
    "    return songs_df\n",
    "\n",
    "def generate_clean_songs(songs_df,for_training=False):\n",
    "    clean_songs_df = generate_clean_header(songs_df,for_training)\n",
    "    additional_line =  \"\\n\" if not for_training else \"\"\n",
    "    append_body_to_header = lambda row: f'{row[\"clean_header\"]}{additional_line}{row[\"clean_body\"]}\\n'\n",
    "    clean_songs_df[\"clean_song\"] = clean_songs_df.apply(append_body_to_header, axis=1)\n",
    "    return clean_songs_df\n",
    "\n",
    "def prepare_dataset_for_EDA(relative_path=\"\",drop_by_occurrences=True, drop_by_extended_chord=True, min_chord_progression_length=20):\n",
    "    absolute_path = os.getcwd()\n",
    "    #relative_path = \"notebooks/data/NottinghamData/nottingham_database\"\n",
    "    #relative_path = \"notebooks/data/NottinghamCleaned/nottingham_match/python/data/nottingham_jukedeck/ABC_cleaned\"\n",
    "    absolute_path = os.path.join(absolute_path, relative_path)\n",
    "    songs_df = pd.DataFrame()\n",
    "    list_tuneBooks = get_tuneBooks_file_names(absolute_path)\n",
    "    for abc_tuneBook_filename in list_tuneBooks:\n",
    "        file_path = os.path.join(absolute_path, abc_tuneBook_filename)\n",
    "        \n",
    "        with open(file_path) as tuneBook:\n",
    "            contents = tuneBook.read()\n",
    "            df = tuneBook_to_dataframe(contents)\n",
    "            df1 = get_songs_metadata(df,abc_tuneBook_filename)\n",
    "            songs_df = pd.concat([songs_df, df1], ignore_index=True)\n",
    "\n",
    "    ## Sets chords columns as int types\n",
    "    chords_names = songs_df.columns[songs_df.columns.str.contains('\"[^\"]*\"')]\n",
    "    songs_df[chords_names] = songs_df[chords_names].fillna(0).astype(int)\n",
    "\n",
    "    \n",
    "\n",
    "    chords_occurrences = songs_df[chords_names].sum().sort_values(ascending=True)\n",
    "\n",
    "    #chords_occurrences = chords_occurrences.reset_index(drop=True)\n",
    "    chords_occurrences = chords_occurrences.reset_index()\n",
    "    chords_occurrences_before_drop = chords_occurrences.rename({0:\"count\",\"index\":\"chords\"},axis=1)\n",
    "    \n",
    "    clean_songs_df = songs_df\n",
    "    clean_songs_df = drop_useless_columns(clean_songs_df)\n",
    "\n",
    "\n",
    "    if(drop_by_occurrences):\n",
    "        clean_songs_df = drop_rows_by_chord_occurrence(clean_songs_df)\n",
    "\n",
    "\n",
    "    if(drop_by_extended_chord):\n",
    "        clean_songs_df = drop_rows_by_extended_chords(clean_songs_df)\n",
    "\n",
    "    \n",
    "    ## Drops rows that don't have the minimum chord progression length\n",
    "    mask_no_chord_progression = clean_songs_df[\"chord_progression\"].str.len()>min_chord_progression_length\n",
    "    clean_songs_df = clean_songs_df[mask_no_chord_progression]\n",
    "    clean_songs_df = clean_songs_df.reset_index(drop=True)\n",
    "\n",
    "    \n",
    "    ## X: 102 meter appears after they key so its meter is dropped in the cleaning.\n",
    "    ## its the only song with this format\n",
    "    clean_songs_df.loc[clean_songs_df[\"meter\"].isna(),\"meter\"] = \"6/8\"\n",
    "    \n",
    "    ## Fill empty values of note_length according to the meter of the song\n",
    "    clean_songs_df = fill_missing_note_length(clean_songs_df)\n",
    "\n",
    "    #for song in clean_songs_df[\"chord_progression\"]:\n",
    "    #    print(song)\n",
    "    #    print(\"**\"*10)\n",
    "    return songs_df, clean_songs_df, chords_occurrences_before_drop\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA Original and Clean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dataset_relative_path = \"notebooks/data/NottinghamCleaned/nottingham_match/python/data/nottingham_jukedeck/ABC_cleaned\"\n",
    "songs_df, clean_songs_df,chords_occurrences = prepare_dataset_for_EDA(relative_path=original_dataset_relative_path)\n",
    "sns.barplot(data=chords_occurrences,y=\"count\", x='chords',edgecolor='none',color=\"magenta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_df[\"original_body\"].str.contains(\"P:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chords_names = clean_songs_df.columns[clean_songs_df.columns.str.contains('\"[^\"]*\"')]\n",
    "\n",
    "chords_occurrences_df = clean_songs_df[chords_names].sum().sort_values(ascending=False).reset_index()\n",
    "print(chords_occurrences_df.columns)\n",
    "sns.barplot(data=chords_occurrences_df,y=\"index\", x=0,edgecolor='none',color=\"magenta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(songs_df[\"key\"].astype(\"category\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, ax = plt.subplots()\n",
    "ax = sns.histplot(songs_df[\"key\"].astype(\"category\"),label=\"abc\")\n",
    "ax = sns.histplot(clean_songs_df[\"key\"].astype(\"category\"), ax=ax,label=\"something 2\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_songs_df.columns[clean_songs_df.columns.str.contains('\"[\\(]')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chord_names = clean_songs_df.columns[clean_songs_df.columns.str.contains('\"')].to_list()\n",
    "sum_chords = clean_songs_df[chord_names].sum().sort_values().reset_index()\n",
    "chords = \"|\"\n",
    "for chord_name,chord_count in zip(sum_chords[\"index\"],sum_chords[0]):\n",
    "    chords = chords+chord_name+'|'\n",
    "    print(chord_name,chord_count)\n",
    "print(chords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clean_songs_df[\"chord_progression\"].str.contains(\"segno\").value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(songs_df[\"chord_progression\"].str.contains(\"segno\").value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for song,title,book in zip(clean_songs_df[\"clean_body\"],clean_songs_df[\"title\"],clean_songs_df[\"tuneBook\"]):\n",
    "    print(title,\" \", book)\n",
    "    print(song)\n",
    "    print(\"**\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptive_names = {\n",
    "                        'X': 'reference_number',\n",
    "                        'T': 'title',\n",
    "                        'S': 'source',\n",
    "                        'M': 'meter',\n",
    "                        'L': 'unit_note_length',\n",
    "                        'R': 'rhythm',\n",
    "                        'P': 'parts',\n",
    "                        'K': 'key',\n",
    "                        'F': 'file_name',\n",
    "                        'N': 'notes'\n",
    "                        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_name = \"meter\"\n",
    "category_value_counts = clean_songs_df[category_name].astype(\"category\").value_counts()\n",
    "print(category_value_counts)\n",
    "print(\"\\nSum of values:\",category_value_counts.sum())\n",
    "print(\"\\nNaN values:\",clean_songs_df[category_name].isna().sum())\n",
    "sns.histplot(clean_songs_df[category_name].astype(\"category\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_name = \"rhythm\"\n",
    "category_value_counts = songs_df[category_name].astype(\"category\").value_counts()\n",
    "print(category_value_counts)\n",
    "print(\"\\nSum of values:\",category_value_counts.sum())\n",
    "print(\"\\nNaN values:\",songs_df[category_name].isna().sum())\n",
    "sns.histplot(songs_df[category_name].astype(\"category\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_name = \"unit_note_length\"\n",
    "category_value_counts = clean_songs_df[category_name].astype(\"category\").value_counts()\n",
    "print(category_value_counts)\n",
    "print(\"\\nSum of values:\",category_value_counts.sum())\n",
    "print(\"\\nNaN values:\",clean_songs_df[category_name].isna().sum())\n",
    "sns.histplot(clean_songs_df[category_name].astype(\"category\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_name = \"key\"\n",
    "category_value_counts = clean_songs_df[category_name].astype(\"category\").value_counts()\n",
    "print(category_value_counts)\n",
    "print(\"\\nSum of values:\",category_value_counts.sum())\n",
    "print(\"\\nNaN values:\",clean_songs_df[category_name].isna().sum())\n",
    "sns.histplot(clean_songs_df[category_name].astype(\"category\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_name = \"tuneBook\"\n",
    "category_value_counts = clean_songs_df[category_name].astype(\"category\").value_counts()\n",
    "print(category_value_counts)\n",
    "print(\"\\nSum of values:\",category_value_counts.sum())\n",
    "print(\"\\nNaN values:\",clean_songs_df[category_name].isna().sum())\n",
    "rename_dict = {category_name:'count','index':\"tuneBook\"}\n",
    "aux_df_1 = clean_songs_df[category_name].astype(\"category\").value_counts().reset_index().rename(rename_dict,axis=1)\n",
    "aux_df_2 = songs_df[category_name].astype(\"category\").value_counts().reset_index().rename(rename_dict,axis=1)\n",
    "ax1= sns.barplot(data=aux_df_1, y='tuneBook', x='count',label=\"Clean Dataset\", color='#8D34DC',alpha=0.9)\n",
    "sns.barplot(ax=ax1,data=aux_df_2, y='tuneBook', x='count',label=\"Original Dataset\", color='magenta',alpha=0.4)\n",
    "plt.title(\"Original vs Preprocessed Dataset\")\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_name = \"unit_note_length\"\n",
    "category_value_counts = clean_songs_df[category_name].astype(\"category\").value_counts()\n",
    "print(category_value_counts)\n",
    "print(\"\\nSum of values:\",category_value_counts.sum())\n",
    "print(\"\\nNaN values:\",clean_songs_df[category_name].isna().sum())\n",
    "rename_dict = {category_name:'count','index':\"unit_note_length\"}\n",
    "aux_df_1 = clean_songs_df[category_name].astype(\"category\").value_counts().reset_index().rename(rename_dict,axis=1)\n",
    "aux_df_2 = songs_df[category_name].astype(\"category\").value_counts().reset_index().rename(rename_dict,axis=1)\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2 ,sharey=True)\n",
    "sns.barplot(ax=ax2,data=aux_df_1, x='unit_note_length', y='count',label=\"Clean Dataset\", color='#8D34DC',alpha=0.9)\n",
    "sns.barplot(ax=ax1,data=aux_df_2, x='unit_note_length', y='count',label=\"Original Dataset\", color='magenta',alpha=0.4)\n",
    "fig.suptitle('Original vs Preprocessed Dataset')\n",
    "fig.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aux = pd.DataFrame({\n",
    "        \"clean dataset\":[clean_songs_df.shape[0]],\n",
    "        \"original dataset\":[songs_df.shape[0]]\n",
    "        })\n",
    "df_aux[\"dropped values\"] = df_aux[\"original dataset\"]-df_aux[\"clean dataset\"]\n",
    "ordered_list_values = [\"dropped values\",\"original dataset\",\"clean dataset\"]\n",
    "magenta_palette = sns.dark_palette('magenta', n_colors=len(ordered_list_values))\n",
    "reversed_magenta_palette = list(reversed(magenta_palette))\n",
    "color_dict  = dict(zip(ordered_list_values, reversed_magenta_palette))\n",
    "ordered_list_values = [\"original dataset\",\"dropped values\",\"clean dataset\"]\n",
    "df_aux = df_aux[ordered_list_values]\n",
    "df_aux = df_aux.T.reset_index().rename({\"index\":\"values\",0:\"count\"},axis=1)\n",
    "ax = sns.barplot(data=df_aux,x=\"values\",y=\"count\",palette=magenta_palette,alpha=0.5)\n",
    "plt.title(\"Values in Dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_songs_df[\"meter\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_songs_df[\"meter\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clean_songs_df[\"unit_note_length\"].value_counts())\n",
    "print(clean_songs_df[\"unit_note_length\"].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_songs_df.columns.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of NaN in the entire dataset:\",clean_songs_df.isna().sum().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Intermediate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print clean dataset for transposing\n",
    "\n",
    "#for song in generate_clean_songs(clean_songs_df,for_training=False)[\"clean_song\"]:\n",
    "#    print(song)\n",
    "def save_dataframe(df,relative_path,file_name):\n",
    "    reorder_columns = df.columns.sort_values(ascending=False)\n",
    "    df_to_be_saved = df[reorder_columns]\n",
    "    print(file_name,\" dataset shape\",df_to_be_saved.shape)\n",
    "    print(df_to_be_saved.columns)\n",
    "    print(\"-\"*10)\n",
    "    df_to_be_saved.to_pickle(f'{relative_path}/{file_name}.pkl')\n",
    "\n",
    "def load_dataframe(relative_path,dataframe_name):\n",
    "    df = pd.read_pickle(f'{relative_path}/{dataframe_name}.pkl')    \n",
    "    return df\n",
    "\n",
    "def write_tuneBook_file(df,relative_path,file_name):\n",
    "    with open(f'{relative_path}/{file_name}.abc', 'w') as f:\n",
    "        for song in df[\"clean_song\"]:\n",
    "            f.write(song+'\\n')\n",
    "relative_path =\"notebooks/data/original_dataset\"\n",
    "clean_dataset_for_augmentation_df = generate_clean_songs(clean_songs_df,for_training=False)\n",
    "filename_name = 'clean_original_data'\n",
    "save_dataframe(clean_dataset_for_augmentation_df,relative_path,filename_name)\n",
    "write_tuneBook_file(clean_dataset_for_augmentation_df,relative_path,filename_name)\n",
    "\n",
    "clean_dataset_for_training_df = generate_clean_songs(clean_songs_df,for_training=True)\n",
    "filename_name = 'clean_original_training_data'\n",
    "save_dataframe(clean_dataset_for_training_df,relative_path,filename_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_path =\"notebooks/data/original_dataset\"\n",
    "filename_name = 'clean_original_data'\n",
    "clean_dataset_for_augmentation_df = load_dataframe(relative_path,filename_name)\n",
    "filename_name = 'clean_original_training_data'\n",
    "clean_dataset_for_training_df = load_dataframe(relative_path,filename_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"augmentation dataset shape\",clean_dataset_for_augmentation_df.shape)\n",
    "print(clean_dataset_for_augmentation_df.columns)\n",
    "print(\"training dataset shape\",clean_dataset_for_training_df.shape)\n",
    "print(clean_dataset_for_training_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for song in clean_dataset_for_augmentation_df.head()[\"clean_song\"]:\n",
    "    print(song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for song in clean_dataset_for_training_df.head()[\"clean_song\"]:\n",
    "    print(song)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Augmented Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "augmented_dataset_relative_path = \"notebooks/data/augmented_dataset\"\n",
    "_, clean_songs_augmented_df ,chords_occurrences_aug= prepare_dataset_for_EDA(relative_path=augmented_dataset_relative_path,drop_by_occurrences=True, drop_by_extended_chord=True)\n",
    "clean_songs_augmented_df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA Augmented datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def get_df_with_key_count(df,col_name=\"key\"):\n",
    "    category_name = col_name\n",
    "    key_categories = df[category_name].astype(\"category\").value_counts()\n",
    "    key_categories = key_categories.reset_index().sort_values(by=[category_name,\"index\"],ascending=[False, True])\n",
    "    #key_categories[\"tuneBook\"] = clean_songs_augmented_df[\"tuneBook\"]\n",
    "    key_categories = key_categories.rename(columns={'index': category_name, category_name: 'count'})\n",
    "\n",
    "    major_or_minor = lambda row: \"Minor\" if 'm' in row[category_name] else \"Major\"\n",
    "    key_categories[\"mode\"] = key_categories.apply(major_or_minor,axis=1)\n",
    "    print(key_categories.columns)\n",
    "    return key_categories\n",
    "\n",
    "key_categories_1 = get_df_with_key_count(clean_songs_augmented_df)\n",
    "key_categories_1[\"dataset_type\"] = \"augmented\"\n",
    "#sns.catplot(data=key_categories_1,y=\"key\",x=\"count\",order=key_categories_1['key'],kind='bar')\n",
    "key_categories_2 = get_df_with_key_count(clean_songs_df)\n",
    "key_categories_2[\"dataset_type\"] = \"original\"\n",
    "#sns.catplot(data=key_categories_2,y=\"key\",x=\"count\",order=key_categories_2['key'],kind='bar')\n",
    "\n",
    "key_categories = pd.concat([key_categories_1,key_categories_2],axis=0)\n",
    "key_categories['dataset_type'] = pd.Categorical(key_categories['dataset_type'], categories=['original', 'augmented'], ordered=True)\n",
    "key_categories['count'] = pd.to_numeric(key_categories['count'])\n",
    "print(key_categories.dtypes)\n",
    "key_categories['key'] = key_categories['key'].astype('category')\n",
    "key_categories['mode'] = key_categories['mode'].astype('category')\n",
    "print(key_categories.dtypes)\n",
    "magenta_palette = sns.light_palette('magenta', n_colors=len(key_categories['count'].unique()))\n",
    "color_dict  = dict(zip(key_categories['count'].sort_values(ascending=True).unique(), magenta_palette))\n",
    "ax = sns.catplot(data=key_categories,y=\"key\",x=\"count\",kind='bar',col='dataset_type')\n",
    "ax.fig.suptitle('Key Count Comparison',y=1.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax1 = sns.barplot(label=\"Augmented Dataset\",data=key_categories_2,y=\"key\",x=\"count\",order=key_categories_2['key'], color='#8D34DC',alpha=0.9)\n",
    "ax1 = sns.barplot(ax=ax1,label=\"Original Dataset\",data=key_categories_1,y=\"key\",x=\"count\",order=key_categories_1['key'], color='magenta',alpha=0.4)\n",
    "ax1.set_title(\"Key Count Comparison\")\n",
    "ax1.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key_bar_plots(old_df, new_df):\n",
    "    chords_occurrences_aug = old_df\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(6, 8),sharex=True)\n",
    "    sns.barplot(ax=ax1,data=chords_occurrences_aug.sort_values(by=\"count\",ascending=False),x=\"count\", y='chords',edgecolor='none',color=\"magenta\",alpha=0.5)\n",
    "    ax1.set_title('Before Preprocessing')\n",
    "\n",
    "    clean_songs_augmented_df= new_df\n",
    "    chords_names_aug = clean_songs_augmented_df.columns[clean_songs_augmented_df.columns.str.contains('\"[^\"]*\"')]\n",
    "\n",
    "    chords_aug_occurrences_df = clean_songs_augmented_df[chords_names_aug].sum().sort_values(ascending=False).reset_index()\n",
    "    print(chords_aug_occurrences_df.columns)\n",
    "    column_names = {\n",
    "        'index': 'chords',\n",
    "        0:'count'\n",
    "        }\n",
    "    chords_aug_occurrences_df = chords_aug_occurrences_df.rename(columns=column_names)\n",
    "    sns.barplot(ax=ax2,data=chords_aug_occurrences_df,y=\"chords\", x=\"count\",edgecolor='none',color=\"magenta\",alpha=0.5)\n",
    "    ax2.set_title('After Preprocessing')\n",
    "\n",
    "    fig.suptitle('Chord Count in Dataset', y=0.94)\n",
    "\n",
    "get_key_bar_plots(chords_occurrences_aug,clean_songs_augmented_df)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Augmented Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_dataset_for_augmentation_df = generate_clean_songs(clean_songs_augmented_df,for_training=False)\n",
    "print(\"dataset shape\",clean_songs_augmented_df.shape)\n",
    "print(\"dataset\\n\",clean_songs_augmented_df.columns.sort_values(ascending=False))\n",
    "clean_augmented_dataset_for_training_df = generate_clean_songs(clean_songs_augmented_df,for_training=True)\n",
    "relative_path =\"notebooks/data/augmented_dataset\"\n",
    "filename_name = 'clean_augmented_data'\n",
    "save_dataframe(df=clean_augmented_dataset_for_training_df,file_name=filename_name,relative_path=relative_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_path =\"notebooks/data/augmented_dataset\"\n",
    "filename_name = 'clean_augmented_data'\n",
    "clean_augmented_dataset_for_training_df = pd.read_pickle(f'{relative_path}/{filename_name}.pkl')\n",
    "clean_augmented_dataset_for_training_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_double_bars(song):\n",
    "    pattern = r'\\|{2,}'\n",
    "    matches = re.findall(pattern, song)\n",
    "    count = len(matches)\n",
    "    return count\n",
    "    \n",
    "\n",
    "\n",
    "s = erase_double_bars('one |||| two || three |a|a| four|||')\n",
    "add_ending_char(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = clean_augmented_dataset_for_training_df['tuneBook'] == 'dataset_min4.abc'\n",
    "sample_df = clean_augmented_dataset_for_training_df[mask]\n",
    "ends = 0\n",
    "in_middle = 0\n",
    "no_end = 0\n",
    "weird = 0\n",
    "regex_bars_found = 0\n",
    "for song in sample_df['clean_body']:\n",
    "    regex_bars_found += count_double_bars(song)\n",
    "    print(song)\n",
    "    if '||' in song:\n",
    "        if song.endswith('||'):\n",
    "            ends +=1\n",
    "        else:\n",
    "            in_middle += 1\n",
    "    else:\n",
    "        no_end += 1\n",
    "    if ']' in song:\n",
    "        no_end += 1\n",
    "print(\"ends\",ends)\n",
    "print(\"in middle\",in_middle)\n",
    "print(\"no end\",no_end)\n",
    "print(\"total\",ends+in_middle+no_end)\n",
    "print(\"regex bars found\", regex_bars_found)\n",
    "print(\"] char in song\", weird)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for song in sample_df['clean_body'].apply(erase_double_bars):\n",
    "    print(song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for song in clean_songs_df[\"clean_body\"]:\n",
    "    print(song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song = '(3ABc|\"D\"d2f2d2|fg|\"D\"afafd2ga|||\"G\"bgbg\"A7\"e2fg||\"D\"dcdAFAdf||\"Em\"edcB\"A7\"A2g2|||||\"D\"fgaf\"A7\"bgec|\"D\"d2f2d2|]'\n",
    "song = erase_double_bars(song)\n",
    "song = add_ending_char(song)\n",
    "song"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
